var documenterSearchIndex = {"docs":
[{"location":"api/bayesian_regression/#Bayesian-Regression-Models","page":"Bayesian Regression Models","title":"Bayesian Regression Models","text":"","category":"section"},{"location":"api/bayesian_regression/","page":"Bayesian Regression Models","title":"Bayesian Regression Models","text":"BayesianRegression","category":"page"},{"location":"api/bayesian_regression/#CRRao.BayesianRegression","page":"Bayesian Regression Models","title":"CRRao.BayesianRegression","text":"BayesianRegression{RegressionType}\n\nType to represent bayesian regression models returned by fit functions. This type is used internally by the package to represent all bayesian regression models. RegressionType is a Symbol representing the model class.\n\n\n\n\n\n","category":"type"},{"location":"api/bayesian_regression/#Linear-Regression","page":"Bayesian Regression Models","title":"Linear Regression","text":"","category":"section"},{"location":"api/bayesian_regression/","page":"Bayesian Regression Models","title":"Bayesian Regression Models","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression, prior::Prior_Ridge, h::Float64 = 0.01, sim_size::Int64 = 1000)\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression, prior::Prior_Laplace, h::Float64 = 0.01, sim_size::Int64 = 1000)\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression, prior::Prior_Cauchy, sim_size::Int64 = 1000)\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression, prior::Prior_TDist, h::Float64 = 2.0, sim_size::Int64 = 1000)\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression, prior::Prior_Uniform, h::Float64 = 0.01, sim_size::Int64 = 1000)","category":"page"},{"location":"api/bayesian_regression/#CRRao.fit","page":"Bayesian Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression, prior::Prior_Ridge, h::Float64 = 0.01, sim_size::Int64 = 1000)\n\nFit a Bayesian Linear Regression model on the input data with a Ridge prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> df = dataset(\"datasets\", \"mtcars\")\n32×12 DataFrame\n Row │ Model              MPG      Cyl    Disp     HP     DRat     WT       QSec     VS     AM     Gear   Carb  \n     │ String31           Float64  Int64  Float64  Int64  Float64  Float64  Float64  Int64  Int64  Int64  Int64 \n─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ Mazda RX4             21.0      6    160.0    110     3.9     2.62     16.46      0      1      4      4\n   2 │ Mazda RX4 Wag         21.0      6    160.0    110     3.9     2.875    17.02      0      1      4      4\n   3 │ Datsun 710            22.8      4    108.0     93     3.85    2.32     18.61      1      1      4      1\n   4 │ Hornet 4 Drive        21.4      6    258.0    110     3.08    3.215    19.44      1      0      3      1\n  ⋮  │         ⋮             ⋮       ⋮       ⋮       ⋮       ⋮        ⋮        ⋮       ⋮      ⋮      ⋮      ⋮\n  30 │ Ferrari Dino          19.7      6    145.0    175     3.62    2.77     15.5       0      1      5      6\n  31 │ Maserati Bora         15.0      8    301.0    335     3.54    3.57     14.6       0      1      5      8\n  32 │ Volvo 142E            21.4      4    121.0    109     4.11    2.78     18.6       1      1      4      2\n                                                                                                 25 rows omitted\njulia> container = fit(@formula(MPG ~ HP + WT + Gear), df, LinearRegression(), Prior_Ridge())\n┌ Info: Found initial step size\n└   ϵ = 0.00078125\nChains MCMC chain (10000×18×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 30.44 seconds\nCompute duration  = 30.44 seconds\nparameters        = v, σ, α, β[1], β[2], β[3]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           v    6.9097    3.7793     0.0378    0.0609   3848.1626    0.9999      126.4013\n           σ    2.6726    0.3878     0.0039    0.0061   3787.1472    1.0000      124.3972\n           α   28.6866    5.4205     0.0542    0.1106   2431.5304    1.0001       79.8690\n        β[1]   -0.0395    0.0106     0.0001    0.0002   4057.7267    0.9999      133.2849\n        β[2]   -2.7056    0.9635     0.0096    0.0176   2897.6230    1.0001       95.1788\n        β[3]    1.5912    0.9825     0.0098    0.0198   2538.0548    1.0001       83.3680\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           v    2.5200    4.5014    6.0397    8.2382   16.6220\n           σ    2.0519    2.4030    2.6297    2.8942    3.5482\n           α   17.6034   25.2623   28.9229   32.3360   38.7343\n        β[1]   -0.0612   -0.0464   -0.0393   -0.0325   -0.0191\n        β[2]   -4.5163   -3.3443   -2.7385   -2.1041   -0.7211\n        β[3]   -0.2205    0.9158    1.5520    2.2028    3.6202\n\n\n\n\n\n","category":"function"},{"location":"api/bayesian_regression/#CRRao.fit-2","page":"Bayesian Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression, prior::Prior_Laplace, h::Float64 = 0.01, sim_size::Int64 = 1000)\n\nFit a Bayesian Linear Regression model on the input data with a Laplace prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> df = dataset(\"datasets\", \"mtcars\")\n32×12 DataFrame\n Row │ Model              MPG      Cyl    Disp     HP     DRat     WT       QSec     VS     AM     Gear   Carb  \n     │ String31           Float64  Int64  Float64  Int64  Float64  Float64  Float64  Int64  Int64  Int64  Int64 \n─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ Mazda RX4             21.0      6    160.0    110     3.9     2.62     16.46      0      1      4      4\n   2 │ Mazda RX4 Wag         21.0      6    160.0    110     3.9     2.875    17.02      0      1      4      4\n   3 │ Datsun 710            22.8      4    108.0     93     3.85    2.32     18.61      1      1      4      1\n   4 │ Hornet 4 Drive        21.4      6    258.0    110     3.08    3.215    19.44      1      0      3      1\n  ⋮  │         ⋮             ⋮       ⋮       ⋮       ⋮       ⋮        ⋮        ⋮       ⋮      ⋮      ⋮      ⋮\n  30 │ Ferrari Dino          19.7      6    145.0    175     3.62    2.77     15.5       0      1      5      6\n  31 │ Maserati Bora         15.0      8    301.0    335     3.54    3.57     14.6       0      1      5      8\n  32 │ Volvo 142E            21.4      4    121.0    109     4.11    2.78     18.6       1      1      4      2\n                                                                                                 25 rows omitted\njulia> container = fit(@formula(MPG ~ HP + WT + Gear), df, LinearRegression(), Prior_Laplace())\n┌ Info: Found initial step size\n└   ϵ = 0.00078125\nChains MCMC chain (10000×18×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 28.55 seconds\nCompute duration  = 28.55 seconds\nparameters        = v, σ, α, β[1], β[2], β[3]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           v    4.2213    3.0653     0.0307    0.0506   3799.4211    0.9999      133.0609\n           σ    2.6713    0.3829     0.0038    0.0068   3782.5307    1.0001      132.4694\n           α   29.0523    5.2589     0.0526    0.1032   3144.5864    1.0004      110.1277\n        β[1]   -0.0398    0.0106     0.0001    0.0002   4429.6471    1.0005      155.1323\n        β[2]   -2.7161    0.9506     0.0095    0.0182   3299.1828    1.0009      115.5419\n        β[3]    1.5129    0.9530     0.0095    0.0180   3383.7096    1.0002      118.5021\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           v    1.2438    2.3788    3.4110    5.1138   11.7423\n           σ    2.0692    2.4016    2.6226    2.8897    3.5602\n           α   17.8056   25.8001   29.2866   32.5385   38.8889\n        β[1]   -0.0614   -0.0466   -0.0395   -0.0326   -0.0194\n        β[2]   -4.5559   -3.3384   -2.7407   -2.1204   -0.7254\n        β[3]   -0.2790    0.8794    1.4691    2.1092    3.5245\n\n\n\n\n\n","category":"function"},{"location":"api/bayesian_regression/#CRRao.fit-3","page":"Bayesian Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression, prior::Prior_Cauchy, sim_size::Int64 = 1000)\n\nFit a Bayesian Linear Regression model on the input data with a Cauchy prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> df = dataset(\"datasets\", \"mtcars\")\n32×12 DataFrame\n Row │ Model              MPG      Cyl    Disp     HP     DRat     WT       QSec     VS     AM     Gear   Carb  \n     │ String31           Float64  Int64  Float64  Int64  Float64  Float64  Float64  Int64  Int64  Int64  Int64 \n─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ Mazda RX4             21.0      6    160.0    110     3.9     2.62     16.46      0      1      4      4\n   2 │ Mazda RX4 Wag         21.0      6    160.0    110     3.9     2.875    17.02      0      1      4      4\n   3 │ Datsun 710            22.8      4    108.0     93     3.85    2.32     18.61      1      1      4      1\n   4 │ Hornet 4 Drive        21.4      6    258.0    110     3.08    3.215    19.44      1      0      3      1\n  ⋮  │         ⋮             ⋮       ⋮       ⋮       ⋮       ⋮        ⋮        ⋮       ⋮      ⋮      ⋮      ⋮\n  30 │ Ferrari Dino          19.7      6    145.0    175     3.62    2.77     15.5       0      1      5      6\n  31 │ Maserati Bora         15.0      8    301.0    335     3.54    3.57     14.6       0      1      5      8\n  32 │ Volvo 142E            21.4      4    121.0    109     4.11    2.78     18.6       1      1      4      2\n                                                                                                 25 rows omitted\njulia> container = fit(@formula(MPG ~ HP + WT + Gear), df, LinearRegression(), Prior_Cauchy(), 20000)\n┌ Info: Found initial step size\n└   ϵ = 0.000390625\nChains MCMC chain (20000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:21000\nNumber of chains  = 1\nSamples per chain = 20000\nWall duration     = 34.1 seconds\nCompute duration  = 34.1 seconds\nparameters        = σ, α, β[1], β[2], β[3]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           σ    2.5891    0.3413     0.0024    0.0036   8611.3030    1.0001      252.5087\n           α   30.2926    4.6666     0.0330    0.0590   5600.5552    1.0013      164.2247\n        β[1]   -0.0394    0.0100     0.0001    0.0001   7985.0944    1.0009      234.1464\n        β[2]   -2.8393    0.8638     0.0061    0.0106   6031.2854    1.0012      176.8550\n        β[3]    1.2738    0.8524     0.0060    0.0107   5814.5026    1.0014      170.4983\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           σ    2.0266    2.3485    2.5547    2.7908    3.3512\n           α   20.8140   27.3265   30.3854   33.4168   39.1369\n        β[1]   -0.0595   -0.0458   -0.0393   -0.0328   -0.0197\n        β[2]   -4.5172   -3.4069   -2.8485   -2.2786   -1.1244\n        β[3]   -0.3576    0.7039    1.2568    1.8199    3.0201\n\n\n\n\n\n","category":"function"},{"location":"api/bayesian_regression/#CRRao.fit-4","page":"Bayesian Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression, prior::Prior_TDist, h::Float64 = 2.0, sim_size::Int64 = 1000)\n\nFit a Bayesian Linear Regression model on the input data with a t(ν) distributed prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsPlots, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> df = dataset(\"datasets\", \"mtcars\")\n32×12 DataFrame\n Row │ Model              MPG      Cyl    Disp     HP     DRat     WT       QSec     VS     AM     Gear   Carb  \n     │ String31           Float64  Int64  Float64  Int64  Float64  Float64  Float64  Int64  Int64  Int64  Int64 \n─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ Mazda RX4             21.0      6    160.0    110     3.9     2.62     16.46      0      1      4      4\n   2 │ Mazda RX4 Wag         21.0      6    160.0    110     3.9     2.875    17.02      0      1      4      4\n   3 │ Datsun 710            22.8      4    108.0     93     3.85    2.32     18.61      1      1      4      1\n   4 │ Hornet 4 Drive        21.4      6    258.0    110     3.08    3.215    19.44      1      0      3      1\n  ⋮  │         ⋮             ⋮       ⋮       ⋮       ⋮       ⋮        ⋮        ⋮       ⋮      ⋮      ⋮      ⋮\n  30 │ Ferrari Dino          19.7      6    145.0    175     3.62    2.77     15.5       0      1      5      6\n  31 │ Maserati Bora         15.0      8    301.0    335     3.54    3.57     14.6       0      1      5      8\n  32 │ Volvo 142E            21.4      4    121.0    109     4.11    2.78     18.6       1      1      4      2\n                                                                                                 25 rows omitted\njulia> container = fit(@formula(MPG ~ HP + WT + Gear), df, LinearRegression(), Prior_TDist())\n┌ Info: Found initial step size\n└   ϵ = 1.1920928955078126e-8\nChains MCMC chain (10000×18×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 41.09 seconds\nCompute duration  = 41.09 seconds\nparameters        = ν, σ, α, β[1], β[2], β[3]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           ν    1.0538    0.5576     0.0056    0.0143   1340.7091    0.9999       32.6318\n           σ    2.6251    0.3559     0.0036    0.0043   6374.0312    0.9999      155.1388\n           α   30.1859    4.7935     0.0479    0.0605   5361.7257    1.0006      130.5001\n        β[1]   -0.0396    0.0103     0.0001    0.0001   5835.9959    1.0003      142.0434\n        β[2]   -2.8099    0.8772     0.0088    0.0114   5301.0033    1.0010      129.0221\n        β[3]    1.2856    0.8699     0.0087    0.0106   5752.1640    1.0003      140.0030\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           ν    0.3670    0.6600    0.9301    1.2961    2.4821\n           σ    2.0327    2.3758    2.5885    2.8442    3.4393\n           α   20.4816   27.0685   30.2787   33.4481   39.3462\n        β[1]   -0.0599   -0.0464   -0.0396   -0.0326   -0.0198\n        β[2]   -4.4924   -3.3902   -2.8250   -2.2351   -1.0257\n        β[3]   -0.3642    0.7021    1.2642    1.8397    3.0849\njulia> plot(container.chain)\n\n\n\n\n\n","category":"function"},{"location":"api/bayesian_regression/#CRRao.fit-5","page":"Bayesian Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression, prior::Prior_Uniform, h::Float64 = 0.01, sim_size::Int64 = 1000)\n\nFit a Bayesian Linear Regression model on the input data with a Uniform prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsPlots, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> df = dataset(\"datasets\", \"mtcars\")\n32×12 DataFrame\n Row │ Model              MPG      Cyl    Disp     HP     DRat     WT       QSec     VS     AM     Gear   Carb  \n     │ String31           Float64  Int64  Float64  Int64  Float64  Float64  Float64  Int64  Int64  Int64  Int64 \n─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ Mazda RX4             21.0      6    160.0    110     3.9     2.62     16.46      0      1      4      4\n   2 │ Mazda RX4 Wag         21.0      6    160.0    110     3.9     2.875    17.02      0      1      4      4\n   3 │ Datsun 710            22.8      4    108.0     93     3.85    2.32     18.61      1      1      4      1\n   4 │ Hornet 4 Drive        21.4      6    258.0    110     3.08    3.215    19.44      1      0      3      1\n  ⋮  │         ⋮             ⋮       ⋮       ⋮       ⋮       ⋮        ⋮        ⋮       ⋮      ⋮      ⋮      ⋮\n  30 │ Ferrari Dino          19.7      6    145.0    175     3.62    2.77     15.5       0      1      5      6\n  31 │ Maserati Bora         15.0      8    301.0    335     3.54    3.57     14.6       0      1      5      8\n  32 │ Volvo 142E            21.4      4    121.0    109     4.11    2.78     18.6       1      1      4      2\n                                                                                                 25 rows omitted\njulia> container = fit(@formula(MPG ~ HP + WT + Gear), df, LinearRegression(), Prior_Uniform())\n┌ Info: Found initial step size\n└   ϵ = 0.00078125\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 34.62 seconds\nCompute duration  = 34.62 seconds\nparameters        = σ, α, β[1], β[2], β[3]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           σ    2.7117    0.3850     0.0039    0.0065   3581.7504    1.0006      103.4590\n           α   31.9599    4.8963     0.0490    0.0866   2347.8428    1.0000       67.8175\n        β[1]   -0.0369    0.0106     0.0001    0.0002   4837.8122    0.9999      139.7404\n        β[2]   -3.1811    0.9042     0.0090    0.0162   2643.2557    0.9999       76.3505\n        β[3]    1.0252    0.9053     0.0091    0.0157   2529.3416    1.0002       73.0601\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           σ    2.0984    2.4368    2.6656    2.9354    3.5799\n           α   22.3918   28.7053   31.9294   35.2194   41.3685\n        β[1]   -0.0580   -0.0438   -0.0370   -0.0299   -0.0161\n        β[2]   -4.9551   -3.7839   -3.1659   -2.6058   -1.3929\n        β[3]   -0.7644    0.4230    1.0254    1.6245    2.8040\njulia> plot(container.chain)\n\n\n\n\n\n","category":"function"},{"location":"api/bayesian_regression/#Logistic-Regression","page":"Bayesian Regression Models","title":"Logistic Regression","text":"","category":"section"},{"location":"api/bayesian_regression/","page":"Bayesian Regression Models","title":"Bayesian Regression Models","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::CRRaoLink, prior::Prior_Ridge, h::Float64 = 0.1, level::Float64 = 0.95, sim_size::Int64 = 1000)\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::CRRaoLink, prior::Prior_Laplace, h::Float64 = 0.1, level::Float64 = 0.95, sim_size::Int64 = 1000)\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::CRRaoLink, prior::Prior_Cauchy, h::Float64 = 0.1, level::Float64 = 0.95, sim_size::Int64 = 1000)\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::CRRaoLink, prior::Prior_TDist, h::Float64 = 1.0, level::Float64 = 0.95, sim_size::Int64 = 1000)\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::CRRaoLink, prior::Prior_Uniform, h::Float64 = 0.01, level::Float64 = 0.95, sim_size::Int64 = 1000)","category":"page"},{"location":"api/bayesian_regression/#CRRao.fit-6","page":"Bayesian Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::CRRaoLink, prior::Prior_Ridge, h::Float64 = 0.1, level::Float64 = 0.95, sim_size::Int64 = 1000)\n\nFit a Bayesian Logistic Regression model on the input data with a Ridge prior with the provided Link function.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> turnout = dataset(\"Zelig\", \"turnout\")\n2000×5 DataFrame\n  Row │ Race   Age    Educate  Income   Vote  \n      │ Cat…   Int32  Float64  Float64  Int32 \n──────┼───────────────────────────────────────\n    1 │ white     60     14.0   3.3458      1\n    2 │ white     51     10.0   1.8561      0\n    3 │ white     24     12.0   0.6304      0\n    4 │ white     38      8.0   3.4183      1\n  ⋮   │   ⋮      ⋮       ⋮        ⋮       ⋮\n 1998 │ white     51     16.0   7.8949      1\n 1999 │ white     22     10.0   2.4811      0\n 2000 │ white     59     10.0   0.5523      0\n                             1993 rows omitted\njulia> container_logit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Logit(), Prior_Ridge())\n┌ Warning: The current proposal will be rejected due to numerical error(s).\n│   isfinite.((θ, r, ℓπ, ℓκ)) = (true, false, false, false)\n└ @ AdvancedHMC ~/.julia/packages/AdvancedHMC/kB7Xa/src/hamiltonian.jl:47\nChains MCMC chain (1000×18×1 Array{Float64, 3}):\n\nIterations        = 501:1:1500\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 6.98 seconds\nCompute duration  = 6.98 seconds\nparameters        = λ, α, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse        ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64    Float64   Float64       Float64 \n\n           λ    2.3722    0.0007     0.0000    0.0001     5.1273    1.1072        0.7346\n           α    0.7872    0.0003     0.0000    0.0001     2.9594    1.5660        0.4240\n        β[1]    0.4843    0.0000     0.0000    0.0000   111.6465    1.0067       15.9952\n        β[2]    0.6183    0.0004     0.0000    0.0001     2.3393    2.2610        0.3351\n        β[3]   -1.4043    0.0003     0.0000    0.0001     3.1790    1.3848        0.4554\n        β[4]   -3.0656    0.0007     0.0000    0.0001     2.4692    1.9479        0.3538\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    2.3706    2.3721    2.3724    2.3726    2.3731\n           α    0.7865    0.7872    0.7873    0.7874    0.7876\n        β[1]    0.4842    0.4843    0.4843    0.4843    0.4843\n        β[2]    0.6178    0.6180    0.6181    0.6186    0.6192\n        β[3]   -1.4046   -1.4045   -1.4044   -1.4041   -1.4036\n        β[4]   -3.0669   -3.0659   -3.0655   -3.0653   -3.0645\n\njulia> container_probit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Probit(), Prior_Ridge())\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 177.96 seconds\nCompute duration  = 177.96 seconds\nparameters        = λ, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    0.0867    0.0533     0.0005    0.0008   3682.1660    0.9999       20.6909\n        β[1]    0.0033    0.0013     0.0000    0.0000   9024.2636    0.9999       50.7092\n        β[2]   -0.0162    0.0577     0.0006    0.0007   5712.7441    1.0000       32.1011\n        β[3]    0.0902    0.0137     0.0001    0.0002   6239.2706    1.0004       35.0598\n        β[4]    0.0220    0.0068     0.0001    0.0001   6004.4582    0.9999       33.7403\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.0354    0.0556    0.0733    0.1018    0.2139\n        β[1]    0.0007    0.0024    0.0033    0.0041    0.0058\n        β[2]   -0.1353   -0.0525   -0.0157    0.0218    0.0958\n        β[3]    0.0634    0.0808    0.0901    0.0992    0.1179\n        β[4]    0.0086    0.0174    0.0221    0.0265    0.0354\njulia> container_cloglog = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cloglog(), Prior_Ridge())\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 94.56 seconds\nCompute duration  = 94.56 seconds\nparameters        = λ, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse       ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64   Float64   Float64       Float64 \n\n           λ    0.4868    0.0003     0.0000    0.0000   20.6437    1.8077        0.2183\n        β[1]   -0.1684    0.0026     0.0000    0.0003   20.2642    2.5746        0.2143\n        β[2]    0.4824    0.0008     0.0000    0.0001   20.4744    2.2619        0.2165\n        β[3]    0.9618    0.0058     0.0001    0.0006   20.2614    2.5797        0.2143\n        β[4]   -0.3887    0.0004     0.0000    0.0000   21.1046    1.7123        0.2232\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.4861    0.4865    0.4869    0.4870    0.4872\n        β[1]   -0.1725   -0.1706   -0.1687   -0.1661   -0.1642\n        β[2]    0.4813    0.4817    0.4823    0.4831    0.4840\n        β[3]    0.9521    0.9566    0.9626    0.9664    0.9707\n        β[4]   -0.3892   -0.3890   -0.3888   -0.3882   -0.3878\njulia> container_cauchit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cauchit(), Prior_Ridge())\n┌ Info: Found initial step size\n└   ϵ = 0.003125\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 153.65 seconds\nCompute duration  = 153.65 seconds\nparameters        = λ, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    0.1737    0.1033     0.0010    0.0015   3731.6701    0.9999       24.2865\n        β[1]    0.0033    0.0024     0.0000    0.0000   8722.9987    1.0000       56.7711\n        β[2]   -0.0598    0.1193     0.0012    0.0017   5364.6587    1.0008       34.9143\n        β[3]    0.2191    0.0365     0.0004    0.0005   5826.8422    0.9999       37.9223\n        β[4]    0.0204    0.0128     0.0001    0.0002   5304.7531    0.9999       34.5245\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.0702    0.1130    0.1483    0.2052    0.4066\n        β[1]   -0.0013    0.0016    0.0032    0.0049    0.0083\n        β[2]   -0.3183   -0.1325   -0.0507    0.0204    0.1532\n        β[3]    0.1488    0.1942    0.2187    0.2429    0.2919\n        β[4]   -0.0046    0.0117    0.0203    0.0292    0.0459\n\n\n\n\n\n","category":"function"},{"location":"api/bayesian_regression/#CRRao.fit-7","page":"Bayesian Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::CRRaoLink, prior::Prior_Laplace, h::Float64 = 0.1, level::Float64 = 0.95, sim_size::Int64 = 1000)\n\nFit a Bayesian Logistic Regression model on the input data with a Laplace prior with the provided Link function.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> turnout = dataset(\"Zelig\", \"turnout\")\n2000×5 DataFrame\n  Row │ Race   Age    Educate  Income   Vote  \n      │ Cat…   Int32  Float64  Float64  Int32 \n──────┼───────────────────────────────────────\n    1 │ white     60     14.0   3.3458      1\n    2 │ white     51     10.0   1.8561      0\n    3 │ white     24     12.0   0.6304      0\n    4 │ white     38      8.0   3.4183      1\n  ⋮   │   ⋮      ⋮       ⋮        ⋮       ⋮\n 1998 │ white     51     16.0   7.8949      1\n 1999 │ white     22     10.0   2.4811      0\n 2000 │ white     59     10.0   0.5523      0\n                             1993 rows omitted\njulia> container_logit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Logit(), Prior_Laplace())\n┌ Info: Found initial step size\n└   ϵ = 0.003125\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 143.44 seconds\nCompute duration  = 143.44 seconds\nparameters        = λ, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    0.1178    0.0826     0.0008    0.0012   4670.5185    0.9999       32.5619\n        β[1]    0.0051    0.0022     0.0000    0.0000   9160.7544    1.0001       63.8669\n        β[2]   -0.0228    0.0890     0.0009    0.0013   4963.2154    1.0002       34.6025\n        β[3]    0.1628    0.0254     0.0003    0.0004   5795.2458    1.0000       40.4033\n        β[4]    0.0321    0.0118     0.0001    0.0002   5366.5589    1.0006       37.4146\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.0380    0.0677    0.0958    0.1410    0.3341\n        β[1]    0.0007    0.0036    0.0051    0.0066    0.0095\n        β[2]   -0.2299   -0.0690   -0.0133    0.0273    0.1522\n        β[3]    0.1145    0.1454    0.1624    0.1796    0.2133\n        β[4]    0.0090    0.0240    0.0323    0.0400    0.0549\njulia> container_probit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Probit(), Prior_Laplace())\n┌ Info: Found initial step size\n└   ϵ = 0.00078125\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 171.43 seconds\nCompute duration  = 171.43 seconds\nparameters        = λ, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    0.0821    0.0551     0.0006    0.0008   4512.1853    1.0003       26.3206\n        β[1]    0.0033    0.0013     0.0000    0.0000   8915.4805    0.9999       52.0059\n        β[2]   -0.0138    0.0553     0.0006    0.0008   5240.6484    1.0000       30.5698\n        β[3]    0.0916    0.0141     0.0001    0.0002   6402.4324    1.0001       37.3468\n        β[4]    0.0212    0.0070     0.0001    0.0001   5508.5643    1.0000       32.1326\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.0275    0.0477    0.0667    0.0988    0.2270\n        β[1]    0.0008    0.0024    0.0033    0.0042    0.0059\n        β[2]   -0.1346   -0.0444   -0.0088    0.0192    0.0907\n        β[3]    0.0641    0.0820    0.0913    0.1011    0.1195\n        β[4]    0.0074    0.0165    0.0213    0.0260    0.0349\njulia> container_cloglog = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cloglog(), Prior_Laplace())\n┌ Info: Found initial step size\n└   ϵ = 0.0015625\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 194.12 seconds\nCompute duration  = 194.12 seconds\nparameters        = λ, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    0.0731    0.0509     0.0005    0.0007   4981.1484    1.0002       25.6608\n        β[1]    0.0008    0.0012     0.0000    0.0000   9615.6483    1.0003       49.5358\n        β[2]   -0.0266    0.0521     0.0005    0.0007   4812.7260    1.0001       24.7932\n        β[3]    0.0759    0.0114     0.0001    0.0002   5448.6076    0.9999       28.0690\n        β[4]    0.0069    0.0060     0.0001    0.0001   4591.7360    0.9999       23.6547\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.0242    0.0421    0.0600    0.0873    0.2035\n        β[1]   -0.0015    0.0000    0.0008    0.0016    0.0031\n        β[2]   -0.1478   -0.0559   -0.0199    0.0063    0.0647\n        β[3]    0.0538    0.0682    0.0760    0.0836    0.0983\n        β[4]   -0.0045    0.0027    0.0068    0.0111    0.0188\njulia> container_cauchit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cauchit(), Prior_Laplace())\n┌ Info: Found initial step size\n└   ϵ = 0.0330078125\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 151.32 seconds\nCompute duration  = 151.32 seconds\nparameters        = λ, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    0.1426    0.1158     0.0012    0.0019   4085.6946    1.0002       27.0013\n        β[1]    0.0032    0.0024     0.0000    0.0000   6944.6444    1.0001       45.8953\n        β[2]   -0.0474    0.1145     0.0011    0.0016   4859.5428    1.0001       32.1154\n        β[3]    0.2237    0.0363     0.0004    0.0005   4613.9690    0.9999       30.4925\n        β[4]    0.0185    0.0126     0.0001    0.0002   4725.1966    0.9999       31.2275\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.0446    0.0790    0.1127    0.1686    0.4193\n        β[1]   -0.0013    0.0016    0.0032    0.0047    0.0081\n        β[2]   -0.3124   -0.1050   -0.0278    0.0204    0.1493\n        β[3]    0.1551    0.1986    0.2228    0.2472    0.2979\n        β[4]   -0.0055    0.0097    0.0182    0.0269    0.0439\n\n\n\n\n\n","category":"function"},{"location":"api/bayesian_regression/#CRRao.fit-8","page":"Bayesian Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::CRRaoLink, prior::Prior_Cauchy, h::Float64 = 0.1, level::Float64 = 0.95, sim_size::Int64 = 1000)\n\nFit a Bayesian Logistic Regression model on the input data with a Cauchy prior with the provided Link function.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> turnout = dataset(\"Zelig\", \"turnout\")\n2000×5 DataFrame\n  Row │ Race   Age    Educate  Income   Vote  \n      │ Cat…   Int32  Float64  Float64  Int32 \n──────┼───────────────────────────────────────\n    1 │ white     60     14.0   3.3458      1\n    2 │ white     51     10.0   1.8561      0\n    3 │ white     24     12.0   0.6304      0\n    4 │ white     38      8.0   3.4183      1\n  ⋮   │   ⋮      ⋮       ⋮        ⋮       ⋮\n 1998 │ white     51     16.0   7.8949      1\n 1999 │ white     22     10.0   2.4811      0\n 2000 │ white     59     10.0   0.5523      0\n                             1993 rows omitted\njulia> container_logit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Logit(), Prior_Cauchy())\n┌ Info: Found initial step size\n└   ϵ = 0.003125\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 152.61 seconds\nCompute duration  = 152.61 seconds\nparameters        = λ, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    0.0792    0.0857     0.0009    0.0012   4663.0871    0.9999       30.5560\n        β[1]    0.0052    0.0022     0.0000    0.0000   7200.5641    0.9999       47.1834\n        β[2]   -0.0205    0.0797     0.0008    0.0013   4355.2582    0.9999       28.5389\n        β[3]    0.1653    0.0256     0.0003    0.0003   4895.3528    1.0004       32.0780\n        β[4]    0.0306    0.0117     0.0001    0.0002   3982.8457    1.0001       26.0985\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.0092    0.0297    0.0545    0.0981    0.2908\n        β[1]    0.0010    0.0038    0.0052    0.0067    0.0094\n        β[2]   -0.2124   -0.0533   -0.0088    0.0194    0.1293\n        β[3]    0.1153    0.1481    0.1652    0.1822    0.2164\n        β[4]    0.0080    0.0227    0.0304    0.0384    0.0537\njulia> container_probit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Probit(), Prior_Cauchy())\n┌ Info: Found initial step size\n└   ϵ = 0.00078125\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 188.49 seconds\nCompute duration  = 188.49 seconds\nparameters        = λ, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    0.0484    0.0512     0.0005    0.0007   5054.3994    1.0000       26.8155\n        β[1]    0.0034    0.0013     0.0000    0.0000   8376.0026    0.9999       44.4379\n        β[2]   -0.0101    0.0470     0.0005    0.0007   3497.1991    1.0000       18.5540\n        β[3]    0.0927    0.0142     0.0001    0.0002   5007.2301    1.0000       26.5652\n        β[4]    0.0202    0.0070     0.0001    0.0001   4277.4390    0.9999       22.6934\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.0057    0.0185    0.0335    0.0599    0.1824\n        β[1]    0.0009    0.0025    0.0034    0.0042    0.0059\n        β[2]   -0.1236   -0.0297   -0.0045    0.0135    0.0783\n        β[3]    0.0649    0.0830    0.0927    0.1021    0.1207\n        β[4]    0.0068    0.0155    0.0202    0.0249    0.0343\njulia> container_cloglog = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cloglog(), Prior_Cauchy())\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 121.33 seconds\nCompute duration  = 121.33 seconds\nparameters        = λ, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse       ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64   Float64   Float64       Float64 \n\n           λ    0.2562    0.0001     0.0000    0.0000   24.5965    1.0444        0.2027\n        β[1]    0.0457    0.0000     0.0000    0.0000   30.9386    1.3997        0.2550\n        β[2]    0.3084    0.0008     0.0000    0.0001   20.7337    1.6095        0.1709\n        β[3]    0.0931    0.0023     0.0000    0.0002   20.2943    2.5468        0.1673\n        β[4]   -1.3797    0.0072     0.0001    0.0007   20.2381    2.7801        0.1668\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.2561    0.2562    0.2562    0.2563    0.2564\n        β[1]    0.0456    0.0457    0.0457    0.0457    0.0457\n        β[2]    0.3069    0.3077    0.3083    0.3089    0.3100\n        β[3]    0.0893    0.0912    0.0933    0.0948    0.0971\n        β[4]   -1.3910   -1.3863   -1.3800   -1.3733   -1.3681\njulia> container_cauchit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cauchit(), Prior_Cauchy())\n┌ Info: Found initial step size\n└   ϵ = 0.05\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 169.7 seconds\nCompute duration  = 169.7 seconds\nparameters        = λ, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    0.0759    0.0909     0.0009    0.0014   3905.2459    0.9999       23.0129\n        β[1]    0.0033    0.0023     0.0000    0.0000   5465.8674    1.0000       32.2094\n        β[2]   -0.0318    0.0969     0.0010    0.0015   4113.9104    0.9999       24.2425\n        β[3]    0.2285    0.0364     0.0004    0.0007   2716.3177    1.0006       16.0068\n        β[4]    0.0158    0.0124     0.0001    0.0002   3167.8247    1.0000       18.6674\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.0050    0.0211    0.0451    0.0970    0.3174\n        β[1]   -0.0011    0.0018    0.0033    0.0048    0.0078\n        β[2]   -0.2849   -0.0621   -0.0083    0.0151    0.1277\n        β[3]    0.1586    0.2033    0.2276    0.2529    0.3004\n        β[4]   -0.0067    0.0069    0.0153    0.0239    0.0415\n\n\n\n\n\n","category":"function"},{"location":"api/bayesian_regression/#CRRao.fit-9","page":"Bayesian Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::CRRaoLink, prior::Prior_TDist, h::Float64 = 1.0, level::Float64 = 0.95, sim_size::Int64 = 1000)\n\nFit a Bayesian Logistic Regression model on the input data with a T-Dist prior with the provided Link function.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> turnout = dataset(\"Zelig\", \"turnout\")\n2000×5 DataFrame\n  Row │ Race   Age    Educate  Income   Vote  \n      │ Cat…   Int32  Float64  Float64  Int32 \n──────┼───────────────────────────────────────\n    1 │ white     60     14.0   3.3458      1\n    2 │ white     51     10.0   1.8561      0\n    3 │ white     24     12.0   0.6304      0\n    4 │ white     38      8.0   3.4183      1\n  ⋮   │   ⋮      ⋮       ⋮        ⋮       ⋮\n 1998 │ white     51     16.0   7.8949      1\n 1999 │ white     22     10.0   2.4811      0\n 2000 │ white     59     10.0   0.5523      0\n                             1993 rows omitted\njulia> container_logit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Logit(), Prior_TDist())\n┌ Info: Found initial step size\n└   ϵ = 0.003125\nChains MCMC chain (10000×18×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 191.64 seconds\nCompute duration  = 191.64 seconds\nparameters        = λ, ν, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean        std   naive_se      mcse          ess      rhat   ess_per_sec \n      Symbol   Float64    Float64    Float64   Float64      Float64   Float64       Float64 \n\n           λ    0.3043     0.1678     0.0017    0.0022    5712.7538    1.0000       29.8104\n           ν   27.2612   549.4979     5.4950    7.5617    5110.0078    1.0002       26.6652\n        β[1]    0.0052     0.0023     0.0000    0.0000   11260.6563    1.0004       58.7607\n        β[2]   -0.0589     0.1247     0.0012    0.0012    8832.9112    1.0000       46.0921\n        β[3]    0.1667     0.0255     0.0003    0.0003    8308.4832    1.0000       43.3555\n        β[4]    0.0333     0.0123     0.0001    0.0001    7630.2720    0.9999       39.8165\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.1211    0.1958    0.2627    0.3635    0.7244\n           ν    0.5077    1.4402    2.9883    7.2135   79.1950\n        β[1]    0.0008    0.0037    0.0052    0.0067    0.0097\n        β[2]   -0.3138   -0.1399   -0.0546    0.0244    0.1796\n        β[3]    0.1177    0.1491    0.1665    0.1841    0.2177\n        β[4]    0.0094    0.0251    0.0334    0.0414    0.0575\njulia> container_probit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Probit(), Prior_TDist())\n┌ Info: Found initial step size\n└   ϵ = 0.00078125\nChains MCMC chain (10000×18×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 262.37 seconds\nCompute duration  = 262.37 seconds\nparameters        = λ, ν, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean        std   naive_se      mcse          ess      rhat   ess_per_sec \n      Symbol   Float64    Float64    Float64   Float64      Float64   Float64       Float64 \n\n           λ    0.2694     0.1551     0.0016    0.0020    5091.0519    1.0002       19.4040\n           ν   21.1549   329.1679     3.2917    4.5051    5189.9802    1.0000       19.7811\n        β[1]    0.0034     0.0013     0.0000    0.0000   12584.4962    0.9999       47.9645\n        β[2]   -0.0331     0.0779     0.0008    0.0009    6959.6501    0.9999       26.5260\n        β[3]    0.0936     0.0138     0.0001    0.0002    6043.0747    1.0012       23.0326\n        β[4]    0.0218     0.0072     0.0001    0.0001    5938.2251    1.0010       22.6329\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.1104    0.1761    0.2314    0.3177    0.6395\n           ν    0.4766    1.3862    2.9031    6.9434   79.8040\n        β[1]    0.0008    0.0025    0.0034    0.0043    0.0060\n        β[2]   -0.1901   -0.0841   -0.0325    0.0184    0.1189\n        β[3]    0.0671    0.0843    0.0935    0.1027    0.1210\n        β[4]    0.0077    0.0169    0.0217    0.0267    0.0361\njulia> container_cloglog = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cloglog(), Prior_TDist())\n┌ Info: Found initial step size\n└   ϵ = 0.0015625\nChains MCMC chain (10000×18×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 241.88 seconds\nCompute duration  = 241.88 seconds\nparameters        = λ, ν, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean        std   naive_se      mcse          ess      rhat   ess_per_sec \n      Symbol   Float64    Float64    Float64   Float64      Float64   Float64       Float64 \n\n           λ    0.2705     0.1490     0.0015    0.0018    6373.8367    1.0001       26.3518\n           ν   25.1429   513.9686     5.1397    7.9414    4083.5546    1.0000       16.8829\n        β[1]    0.0010     0.0012     0.0000    0.0000   11899.8637    0.9999       49.1984\n        β[2]   -0.0562     0.0693     0.0007    0.0009    6611.6159    0.9999       27.3348\n        β[3]    0.0774     0.0115     0.0001    0.0001    6350.5188    0.9999       26.2554\n        β[4]    0.0081     0.0066     0.0001    0.0001    5974.2918    1.0000       24.6999\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.1079    0.1732    0.2318    0.3236    0.6667\n           ν    0.4629    1.3747    2.7811    6.9236   88.1070\n        β[1]   -0.0014    0.0001    0.0010    0.0018    0.0034\n        β[2]   -0.1960   -0.1021   -0.0563   -0.0089    0.0781\n        β[3]    0.0549    0.0696    0.0775    0.0851    0.0996\n        β[4]   -0.0050    0.0036    0.0081    0.0126    0.0209\njulia> container_cauchit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cauchit(), Prior_TDist())\n┌ Info: Found initial step size\n└   ϵ = 0.009375000000000001\nChains MCMC chain (10000×18×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 224.46 seconds\nCompute duration  = 224.46 seconds\nparameters        = λ, ν, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean        std   naive_se      mcse          ess      rhat   ess_per_sec \n      Symbol   Float64    Float64    Float64   Float64      Float64   Float64       Float64 \n\n           λ    0.3293     0.1847     0.0018    0.0022    6475.4472    1.0000       28.8487\n           ν   16.2524   148.5657     1.4857    2.0404    5495.7033    1.0003       24.4839\n        β[1]    0.0036     0.0025     0.0000    0.0000   10948.8161    0.9999       48.7780\n        β[2]   -0.1076     0.1519     0.0015    0.0016    7435.6058    1.0000       33.1263\n        β[3]    0.2321     0.0361     0.0004    0.0004    7391.3297    0.9999       32.9291\n        β[4]    0.0198     0.0133     0.0001    0.0002    6891.4114    1.0000       30.7019\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.1305    0.2117    0.2828    0.3905    0.7982\n           ν    0.5166    1.4512    2.9337    6.9187   79.9496\n        β[1]   -0.0011    0.0019    0.0036    0.0052    0.0087\n        β[2]   -0.4347   -0.2049   -0.0983   -0.0031    0.1670\n        β[3]    0.1629    0.2075    0.2313    0.2560    0.3057\n        β[4]   -0.0058    0.0106    0.0196    0.0287    0.0461\n\n\n\n\n\n","category":"function"},{"location":"api/bayesian_regression/#CRRao.fit-10","page":"Bayesian Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::CRRaoLink, prior::Prior_Uniform, h::Float64 = 0.01, level::Float64 = 0.95, sim_size::Int64 = 1000)\n\nFit a Bayesian Logistic Regression model on the input data with a Uniform prior with the provided Link function.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> turnout = dataset(\"Zelig\", \"turnout\")\n2000×5 DataFrame\n  Row │ Race   Age    Educate  Income   Vote  \n      │ Cat…   Int32  Float64  Float64  Int32 \n──────┼───────────────────────────────────────\n    1 │ white     60     14.0   3.3458      1\n    2 │ white     51     10.0   1.8561      0\n    3 │ white     24     12.0   0.6304      0\n    4 │ white     38      8.0   3.4183      1\n  ⋮   │   ⋮      ⋮       ⋮        ⋮       ⋮\n 1998 │ white     51     16.0   7.8949      1\n 1999 │ white     22     10.0   2.4811      0\n 2000 │ white     59     10.0   0.5523      0\n                             1993 rows omitted\njulia> container_logit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Logit(), Prior_Uniform())\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 189.43 seconds\nCompute duration  = 189.43 seconds\nparameters        = v, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters                                                                                                             ⋯\n      Symbol                                                                                                             ⋯\n\n           v   675917680092296823408089342391941239673271191525001008413719254637203390096714276526402253793438558348179 ⋯\n        β[1]                                                                                                             ⋯\n        β[2]                                                                                                             ⋯\n        β[3]                                                                                                             ⋯\n        β[4]                                                                                                             ⋯\n                                                                                                         7 columns omitted\n\nQuantiles\n  parameters      2.5%                25.0%                                  50.0%                                       ⋯\n      Symbol   Float64              Float64                                Float64                                       ⋯\n\n           v   10.0068   6671382021570.9727   2875917206819862279706875265024.0000   10765098578457618185304163237787764 ⋯\n        β[1]    0.0025               0.0052                                 0.0066                                       ⋯\n        β[2]   -0.2792              -0.2792                                -0.2792                                       ⋯\n        β[3]    0.1295               0.1615                                 0.1791                                       ⋯\n        β[4]    0.0180               0.0327                                 0.0399                                       ⋯\n                                                                                                         2 columns omitted\njulia> container_probit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Probit(), Prior_Uniform())\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 271.82 seconds\nCompute duration  = 271.82 seconds\nparameters        = v, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters                                                                                                             ⋯\n      Symbol                                                                                                             ⋯\n\n           v   315982558180625088687517999180569534700873081964494331646387645413807923241067841219325555242963795532234 ⋯\n        β[1]                                                                                                             ⋯\n        β[2]                                                                                                             ⋯\n        β[3]                                                                                                             ⋯\n        β[4]                                                                                                             ⋯\n                                                                                                         7 columns omitted\n\nQuantiles\n  parameters      2.5%               25.0%                                 50.0%                                         ⋯\n      Symbol   Float64             Float64                               Float64                                         ⋯\n\n           v    2.9888   500027163480.9627   114267416620826600088306450432.0000   8731986448381257740408362721640235131 ⋯\n        β[1]    0.0008              0.0025                                0.0033                                         ⋯\n        β[2]   -0.2059             -0.0982                               -0.0400                                         ⋯\n        β[3]    0.0668              0.0850                                0.0941                                         ⋯\n        β[4]    0.0078              0.0169                                0.0220                                         ⋯\n                                                                                                         2 columns omitted\njulia> container_cloglog = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cloglog(), Prior_Uniform())\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 221.98 seconds\nCompute duration  = 221.98 seconds\nparameters        = v, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters                                                                                                             ⋯\n      Symbol                                                                                                             ⋯\n\n           v   997108327601492157485369149438354840181147942886188650973018774473864498882782024293920677223471145239174 ⋯\n        β[1]                                                                                                             ⋯\n        β[2]                                                                                                             ⋯\n        β[3]                                                                                                             ⋯\n        β[4]                                                                                                             ⋯\n                                                                                                         7 columns omitted\n\nQuantiles\n  parameters      2.5%                25.0%                                50.0%                                         ⋯\n      Symbol   Float64              Float64                              Float64                                         ⋯\n\n           v    2.9344   1617934162465.7087   79417083014024744675909304320.0000   6335481071385452850562280409696628675 ⋯\n        β[1]   -0.0013               0.0002                               0.0010                                         ⋯\n        β[2]   -0.2056              -0.1137                              -0.0639                                         ⋯\n        β[3]    0.0555               0.0705                               0.0779                                         ⋯\n        β[4]   -0.0042               0.0036                               0.0079                                         ⋯\n                                                                                                         2 columns omitted\njulia> container_cauchit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cauchit(), Prior_Uniform())\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 250.81 seconds\nCompute duration  = 250.81 seconds\nparameters        = v, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters                                                                                                             ⋯\n      Symbol                                                                                                             ⋯\n\n           v   145105825023746239211260804740935487372396420958652923206621612953582232010367942920611301369886999826663 ⋯\n        β[1]                                                                                                             ⋯\n        β[2]                                                                                                             ⋯\n        β[3]                                                                                                             ⋯\n        β[4]                                                                                                             ⋯\n                                                                                                         7 columns omitted\n\nQuantiles\n  parameters      2.5%               25.0%                                50.0%                                          ⋯\n      Symbol   Float64             Float64                              Float64                                          ⋯\n\n           v   10.8874   371822401390.3905   15665245298723267168052445184.0000   97091064597143721776615147479481412134 ⋯\n        β[1]   -0.0009              0.0022                               0.0039                                          ⋯\n        β[2]   -0.5461             -0.2897                              -0.1591                                          ⋯\n        β[3]    0.1679              0.2138                               0.2371                                          ⋯\n        β[4]   -0.0047              0.0122                               0.0212                                          ⋯\n                                                                                                         2 columns omitted\n\n\n\n\n\n","category":"function"},{"location":"api/bayesian_regression/#Negative-Binomial-Regression","page":"Bayesian Regression Models","title":"Negative Binomial Regression","text":"","category":"section"},{"location":"api/bayesian_regression/","page":"Bayesian Regression Models","title":"Bayesian Regression Models","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::NegBinomRegression, prior::Prior_Ridge, h::Float64 = 0.1, sim_size::Int64 = 1000)\nfit(formula::FormulaTerm, data::DataFrame, modelClass::NegBinomRegression, prior::Prior_Laplace, h::Float64 = 0.01, sim_size::Int64 = 1000)\nfit(formula::FormulaTerm, data::DataFrame, modelClass::NegBinomRegression, prior::Prior_Cauchy, h::Float64 = 1.0, sim_size::Int64 = 1000)\nfit(formula::FormulaTerm, data::DataFrame, modelClass::NegBinomRegression, prior::Prior_TDist, h::Float64 = 1.0, sim_size::Int64 = 1000)\nfit(formula::FormulaTerm, data::DataFrame, modelClass::NegBinomRegression, prior::Prior_Uniform, h::Float64 = 0.1, sim_size::Int64 = 1000)","category":"page"},{"location":"api/bayesian_regression/#CRRao.fit-11","page":"Bayesian Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::NegBinomRegression, prior::Prior_Ridge, h::Float64 = 0.1, sim_size::Int64 = 1000)\n\nFit a Bayesian Negative Binomial Regression model on the input data with a Ridge prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> sanction = dataset(\"Zelig\", \"sanction\")\n78×8 DataFrame\n Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         \n     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          \n─────┼───────────────────────────────────────────────────────────────────\n   1 │     1      4       3       1       1      4     15  major loss\n   2 │     0      2       3       0       1      3      4  modest loss\n   3 │     0      1       3       1       0      2      1  little effect\n   4 │     1      1       3       1       1      2      1  little effect\n  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮\n  76 │     0      4       3       1       0      2     13  little effect\n  77 │     0      1       2       0       0      1      1  net gain\n  78 │     1      3       1       1       1      2     10  little effect\n                                                          71 rows omitted\njulia> container = fit(@formula(Num ~ Target + Coop + NCost), sanction, NegBinomRegression(), Prior_Ridge())\n┌ Info: Found initial step size\n└   ϵ = 0.025\nChains MCMC chain (10000×19×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 26.52 seconds\nCompute duration  = 26.52 seconds\nparameters        = λ, α, β[1], β[2], β[3], β[4], β[5]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    2.0416    0.4460     0.0045    0.0045   8499.3498    0.9999      320.5246\n           α   -1.0792    0.5148     0.0051    0.0089   3405.4069    1.0010      128.4235\n        β[1]   -0.0049    0.1614     0.0016    0.0023   4627.1117    1.0009      174.4960\n        β[2]    1.0615    0.1319     0.0013    0.0020   5046.9022    1.0001      190.3270\n        β[3]   -0.1757    0.5563     0.0056    0.0063   8056.2338    1.0001      303.8139\n        β[4]    1.2810    0.3214     0.0032    0.0035   6779.1552    0.9999      255.6532\n        β[5]    0.1493    0.2799     0.0028    0.0036   6164.9114    1.0004      232.4890\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    1.3159    1.7243    1.9928    2.3049    3.0445\n           α   -2.0865   -1.4300   -1.0908   -0.7306   -0.0721\n        β[1]   -0.3180   -0.1136   -0.0044    0.1053    0.3146\n        β[2]    0.8046    0.9738    1.0594    1.1483    1.3262\n        β[3]   -1.2332   -0.5561   -0.1992    0.2020    0.9502\n        β[4]    0.6571    1.0654    1.2744    1.4900    1.9274\n        β[5]   -0.4064   -0.0370    0.1501    0.3388    0.6903\n\n\n\n\n\n","category":"function"},{"location":"api/bayesian_regression/#CRRao.fit-12","page":"Bayesian Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::NegBinomRegression, prior::Prior_Laplace, h::Float64 = 0.01, sim_size::Int64 = 1000)\n\nFit a Bayesian Negative Binomial Regression model on the input data with a Laplace prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> sanction = dataset(\"Zelig\", \"sanction\")\n78×8 DataFrame\n Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         \n     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          \n─────┼───────────────────────────────────────────────────────────────────\n   1 │     1      4       3       1       1      4     15  major loss\n   2 │     0      2       3       0       1      3      4  modest loss\n   3 │     0      1       3       1       0      2      1  little effect\n   4 │     1      1       3       1       1      2      1  little effect\n  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮\n  76 │     0      4       3       1       0      2     13  little effect\n  77 │     0      1       2       0       0      1      1  net gain\n  78 │     1      3       1       1       1      2     10  little effect\n                                                          71 rows omitted\njulia> container = fit(@formula(Num ~ Target + Coop + NCost), sanction, NegBinomRegression(), Prior_Laplace())\n┌ Info: Found initial step size\n└   ϵ = 0.05\nChains MCMC chain (10000×19×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 26.96 seconds\nCompute duration  = 26.96 seconds\nparameters        = λ, α, β[1], β[2], β[3], β[4], β[5]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    2.1058    0.4611     0.0046    0.0052   8213.6672    0.9999      304.6048\n           α   -1.0014    0.5020     0.0050    0.0084   3465.0499    1.0000      128.5018\n        β[1]   -0.0207    0.1583     0.0016    0.0021   5223.4434    0.9999      193.7120\n        β[2]    1.0465    0.1301     0.0013    0.0017   5029.9415    1.0000      186.5359\n        β[3]   -0.1426    0.4996     0.0050    0.0057   7487.9201    0.9999      277.6903\n        β[4]    1.2832    0.3245     0.0032    0.0035   6912.6238    0.9999      256.3554\n        β[5]    0.1198    0.2656     0.0027    0.0039   5505.7699    1.0000      204.1821\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    1.3431    1.7782    2.0523    2.3788    3.1662\n           α   -2.0082   -1.3266   -1.0000   -0.6730   -0.0202\n        β[1]   -0.3373   -0.1240   -0.0190    0.0823    0.2921\n        β[2]    0.7927    0.9595    1.0454    1.1337    1.3056\n        β[3]   -1.1412   -0.4702   -0.1379    0.1801    0.8557\n        β[4]    0.6480    1.0707    1.2824    1.4966    1.9203\n        β[5]   -0.4026   -0.0558    0.1158    0.2980    0.6499\n\n\n\n\n\n","category":"function"},{"location":"api/bayesian_regression/#CRRao.fit-13","page":"Bayesian Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::NegBinomRegression, prior::Prior_Cauchy, h::Float64 = 1.0, sim_size::Int64 = 1000)\n\nFit a Bayesian Negative Binomial Regression model on the input data with a Cauchy prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> sanction = dataset(\"Zelig\", \"sanction\")\n78×8 DataFrame\n Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         \n     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          \n─────┼───────────────────────────────────────────────────────────────────\n   1 │     1      4       3       1       1      4     15  major loss\n   2 │     0      2       3       0       1      3      4  modest loss\n   3 │     0      1       3       1       0      2      1  little effect\n   4 │     1      1       3       1       1      2      1  little effect\n  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮\n  76 │     0      4       3       1       0      2     13  little effect\n  77 │     0      1       2       0       0      1      1  net gain\n  78 │     1      3       1       1       1      2     10  little effect\n                                                          71 rows omitted\njulia> container = fit(@formula(Num ~ Target + Coop + NCost), sanction, NegBinomRegression(), Prior_Cauchy())\n┌ Info: Found initial step size\n└   ϵ = 0.2\nChains MCMC chain (10000×19×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 27.58 seconds\nCompute duration  = 27.58 seconds\nparameters        = λ, α, β[1], β[2], β[3], β[4], β[5]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    2.0219    0.4304     0.0043    0.0047   7839.1614    1.0001      284.1923\n           α   -1.0233    0.5192     0.0052    0.0091   3193.5541    1.0010      115.7756\n        β[1]   -0.0192    0.1632     0.0016    0.0025   4320.9927    1.0006      156.6485\n        β[2]    1.0535    0.1327     0.0013    0.0021   4739.9448    1.0008      171.8367\n        β[3]   -0.1552    0.5453     0.0055    0.0069   7763.7273    1.0002      281.4576\n        β[4]    1.2743    0.3250     0.0032    0.0041   6655.6093    1.0008      241.2851\n        β[5]    0.1298    0.2822     0.0028    0.0036   5253.2578    1.0000      190.4458\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    1.3226    1.7126    1.9731    2.2757    2.9804\n           α   -2.0538   -1.3647   -1.0180   -0.6733   -0.0207\n        β[1]   -0.3375   -0.1285   -0.0189    0.0881    0.3042\n        β[2]    0.8001    0.9647    1.0516    1.1418    1.3138\n        β[3]   -1.1825   -0.5301   -0.1676    0.2010    0.9589\n        β[4]    0.6478    1.0553    1.2704    1.4870    1.9319\n        β[5]   -0.4131   -0.0613    0.1305    0.3166    0.6901\n\n\n\n\n\n","category":"function"},{"location":"api/bayesian_regression/#CRRao.fit-14","page":"Bayesian Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::NegBinomRegression, prior::Prior_TDist, h::Float64 = 1.0, sim_size::Int64 = 1000)\n\nFit a Bayesian Negative Binomial Regression model on the input data with a t(ν) distributed prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> sanction = dataset(\"Zelig\", \"sanction\")\n78×8 DataFrame\n Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         \n     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          \n─────┼───────────────────────────────────────────────────────────────────\n   1 │     1      4       3       1       1      4     15  major loss\n   2 │     0      2       3       0       1      3      4  modest loss\n   3 │     0      1       3       1       0      2      1  little effect\n   4 │     1      1       3       1       1      2      1  little effect\n  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮\n  76 │     0      4       3       1       0      2     13  little effect\n  77 │     0      1       2       0       0      1      1  net gain\n  78 │     1      3       1       1       1      2     10  little effect\n                                                          71 rows omitted\njulia> container = fit(@formula(Num ~ Target + Coop + NCost), sanction, NegBinomRegression(), Prior_TDist())\n┌ Info: Found initial step size\n└   ϵ = 0.05\nChains MCMC chain (10000×20×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 32.4 seconds\nCompute duration  = 32.4 seconds\nparameters        = λ, ν, α, β[1], β[2], β[3], β[4], β[5]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean        std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64    Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    2.0021     0.4262     0.0043    0.0048   7946.6182    0.9999      245.2887\n           ν   20.4978   213.5274     2.1353    2.7473   6455.1193    0.9999      199.2505\n           α   -1.0562     0.5154     0.0052    0.0076   4162.0565    1.0010      128.4704\n        β[1]   -0.0096     0.1617     0.0016    0.0022   5232.3275    1.0005      161.5065\n        β[2]    1.0581     0.1308     0.0013    0.0016   5850.3314    1.0004      180.5825\n        β[3]   -0.1725     0.5396     0.0054    0.0056   7961.1718    0.9999      245.7379\n        β[4]    1.2762     0.3222     0.0032    0.0036   7541.0855    0.9999      232.7711\n        β[5]    0.1400     0.2822     0.0028    0.0037   6538.0847    1.0009      201.8114\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%      97.5% \n      Symbol   Float64   Float64   Float64   Float64    Float64 \n\n           λ    1.2972    1.6983    1.9536    2.2528     2.9704\n           ν    0.6529    1.9144    3.8831    9.7303   108.9970\n           α   -2.0694   -1.4014   -1.0565   -0.7076    -0.0696\n        β[1]   -0.3338   -0.1176   -0.0090    0.0979     0.3056\n        β[2]    0.8046    0.9695    1.0576    1.1482     1.3157\n        β[3]   -1.2170   -0.5366   -0.1879    0.1806     0.9140\n        β[4]    0.6519    1.0551    1.2762    1.4910     1.9133\n        β[5]   -0.4045   -0.0511    0.1377    0.3314     0.6957\n\n\n\n\n\n","category":"function"},{"location":"api/bayesian_regression/#CRRao.fit-15","page":"Bayesian Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::NegBinomRegression, prior::Prior_Uniform, h::Float64 = 0.1, sim_size::Int64 = 1000)\n\nFit a Bayesian Negative Binomial Regression model on the input data with a Uniform prior. Ibrahim and Laud (JASA, 1990) showed that the uniform flat priors for GLMs can lead to improper posterior distributions thus making them undesirable. In such cases, the Markov Chain struggles to converge. Even if it converges, results are unreliable.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> sanction = dataset(\"Zelig\", \"sanction\")\n78×8 DataFrame\n Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         \n     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          \n─────┼───────────────────────────────────────────────────────────────────\n   1 │     1      4       3       1       1      4     15  major loss\n   2 │     0      2       3       0       1      3      4  modest loss\n   3 │     0      1       3       1       0      2      1  little effect\n   4 │     1      1       3       1       1      2      1  little effect\n  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮\n  76 │     0      4       3       1       0      2     13  little effect\n  77 │     0      1       2       0       0      1      1  net gain\n  78 │     1      3       1       1       1      2     10  little effect\n                                                          71 rows omitted\njulia> container = fit(@formula(Num ~ Target + Coop + NCost), sanction, NegBinomRegression(), Prior_Uniform())\nChains MCMC chain (10000×19×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 294.79 seconds\nCompute duration  = 294.79 seconds\nparameters        = λ, α, β[1], β[2], β[3], β[4], β[5]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    0.9182    0.1461     0.0015    0.0016   9715.0842    1.0003       32.9562\n           α    0.2792    0.0000     0.0000    0.0000     20.5530    0.9999        0.0697\n        β[1]    0.2792    0.0000     0.0000    0.0000     20.5530    0.9999        0.0697\n        β[2]    0.2792    0.0000     0.0000    0.0000     20.5530    0.9999        0.0697\n        β[3]   -0.1238    0.2502     0.0025    0.0234     24.2396    1.5013        0.0822\n        β[4]    0.2792    0.0000     0.0000    0.0000     20.5530    0.9999        0.0697\n        β[5]   -0.2643    0.0899     0.0009    0.0075     81.7627    1.0276        0.2774\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.6696    0.8140    0.9070    1.0073    1.2411\n           α    0.2792    0.2792    0.2792    0.2792    0.2792\n        β[1]    0.2792    0.2792    0.2792    0.2792    0.2792\n        β[2]    0.2792    0.2792    0.2792    0.2792    0.2792\n        β[3]   -0.2792   -0.2792   -0.2792    0.2792    0.2792\n        β[4]    0.2792    0.2792    0.2792    0.2792    0.2792\n        β[5]   -0.2792   -0.2792   -0.2792   -0.2792    0.2792\n\n\n\n\n\n","category":"function"},{"location":"api/bayesian_regression/#Poisson-Regression","page":"Bayesian Regression Models","title":"Poisson Regression","text":"","category":"section"},{"location":"api/bayesian_regression/","page":"Bayesian Regression Models","title":"Bayesian Regression Models","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::PoissonRegression, prior::Prior_Ridge, h::Float64 = 0.1, sim_size::Int64 = 1000)\nfit(formula::FormulaTerm, data::DataFrame, modelClass::PoissonRegression, prior::Prior_Laplace, h::Float64 = 0.1, sim_size::Int64 = 1000)\nfit(formula::FormulaTerm, data::DataFrame, modelClass::PoissonRegression, prior::Prior_Cauchy, h::Float64 = 1.0, sim_size::Int64 = 1000)\nfit(formula::FormulaTerm, data::DataFrame, modelClass::PoissonRegression, prior::Prior_TDist, h::Float64 = 2.0, sim_size::Int64 = 1000)\nfit(formula::FormulaTerm, data::DataFrame, modelClass::PoissonRegression, prior::Prior_Uniform, h::Float64 = 1.0, sim_size::Int64 = 1000)","category":"page"},{"location":"api/bayesian_regression/#CRRao.fit-16","page":"Bayesian Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::PoissonRegression, prior::Prior_Ridge, h::Float64 = 0.1, sim_size::Int64 = 1000)\n\nFit a Bayesian Poisson Regression model on the input data with a Ridge prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> sanction = dataset(\"Zelig\", \"sanction\")\n78×8 DataFrame\n Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         \n     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          \n─────┼───────────────────────────────────────────────────────────────────\n   1 │     1      4       3       1       1      4     15  major loss\n   2 │     0      2       3       0       1      3      4  modest loss\n   3 │     0      1       3       1       0      2      1  little effect\n   4 │     1      1       3       1       1      2      1  little effect\n  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮\n  76 │     0      4       3       1       0      2     13  little effect\n  77 │     0      1       2       0       0      1      1  net gain\n  78 │     1      3       1       1       1      2     10  little effect\n                                                          71 rows omitted\njulia> container = fit(@formula(Num ~ Target + Coop + NCost), sanction, PoissonRegression(), Prior_Ridge())\n┌ Info: Found initial step size\n└   ϵ = 0.025\nChains MCMC chain (10000×19×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 28.3 seconds\nCompute duration  = 28.3 seconds\nparameters        = λ, α, β[1], β[2], β[3], β[4], β[5]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    1.3118    0.4894     0.0049    0.0066   5733.9828    1.0000      202.6214\n           α   -1.8003    0.2607     0.0026    0.0038   4247.2367    1.0000      150.0843\n        β[1]    0.1392    0.0656     0.0007    0.0008   5949.9827    1.0000      210.2542\n        β[2]    1.1334    0.0563     0.0006    0.0007   5344.6101    1.0003      188.8622\n        β[3]   -0.3259    0.2281     0.0023    0.0026   7065.4440    0.9999      249.6712\n        β[4]    1.6983    0.0988     0.0010    0.0012   6534.2641    1.0001      230.9009\n        β[5]    0.4053    0.1688     0.0017    0.0023   5330.2762    1.0006      188.3556\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.7113    0.9826    1.2040    1.5098    2.5620\n           α   -2.3202   -1.9764   -1.7971   -1.6229   -1.3003\n        β[1]    0.0115    0.0950    0.1399    0.1825    0.2690\n        β[2]    1.0246    1.0950    1.1331    1.1712    1.2451\n        β[3]   -0.7923   -0.4776   -0.3205   -0.1703    0.1022\n        β[4]    1.5095    1.6308    1.6977    1.7645    1.8936\n        β[5]    0.0755    0.2930    0.4068    0.5190    0.7331\n\n\n\n\n\n","category":"function"},{"location":"api/bayesian_regression/#CRRao.fit-17","page":"Bayesian Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::PoissonRegression, prior::Prior_Laplace, h::Float64 = 0.1, sim_size::Int64 = 1000)\n\nFit a Bayesian Poisson Regression model on the input data with a Laplace prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> sanction = dataset(\"Zelig\", \"sanction\")\n78×8 DataFrame\n Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         \n     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          \n─────┼───────────────────────────────────────────────────────────────────\n   1 │     1      4       3       1       1      4     15  major loss\n   2 │     0      2       3       0       1      3      4  modest loss\n   3 │     0      1       3       1       0      2      1  little effect\n   4 │     1      1       3       1       1      2      1  little effect\n  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮\n  76 │     0      4       3       1       0      2     13  little effect\n  77 │     0      1       2       0       0      1      1  net gain\n  78 │     1      3       1       1       1      2     10  little effect\n                                                          71 rows omitted\njulia> container = fit(@formula(Num ~ Target + Coop + NCost), sanction, PoissonRegression(), Prior_Laplace())\n┌ Info: Found initial step size\n└   ϵ = 0.025\nChains MCMC chain (10000×19×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 26.38 seconds\nCompute duration  = 26.38 seconds\nparameters        = λ, α, β[1], β[2], β[3], β[4], β[5]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    1.1036    0.5676     0.0057    0.0086   5101.9856    1.0003      193.4109\n           α   -1.7912    0.2625     0.0026    0.0041   4611.2398    1.0002      174.8072\n        β[1]    0.1360    0.0649     0.0006    0.0008   6345.1220    0.9999      240.5369\n        β[2]    1.1324    0.0561     0.0006    0.0008   6267.6347    1.0006      237.5994\n        β[3]   -0.2965    0.2234     0.0022    0.0027   7304.0984    1.0001      276.8906\n        β[4]    1.7010    0.1012     0.0010    0.0013   7420.3061    0.9999      281.2960\n        β[5]    0.3928    0.1730     0.0017    0.0021   6264.6983    0.9999      237.4881\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.4544    0.7383    0.9620    1.3049    2.5869\n           α   -2.3130   -1.9684   -1.7862   -1.6133   -1.2838\n        β[1]    0.0093    0.0924    0.1354    0.1801    0.2627\n        β[2]    1.0241    1.0943    1.1313    1.1698    1.2448\n        β[3]   -0.7542   -0.4437   -0.2889   -0.1370    0.1132\n        β[4]    1.5029    1.6331    1.6994    1.7690    1.9002\n        β[5]    0.0581    0.2740    0.3946    0.5113    0.7309\n\n\n\n\n\n","category":"function"},{"location":"api/bayesian_regression/#CRRao.fit-18","page":"Bayesian Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression, prior::Prior_Cauchy, h::Float64 = 1.0, sim_size::Int64 = 1000)\n\nFit a Bayesian Poisson Regression model on the input data with a Cauchy prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> sanction = dataset(\"Zelig\", \"sanction\")\n78×8 DataFrame\n Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         \n     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          \n─────┼───────────────────────────────────────────────────────────────────\n   1 │     1      4       3       1       1      4     15  major loss\n   2 │     0      2       3       0       1      3      4  modest loss\n   3 │     0      1       3       1       0      2      1  little effect\n   4 │     1      1       3       1       1      2      1  little effect\n  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮\n  76 │     0      4       3       1       0      2     13  little effect\n  77 │     0      1       2       0       0      1      1  net gain\n  78 │     1      3       1       1       1      2     10  little effect\n                                                          71 rows omitted\njulia> container = fit(@formula(Num ~ Target + Coop + NCost), sanction, PoissonRegression(), Prior_Cauchy())\n┌ Info: Found initial step size\n└   ϵ = 0.025\nChains MCMC chain (10000×19×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 27.23 seconds\nCompute duration  = 27.23 seconds\nparameters        = λ, α, β[1], β[2], β[3], β[4], β[5]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    0.8558    0.4620     0.0046    0.0050   7120.2358    0.9999      261.5138\n           α   -1.7984    0.2622     0.0026    0.0038   4736.5277    0.9999      173.9644\n        β[1]    0.1383    0.0649     0.0006    0.0008   6989.3372    1.0001      256.7061\n        β[2]    1.1322    0.0573     0.0006    0.0008   5442.3181    0.9999      199.8868\n        β[3]   -0.2928    0.2169     0.0022    0.0025   6830.7146    1.0000      250.8802\n        β[4]    1.7040    0.0974     0.0010    0.0011   6738.4680    0.9999      247.4921\n        β[5]    0.3945    0.1673     0.0017    0.0023   5730.9957    0.9999      210.4894\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.2927    0.5424    0.7551    1.0504    1.9964\n           α   -2.3125   -1.9749   -1.7957   -1.6220   -1.2893\n        β[1]    0.0112    0.0950    0.1366    0.1813    0.2677\n        β[2]    1.0198    1.0937    1.1315    1.1709    1.2457\n        β[3]   -0.7403   -0.4351   -0.2887   -0.1398    0.1058\n        β[4]    1.5135    1.6384    1.7053    1.7704    1.8926\n        β[5]    0.0677    0.2823    0.3952    0.5066    0.7253\n\n\n\n\n\n","category":"function"},{"location":"api/bayesian_regression/#CRRao.fit-19","page":"Bayesian Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::PoissonRegression, prior::Prior_TDist, h::Float64 = 2.0, sim_size::Int64 = 1000)\n\nFit a Bayesian Poisson Regression model on the input data with a t(ν) distributed prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> sanction = dataset(\"Zelig\", \"sanction\")\n78×8 DataFrame\n Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         \n     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          \n─────┼───────────────────────────────────────────────────────────────────\n   1 │     1      4       3       1       1      4     15  major loss\n   2 │     0      2       3       0       1      3      4  modest loss\n   3 │     0      1       3       1       0      2      1  little effect\n   4 │     1      1       3       1       1      2      1  little effect\n  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮\n  76 │     0      4       3       1       0      2     13  little effect\n  77 │     0      1       2       0       0      1      1  net gain\n  78 │     1      3       1       1       1      2     10  little effect\n                                                          71 rows omitted\njulia> container = fit(@formula(Num ~ Target + Coop + NCost), sanction, PoissonRegression(), Prior_TDist())\n┌ Info: Found initial step size\n└   ϵ = 0.0125\nChains MCMC chain (10000×20×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 28.44 seconds\nCompute duration  = 28.44 seconds\nparameters        = λ, ν, α, β[1], β[2], β[3], β[4], β[5]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    0.9887    0.4187     0.0042    0.0046   6943.4330    1.0001      244.1861\n           ν    3.0837    7.9963     0.0800    0.1140   4422.8043    1.0000      155.5409\n           α   -1.8065    0.2648     0.0026    0.0042   3384.5428    0.9999      119.0274\n        β[1]    0.1399    0.0656     0.0007    0.0009   5242.0449    1.0001      184.3518\n        β[2]    1.1339    0.0565     0.0006    0.0009   4397.9611    1.0004      154.6672\n        β[3]   -0.3097    0.2208     0.0022    0.0029   5930.8888    1.0000      208.5771\n        β[4]    1.7026    0.1000     0.0010    0.0012   5706.3129    0.9999      200.6792\n        β[5]    0.4025    0.1701     0.0017    0.0024   4239.8288    0.9999      149.1060\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.4024    0.7020    0.9159    1.1889    2.0151\n           ν    0.5755    1.1693    1.8160    3.0737   12.7800\n           α   -2.3383   -1.9843   -1.8024   -1.6222   -1.3155\n        β[1]    0.0116    0.0952    0.1392    0.1836    0.2704\n        β[2]    1.0255    1.0953    1.1331    1.1717    1.2464\n        β[3]   -0.7635   -0.4518   -0.3017   -0.1559    0.1005\n        β[4]    1.5112    1.6334    1.7023    1.7700    1.9025\n        β[5]    0.0680    0.2864    0.4016    0.5174    0.7395\n\n\n\n\n\n","category":"function"},{"location":"api/bayesian_regression/#CRRao.fit-20","page":"Bayesian Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::PoissonRegression, prior::Prior_Uniform, h::Float64 = 1.0, sim_size::Int64 = 1000)\n\nFit a Bayesian Poisson Regression model on the input data with a Uniform prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> sanction = dataset(\"Zelig\", \"sanction\")\n78×8 DataFrame\n Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         \n     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          \n─────┼───────────────────────────────────────────────────────────────────\n   1 │     1      4       3       1       1      4     15  major loss\n   2 │     0      2       3       0       1      3      4  modest loss\n   3 │     0      1       3       1       0      2      1  little effect\n   4 │     1      1       3       1       1      2      1  little effect\n  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮\n  76 │     0      4       3       1       0      2     13  little effect\n  77 │     0      1       2       0       0      1      1  net gain\n  78 │     1      3       1       1       1      2     10  little effect\n                                                          71 rows omitted\njulia> container = fit(@formula(Num ~ Target + Coop + NCost), sanction, PoissonRegression(), Prior_Uniform())\nChains MCMC chain (10000×19×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 226.71 seconds\nCompute duration  = 226.71 seconds\nparameters        = λ, α, β[1], β[2], β[3], β[4], β[5]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse          ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64      Float64   Float64       Float64 \n\n           λ    6.8751   14.2909     0.1429    0.9915     139.5154    1.0000        0.6154\n           α    0.2792    0.0000     0.0000    0.0000      20.5530    0.9999        0.0907\n        β[1]    0.2792    0.0000     0.0000    0.0000      20.5530    0.9999        0.0907\n        β[2]    0.2792    0.0000     0.0000    0.0000      20.5530    0.9999        0.0907\n        β[3]    0.2791    0.0056     0.0001    0.0001   10004.0032    1.0000       44.1261\n        β[4]    0.2792    0.0000     0.0000    0.0000      20.5530    0.9999        0.0907\n        β[5]   -0.2792    0.0000     0.0000    0.0000      20.5530    0.9999        0.0907\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.6280    1.3666    2.6401    6.3038   39.3119\n           α    0.2792    0.2792    0.2792    0.2792    0.2792\n        β[1]    0.2792    0.2792    0.2792    0.2792    0.2792\n        β[2]    0.2792    0.2792    0.2792    0.2792    0.2792\n        β[3]    0.2792    0.2792    0.2792    0.2792    0.2792\n        β[4]    0.2792    0.2792    0.2792    0.2792    0.2792\n        β[5]   -0.2792   -0.2792   -0.2792   -0.2792   -0.2792\n\n\n\n\n\n","category":"function"},{"location":"api/frequentist_regression/#Frequentist-Regression-Models","page":"Frequentist Regression Models","title":"Frequentist Regression Models","text":"","category":"section"},{"location":"api/frequentist_regression/","page":"Frequentist Regression Models","title":"Frequentist Regression Models","text":"FrequentistRegression","category":"page"},{"location":"api/frequentist_regression/#CRRao.FrequentistRegression","page":"Frequentist Regression Models","title":"CRRao.FrequentistRegression","text":"FrequentistRegression{RegressionType}\n\nType to represent frequentist regression models returned by fit functions. This type is used internally by the package to represent all frequentist regression models. RegressionType is a Symbol representing the model class.\n\n\n\n\n\n","category":"type"},{"location":"api/frequentist_regression/#Linear-Regression","page":"Frequentist Regression Models","title":"Linear Regression","text":"","category":"section"},{"location":"api/frequentist_regression/","page":"Frequentist Regression Models","title":"Frequentist Regression Models","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression)","category":"page"},{"location":"api/frequentist_regression/#CRRao.fit-Tuple{FormulaTerm, DataFrame, LinearRegression}","page":"Frequentist Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression)\n\nFit an OLS Linear Regression model on the input data. Uses the lm method from the GLM package under the hood. Returns an object of type FrequentistRegression{:LinearRegression}.\n\nExample\n\njulia> using CRRao, RDatasets, StatsPlots, StatsModels\njulia> df = dataset(\"datasets\", \"mtcars\")\n32×12 DataFrame\n Row │ Model              MPG      Cyl    Disp     HP     DRat     WT       QSec     VS     AM     Gear   Carb  \n     │ String31           Float64  Int64  Float64  Int64  Float64  Float64  Float64  Int64  Int64  Int64  Int64 \n─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ Mazda RX4             21.0      6    160.0    110     3.9     2.62     16.46      0      1      4      4\n   2 │ Mazda RX4 Wag         21.0      6    160.0    110     3.9     2.875    17.02      0      1      4      4\n   3 │ Datsun 710            22.8      4    108.0     93     3.85    2.32     18.61      1      1      4      1\n   4 │ Hornet 4 Drive        21.4      6    258.0    110     3.08    3.215    19.44      1      0      3      1\n   5 │ Hornet Sportabout     18.7      8    360.0    175     3.15    3.44     17.02      0      0      3      2\n   6 │ Valiant               18.1      6    225.0    105     2.76    3.46     20.22      1      0      3      1\n  ⋮  │         ⋮             ⋮       ⋮       ⋮       ⋮       ⋮        ⋮        ⋮       ⋮      ⋮      ⋮      ⋮\n  27 │ Porsche 914-2         26.0      4    120.3     91     4.43    2.14     16.7       0      1      5      2\n  28 │ Lotus Europa          30.4      4     95.1    113     3.77    1.513    16.9       1      1      5      2\n  29 │ Ford Pantera L        15.8      8    351.0    264     4.22    3.17     14.5       0      1      5      4\n  30 │ Ferrari Dino          19.7      6    145.0    175     3.62    2.77     15.5       0      1      5      6\n  31 │ Maserati Bora         15.0      8    301.0    335     3.54    3.57     14.6       0      1      5      8\n  32 │ Volvo 142E            21.4      4    121.0    109     4.11    2.78     18.6       1      1      4      2\n                                                                                                 20 rows omitted\njulia> container = fit(@formula(MPG ~ HP + WT + Gear), df, LinearRegression())\nModel Class: Linear Regression\nLikelihood Mode: Gauss\nLink Function: Identity\nComputing Method: Optimization\n────────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error      t  Pr(>|t|)   Lower 95%   Upper 95%\n────────────────────────────────────────────────────────────────────────────\n(Intercept)  32.0137     4.63226      6.91    <1e-06  22.5249     41.5024\nHP           -0.0367861  0.00989146  -3.72    0.0009  -0.0570478  -0.0165243\nWT           -3.19781    0.846546    -3.78    0.0008  -4.93188    -1.46374\nGear          1.01998    0.851408     1.20    0.2410  -0.72405     2.76401\n────────────────────────────────────────────────────────────────────────────\njulia> coeftable(container)\n────────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error      t  Pr(>|t|)   Lower 95%   Upper 95%\n────────────────────────────────────────────────────────────────────────────\n(Intercept)  32.0137     4.63226      6.91    <1e-06  22.5249     41.5024\nHP           -0.0367861  0.00989146  -3.72    0.0009  -0.0570478  -0.0165243\nWT           -3.19781    0.846546    -3.78    0.0008  -4.93188    -1.46374\nGear          1.01998    0.851408     1.20    0.2410  -0.72405     2.76401\n────────────────────────────────────────────────────────────────────────────\njulia> sigma(container)\n2.5741691724978972\njulia> aic(container)\n157.05277871921942\njulia> predict(container)\n32-element Vector{Float64}:\n 23.668849952338718\n 22.85340824320634\n 25.253556140740894\n 20.746171762311384\n 17.635570543830177\n 20.14663845388644\n 14.644831040166633\n 23.61182872351372\n  ⋮\n 16.340457241090512\n 27.47793682112109\n 26.922715039574857\n 28.11844900519874\n 17.264981908248554\n 21.818065399379595\n 13.374047477198516\n 23.193986311384343\njulia> residuals(container)\n32-element Vector{Float64}:\n -2.668849952338718\n -1.8534082432063386\n -2.4535561407408935\n  0.6538282376886144\n  1.0644294561698224\n -2.0466384538864375\n -0.3448310401666319\n  0.7881712764862776\n  ⋮\n  2.8595427589094875\n -0.1779368211210901\n -0.9227150395748573\n  2.2815509948012576\n -1.4649819082485536\n -2.1180653993795957\n  1.6259525228014837\n -1.7939863113843444\njulia> plot(cooksdistance(container))\n\n\n\n\n\n","category":"method"},{"location":"api/frequentist_regression/#Logistic-Regression","page":"Frequentist Regression Models","title":"Logistic Regression","text":"","category":"section"},{"location":"api/frequentist_regression/","page":"Frequentist Regression Models","title":"Frequentist Regression Models","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::Logit)\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::Probit)\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::Cloglog)\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::Cauchit)","category":"page"},{"location":"api/frequentist_regression/#CRRao.fit-Tuple{FormulaTerm, DataFrame, LogisticRegression, Logit}","page":"Frequentist Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::Logit)\n\nFit a Logistic Regression model on the input data using the Logit link. Uses the glm method from the GLM package under the hood. Returns an object of type FrequentistRegression{:LogisticRegression}.\n\nExample\n\njulia> using CRRao, RDatasets, StatsModels\njulia> turnout = dataset(\"Zelig\", \"turnout\")\n2000×5 DataFrame\n  Row │ Race   Age    Educate  Income   Vote  \n      │ Cat…   Int32  Float64  Float64  Int32 \n──────┼───────────────────────────────────────\n    1 │ white     60     14.0   3.3458      1\n    2 │ white     51     10.0   1.8561      0\n    3 │ white     24     12.0   0.6304      0\n    4 │ white     38      8.0   3.4183      1\n    5 │ white     25     12.0   2.7852      1\n    6 │ white     67     12.0   2.3866      1\n  ⋮   │   ⋮      ⋮       ⋮        ⋮       ⋮\n 1995 │ white     22      7.0   0.2364      0\n 1996 │ white     26     16.0   3.3834      0\n 1997 │ white     34     12.0   2.917       1\n 1998 │ white     51     16.0   7.8949      1\n 1999 │ white     22     10.0   2.4811      0\n 2000 │ white     59     10.0   0.5523      0\n                             1988 rows omitted\njulia> container = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Logit())\nModel Class: Logistic Regression\nLikelihood Mode: Binomial\nLink Function: Identity\nComputing Method: Optimization\n────────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error      z  Pr(>|z|)   Lower 95%   Upper 95%\n────────────────────────────────────────────────────────────────────────────\n(Intercept)  -3.03426    0.325927    -9.31    <1e-19  -3.67307    -2.39546\nAge           0.0283543  0.00346034   8.19    <1e-15   0.0215722   0.0351365\nRace: white   0.250798   0.146457     1.71    0.0868  -0.0362521   0.537847\nIncome        0.177112   0.0271516    6.52    <1e-10   0.123896    0.230328\nEducate       0.175634   0.0203308    8.64    <1e-17   0.135786    0.215481\n────────────────────────────────────────────────────────────────────────────\njulia> coeftable(container)\n────────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error      z  Pr(>|z|)   Lower 95%   Upper 95%\n────────────────────────────────────────────────────────────────────────────\n(Intercept)  -3.03426    0.325927    -9.31    <1e-19  -3.67307    -2.39546\nAge           0.0283543  0.00346034   8.19    <1e-15   0.0215722   0.0351365\nRace: white   0.250798   0.146457     1.71    0.0868  -0.0362521   0.537847\nIncome        0.177112   0.0271516    6.52    <1e-10   0.123896    0.230328\nEducate       0.175634   0.0203308    8.64    <1e-17   0.135786    0.215481\n────────────────────────────────────────────────────────────────────────────\njulia> loglikelihood(container)\n-1011.9906318515575\njulia> aic(container)\n2033.981263703115\njulia> bic(container)\n2061.9857760008254\n\n\n\n\n\n","category":"method"},{"location":"api/frequentist_regression/#CRRao.fit-Tuple{FormulaTerm, DataFrame, LogisticRegression, Probit}","page":"Frequentist Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::Probit)\n\nFit a Logistic Regression model on the input data using the Probit link. Uses the glm method from the GLM package under the hood. Returns an object of type FrequentistRegression{:LogisticRegression}.\n\n\n\n\n\n","category":"method"},{"location":"api/frequentist_regression/#CRRao.fit-Tuple{FormulaTerm, DataFrame, LogisticRegression, Cloglog}","page":"Frequentist Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::Cloglog)\n\nFit a Logistic Regression model on the input data using the Cloglog link. Uses the glm method from the GLM package under the hood. Returns an object of type FrequentistRegression{:LogisticRegression}.\n\n\n\n\n\n","category":"method"},{"location":"api/frequentist_regression/#CRRao.fit-Tuple{FormulaTerm, DataFrame, LogisticRegression, Cauchit}","page":"Frequentist Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::Cauchit)\n\nFit a Logistic Regression model on the input data using the Cauchit link. Uses the glm method from the GLM package under the hood. Returns an object of type FrequentistRegression{:LogisticRegression}.\n\n\n\n\n\n","category":"method"},{"location":"api/frequentist_regression/#Negative-Binomial-Regression","page":"Frequentist Regression Models","title":"Negative Binomial Regression","text":"","category":"section"},{"location":"api/frequentist_regression/","page":"Frequentist Regression Models","title":"Frequentist Regression Models","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::NegBinomRegression)","category":"page"},{"location":"api/frequentist_regression/#CRRao.fit-Tuple{FormulaTerm, DataFrame, NegBinomRegression}","page":"Frequentist Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::NegBinomRegression)\n\nFit a Negative Binomial Regression model on the input data (with the default link function being the Log link). Uses the glm method from the GLM package under the hood. Returns an object of type FrequentistRegression{:NegativeBinomialRegression}.\n\nExample\n\njulia> using CRRao, RDatasets, StatsModels\njulia> sanction = dataset(\"Zelig\", \"sanction\")\n78×8 DataFrame\n Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         \n     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          \n─────┼───────────────────────────────────────────────────────────────────\n   1 │     1      4       3       1       1      4     15  major loss\n   2 │     0      2       3       0       1      3      4  modest loss\n   3 │     0      1       3       1       0      2      1  little effect\n   4 │     1      1       3       1       1      2      1  little effect\n   5 │     0      1       3       1       1      2      1  little effect\n   6 │     0      1       3       0       1      2      1  little effect\n  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮\n  73 │     1      3       1       1       1      2     14  little effect\n  74 │     0      2       1       0       0      1      2  net gain\n  75 │     0      1       3       0       1      2      1  little effect\n  76 │     0      4       3       1       0      2     13  little effect\n  77 │     0      1       2       0       0      1      1  net gain\n  78 │     1      3       1       1       1      2     10  little effect\n                                                          66 rows omitted\njulia> container = fit(@formula(Num ~ Target + Coop + NCost), sanction, NegBinomRegression())\nModel Class: Count Regression\nLikelihood Mode: Negative Binomial\nLink Function: Log\nComputing Method: Optimization\n─────────────────────────────────────────────────────────────────────────────────\n                         Coef.  Std. Error      z  Pr(>|z|)  Lower 95%  Upper 95%\n─────────────────────────────────────────────────────────────────────────────────\n(Intercept)         -1.10939      0.459677  -2.41    0.0158  -2.01034   -0.208444\nTarget               0.0117398    0.142779   0.08    0.9345  -0.268101   0.291581\nCoop                 1.0506       0.111556   9.42    <1e-20   0.831949   1.26924\nNCost: major loss   -0.204244     0.508156  -0.40    0.6877  -1.20021    0.791723\nNCost: modest loss   1.27142      0.290427   4.38    <1e-04   0.702197   1.84065\nNCost: net gain      0.176797     0.254291   0.70    0.4869  -0.321604   0.675197\n─────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n","category":"method"},{"location":"api/frequentist_regression/#Poisson-Regression","page":"Frequentist Regression Models","title":"Poisson Regression","text":"","category":"section"},{"location":"api/frequentist_regression/","page":"Frequentist Regression Models","title":"Frequentist Regression Models","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::PoissonRegression)","category":"page"},{"location":"api/frequentist_regression/#CRRao.fit-Tuple{FormulaTerm, DataFrame, PoissonRegression}","page":"Frequentist Regression Models","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::PoissonRegression)\n\nFit a Poisson Regression model on the input data (with the default link function being the Log link). Uses the glm method from the GLM package under the hood. Returns an object of type FrequentistRegression{:PoissonRegression}.\n\nExample\n\njulia> using CRRao, RDatasets, StatsModels\njulia> sanction = dataset(\"Zelig\", \"sanction\")\n78×8 DataFrame\n Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         \n     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          \n─────┼───────────────────────────────────────────────────────────────────\n   1 │     1      4       3       1       1      4     15  major loss\n   2 │     0      2       3       0       1      3      4  modest loss\n   3 │     0      1       3       1       0      2      1  little effect\n   4 │     1      1       3       1       1      2      1  little effect\n   5 │     0      1       3       1       1      2      1  little effect\n   6 │     0      1       3       0       1      2      1  little effect\n  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮\n  73 │     1      3       1       1       1      2     14  little effect\n  74 │     0      2       1       0       0      1      2  net gain\n  75 │     0      1       3       0       1      2      1  little effect\n  76 │     0      4       3       1       0      2     13  little effect\n  77 │     0      1       2       0       0      1      1  net gain\n  78 │     1      3       1       1       1      2     10  little effect\n                                                          66 rows omitted\njulia> container = fit(@formula(Num ~ Target + Coop + NCost), sanction, PoissonRegression())\nModel Class: Poisson Regression\nLikelihood Mode: Poison\nLink Function: Log\nComputing Method: Optimization\n─────────────────────────────────────────────────────────────────────────────────\n                        Coef.  Std. Error      z  Pr(>|z|)   Lower 95%  Upper 95%\n─────────────────────────────────────────────────────────────────────────────────\n(Intercept)         -1.91392    0.261667   -7.31    <1e-12  -2.42678    -1.40106\nTarget               0.157769   0.0653822   2.41    0.0158   0.0296218   0.285915\nCoop                 1.15127    0.0561861  20.49    <1e-92   1.04114     1.26139\nNCost: major loss   -0.324051   0.230055   -1.41    0.1590  -0.774951    0.126848\nNCost: modest loss   1.71973    0.100518   17.11    <1e-64   1.52272     1.91674\nNCost: net gain      0.463907   0.16992     2.73    0.0063   0.13087     0.796944\n─────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n","category":"method"},{"location":"api/frequentist_regression/#Extended-functions-from-[StatsAPI.jl](https://github.com/JuliaStats/StatsAPI.jl)","page":"Frequentist Regression Models","title":"Extended functions from StatsAPI.jl","text":"","category":"section"},{"location":"api/frequentist_regression/","page":"Frequentist Regression Models","title":"Frequentist Regression Models","text":"coeftable(container::FrequentistRegression)\nr2(container::FrequentistRegression)\nadjr2(container::FrequentistRegression)\nloglikelihood(container::FrequentistRegression)\naic(container::FrequentistRegression)\nbic(container::FrequentistRegression)\nsigma(container::FrequentistRegression)\npredict(container::FrequentistRegression)\nresiduals(container::FrequentistRegression)\ncooksdistance(container::FrequentistRegression)","category":"page"},{"location":"api/frequentist_regression/#CRRao.coeftable-Tuple{FrequentistRegression}","page":"Frequentist Regression Models","title":"CRRao.coeftable","text":"coeftable(container::FrequentistRegression)\n\nTable of coefficients and other statistics of the model. Extends the coeftable method from StatsAPI.jl.\n\nExample\n\nusing CRRao, RDatasets, StatsModels\n\n# Get the dataset\nmtcars = dataset(\"datasets\", \"mtcars\")\n\n# Train the model\ncontainer = fit(@formula(MPG ~ HP + WT + Gear), mtcars, LinearRegression())\n\n# Get table of coefficients\ncoeftable(container)\n\n\n\n\n\n","category":"method"},{"location":"api/frequentist_regression/#CRRao.r2-Tuple{FrequentistRegression}","page":"Frequentist Regression Models","title":"CRRao.r2","text":"r2(container::FrequentistRegression)\n\nCoeffient of determination. Extends the r2 method from StatsAPI.jl.\n\nExample\n\nusing CRRao, RDatasets, StatsModels\n\n# Get the dataset\nmtcars = dataset(\"datasets\", \"mtcars\")\n\n# Train the model\ncontainer = fit(@formula(MPG ~ HP + WT + Gear), mtcars, LinearRegression())\n\n# Get r2\nr2(container)\n\n\n\n\n\n","category":"method"},{"location":"api/frequentist_regression/#CRRao.adjr2-Tuple{FrequentistRegression}","page":"Frequentist Regression Models","title":"CRRao.adjr2","text":"adjr2(container::FrequentistRegression)\n\nAdjusted coeffient of determination. Extends the adjr2 method from StatsAPI.jl.\n\nExample\n\nusing CRRao, RDatasets, StatsModels\n\n# Get the dataset\nmtcars = dataset(\"datasets\", \"mtcars\")\n\n# Train the model\ncontainer = fit(@formula(MPG ~ HP + WT + Gear), mtcars, LinearRegression())\n\n# Get adjr2\nadjr2(container)\n\n\n\n\n\n","category":"method"},{"location":"api/frequentist_regression/#CRRao.loglikelihood-Tuple{FrequentistRegression}","page":"Frequentist Regression Models","title":"CRRao.loglikelihood","text":"loglikelihood(container::FrequentistRegression)\n\nLog-likelihood of the model. Extends the loglikelihood method from StatsAPI.jl.\n\nExample\n\nusing CRRao, RDatasets, StatsModels\n\n# Get the dataset\nmtcars = dataset(\"datasets\", \"mtcars\")\n\n# Train the model\ncontainer = fit(@formula(MPG ~ HP + WT + Gear), mtcars, LinearRegression())\n\n# Get loglikelihood\nadjr2(container)\n\n\n\n\n\n","category":"method"},{"location":"api/frequentist_regression/#CRRao.aic-Tuple{FrequentistRegression}","page":"Frequentist Regression Models","title":"CRRao.aic","text":"aic(container::FrequentistRegression)\n\nAkaike's Information Criterion. Extends the aic method from StatsAPI.jl.\n\nExample\n\nusing CRRao, RDatasets, StatsModels\n\n# Get the dataset\nmtcars = dataset(\"datasets\", \"mtcars\")\n\n# Train the model\ncontainer = fit(@formula(MPG ~ HP + WT + Gear), mtcars, LinearRegression())\n\n# Get aic\naic(container)\n\n\n\n\n\n","category":"method"},{"location":"api/frequentist_regression/#CRRao.bic-Tuple{FrequentistRegression}","page":"Frequentist Regression Models","title":"CRRao.bic","text":"bic(container::FrequentistRegression)\n\nBayesian Information Criterion. Extends the bic method from StatsAPI.jl.\n\nExample\n\nusing CRRao, RDatasets, StatsModels\n\n# Get the dataset\nmtcars = dataset(\"datasets\", \"mtcars\")\n\n# Train the model\ncontainer = fit(@formula(MPG ~ HP + WT + Gear), mtcars, LinearRegression())\n\n# Get bic\nbic(container)\n\n\n\n\n\n","category":"method"},{"location":"api/frequentist_regression/#CRRao.predict-Tuple{FrequentistRegression}","page":"Frequentist Regression Models","title":"CRRao.predict","text":"predict(container::FrequentistRegression)\n\nPredicted response of the model. Extends the predict method from StatsAPI.jl.\n\nExample\n\nusing CRRao, RDatasets, StatsModels\n\n# Get the dataset\nmtcars = dataset(\"datasets\", \"mtcars\")\n\n# Train the model\ncontainer = fit(@formula(MPG ~ HP + WT + Gear), mtcars, LinearRegression())\n\n# Get predicted response\npredict(container)\n\n\n\n\n\n","category":"method"},{"location":"api/frequentist_regression/#CRRao.residuals-Tuple{FrequentistRegression}","page":"Frequentist Regression Models","title":"CRRao.residuals","text":"residuals(container::FrequentistRegression)\n\nResiduals of the model. Extends the residuals method from StatsAPI.jl.\n\nExample\n\nusing CRRao, RDatasets, StatsModels\n\n# Get the dataset\nmtcars = dataset(\"datasets\", \"mtcars\")\n\n# Train the model\ncontainer = fit(@formula(MPG ~ HP + WT + Gear), mtcars, LinearRegression())\n\n# Get residuals\nresiduals(container)\n\n\n\n\n\n","category":"method"},{"location":"api/frequentist_regression/#CRRao.cooksdistance-Tuple{FrequentistRegression}","page":"Frequentist Regression Models","title":"CRRao.cooksdistance","text":"cooksdistance(container::FrequentistRegression)\n\nCompute Cook's distance for each observation in a linear model. Extends the cooksdistance method from StatsAPI.jl.\n\nExample\n\nusing CRRao, RDatasets, StatsModels\n\n# Get the dataset\nmtcars = dataset(\"datasets\", \"mtcars\")\n\n# Train the model\ncontainer = fit(@formula(MPG ~ HP + WT + Gear), mtcars, LinearRegression())\n\n# Get vector of Cook's distances\ncooksdistance(container)\n\n\n\n\n\n","category":"method"},{"location":"api/interface/#General-Interface","page":"General Interface","title":"General Interface","text":"","category":"section"},{"location":"api/interface/#Understanding-the-interface","page":"General Interface","title":"Understanding the interface","text":"","category":"section"},{"location":"api/interface/","page":"General Interface","title":"General Interface","text":"CRRao exports the fit function, which is used to train all types of models supported by the package. As of now, the function supports the following signatures.","category":"page"},{"location":"api/interface/","page":"General Interface","title":"General Interface","text":"fit(formula, data, modelClass)\nfit(formula, data, modelClass, link)\nfit(formula, data, modelClass, prior)\nfit(formula, data, modelClass, link, prior)","category":"page"},{"location":"api/interface/","page":"General Interface","title":"General Interface","text":"It should be noted that not all model classes support every type of signature. The parameters passed above mean the following.","category":"page"},{"location":"api/interface/","page":"General Interface","title":"General Interface","text":"The parameter formula must be a formula of type StatsModels.FormulaTerm. Any formula has an LHS and an RHS. The LHS represents the response variable, and the RHS represents the independent variables.\nThe parameter data must be a DataFrame. This variable represents the dataset on which the model must be trained.\nmodelClass represents the type of the statistical model to be used. Currently, CRRao supports four regression models, and the type of modelClass must be one of the following:\nLinearRegression\nLogisticRegression\nNegBinomRegression\nPoissonRegression\nCertain model classes (like Logistic Regression) support link functions; this is represented by the link parameter. Currently four link functions are supported: Logit, Probit, Cloglog and Cauchit. So, the type of link must be one of the following:\nLogit\nProbit\nCloglog\nCauchit\nCRRao also supports Bayesian models, and the priors to be can be specified while calling fit. Currently CRRao supports five different kinds of priors, and the type of the prior parameter must be one of the following.\nPrior_Ridge\nPrior_Laplace\nPrior_Cauchy\nPrior_TDist\nPrior_Uniform","category":"page"},{"location":"api/interface/","page":"General Interface","title":"General Interface","text":"fit","category":"page"},{"location":"api/interface/#CRRao.fit","page":"General Interface","title":"CRRao.fit","text":"fit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression)\n\nFit an OLS Linear Regression model on the input data. Uses the lm method from the GLM package under the hood. Returns an object of type FrequentistRegression{:LinearRegression}.\n\nExample\n\njulia> using CRRao, RDatasets, StatsPlots, StatsModels\njulia> df = dataset(\"datasets\", \"mtcars\")\n32×12 DataFrame\n Row │ Model              MPG      Cyl    Disp     HP     DRat     WT       QSec     VS     AM     Gear   Carb  \n     │ String31           Float64  Int64  Float64  Int64  Float64  Float64  Float64  Int64  Int64  Int64  Int64 \n─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ Mazda RX4             21.0      6    160.0    110     3.9     2.62     16.46      0      1      4      4\n   2 │ Mazda RX4 Wag         21.0      6    160.0    110     3.9     2.875    17.02      0      1      4      4\n   3 │ Datsun 710            22.8      4    108.0     93     3.85    2.32     18.61      1      1      4      1\n   4 │ Hornet 4 Drive        21.4      6    258.0    110     3.08    3.215    19.44      1      0      3      1\n   5 │ Hornet Sportabout     18.7      8    360.0    175     3.15    3.44     17.02      0      0      3      2\n   6 │ Valiant               18.1      6    225.0    105     2.76    3.46     20.22      1      0      3      1\n  ⋮  │         ⋮             ⋮       ⋮       ⋮       ⋮       ⋮        ⋮        ⋮       ⋮      ⋮      ⋮      ⋮\n  27 │ Porsche 914-2         26.0      4    120.3     91     4.43    2.14     16.7       0      1      5      2\n  28 │ Lotus Europa          30.4      4     95.1    113     3.77    1.513    16.9       1      1      5      2\n  29 │ Ford Pantera L        15.8      8    351.0    264     4.22    3.17     14.5       0      1      5      4\n  30 │ Ferrari Dino          19.7      6    145.0    175     3.62    2.77     15.5       0      1      5      6\n  31 │ Maserati Bora         15.0      8    301.0    335     3.54    3.57     14.6       0      1      5      8\n  32 │ Volvo 142E            21.4      4    121.0    109     4.11    2.78     18.6       1      1      4      2\n                                                                                                 20 rows omitted\njulia> container = fit(@formula(MPG ~ HP + WT + Gear), df, LinearRegression())\nModel Class: Linear Regression\nLikelihood Mode: Gauss\nLink Function: Identity\nComputing Method: Optimization\n────────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error      t  Pr(>|t|)   Lower 95%   Upper 95%\n────────────────────────────────────────────────────────────────────────────\n(Intercept)  32.0137     4.63226      6.91    <1e-06  22.5249     41.5024\nHP           -0.0367861  0.00989146  -3.72    0.0009  -0.0570478  -0.0165243\nWT           -3.19781    0.846546    -3.78    0.0008  -4.93188    -1.46374\nGear          1.01998    0.851408     1.20    0.2410  -0.72405     2.76401\n────────────────────────────────────────────────────────────────────────────\njulia> coeftable(container)\n────────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error      t  Pr(>|t|)   Lower 95%   Upper 95%\n────────────────────────────────────────────────────────────────────────────\n(Intercept)  32.0137     4.63226      6.91    <1e-06  22.5249     41.5024\nHP           -0.0367861  0.00989146  -3.72    0.0009  -0.0570478  -0.0165243\nWT           -3.19781    0.846546    -3.78    0.0008  -4.93188    -1.46374\nGear          1.01998    0.851408     1.20    0.2410  -0.72405     2.76401\n────────────────────────────────────────────────────────────────────────────\njulia> sigma(container)\n2.5741691724978972\njulia> aic(container)\n157.05277871921942\njulia> predict(container)\n32-element Vector{Float64}:\n 23.668849952338718\n 22.85340824320634\n 25.253556140740894\n 20.746171762311384\n 17.635570543830177\n 20.14663845388644\n 14.644831040166633\n 23.61182872351372\n  ⋮\n 16.340457241090512\n 27.47793682112109\n 26.922715039574857\n 28.11844900519874\n 17.264981908248554\n 21.818065399379595\n 13.374047477198516\n 23.193986311384343\njulia> residuals(container)\n32-element Vector{Float64}:\n -2.668849952338718\n -1.8534082432063386\n -2.4535561407408935\n  0.6538282376886144\n  1.0644294561698224\n -2.0466384538864375\n -0.3448310401666319\n  0.7881712764862776\n  ⋮\n  2.8595427589094875\n -0.1779368211210901\n -0.9227150395748573\n  2.2815509948012576\n -1.4649819082485536\n -2.1180653993795957\n  1.6259525228014837\n -1.7939863113843444\njulia> plot(cooksdistance(container))\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::Logit)\n\nFit a Logistic Regression model on the input data using the Logit link. Uses the glm method from the GLM package under the hood. Returns an object of type FrequentistRegression{:LogisticRegression}.\n\nExample\n\njulia> using CRRao, RDatasets, StatsModels\njulia> turnout = dataset(\"Zelig\", \"turnout\")\n2000×5 DataFrame\n  Row │ Race   Age    Educate  Income   Vote  \n      │ Cat…   Int32  Float64  Float64  Int32 \n──────┼───────────────────────────────────────\n    1 │ white     60     14.0   3.3458      1\n    2 │ white     51     10.0   1.8561      0\n    3 │ white     24     12.0   0.6304      0\n    4 │ white     38      8.0   3.4183      1\n    5 │ white     25     12.0   2.7852      1\n    6 │ white     67     12.0   2.3866      1\n  ⋮   │   ⋮      ⋮       ⋮        ⋮       ⋮\n 1995 │ white     22      7.0   0.2364      0\n 1996 │ white     26     16.0   3.3834      0\n 1997 │ white     34     12.0   2.917       1\n 1998 │ white     51     16.0   7.8949      1\n 1999 │ white     22     10.0   2.4811      0\n 2000 │ white     59     10.0   0.5523      0\n                             1988 rows omitted\njulia> container = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Logit())\nModel Class: Logistic Regression\nLikelihood Mode: Binomial\nLink Function: Identity\nComputing Method: Optimization\n────────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error      z  Pr(>|z|)   Lower 95%   Upper 95%\n────────────────────────────────────────────────────────────────────────────\n(Intercept)  -3.03426    0.325927    -9.31    <1e-19  -3.67307    -2.39546\nAge           0.0283543  0.00346034   8.19    <1e-15   0.0215722   0.0351365\nRace: white   0.250798   0.146457     1.71    0.0868  -0.0362521   0.537847\nIncome        0.177112   0.0271516    6.52    <1e-10   0.123896    0.230328\nEducate       0.175634   0.0203308    8.64    <1e-17   0.135786    0.215481\n────────────────────────────────────────────────────────────────────────────\njulia> coeftable(container)\n────────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error      z  Pr(>|z|)   Lower 95%   Upper 95%\n────────────────────────────────────────────────────────────────────────────\n(Intercept)  -3.03426    0.325927    -9.31    <1e-19  -3.67307    -2.39546\nAge           0.0283543  0.00346034   8.19    <1e-15   0.0215722   0.0351365\nRace: white   0.250798   0.146457     1.71    0.0868  -0.0362521   0.537847\nIncome        0.177112   0.0271516    6.52    <1e-10   0.123896    0.230328\nEducate       0.175634   0.0203308    8.64    <1e-17   0.135786    0.215481\n────────────────────────────────────────────────────────────────────────────\njulia> loglikelihood(container)\n-1011.9906318515575\njulia> aic(container)\n2033.981263703115\njulia> bic(container)\n2061.9857760008254\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::Probit)\n\nFit a Logistic Regression model on the input data using the Probit link. Uses the glm method from the GLM package under the hood. Returns an object of type FrequentistRegression{:LogisticRegression}.\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::Cloglog)\n\nFit a Logistic Regression model on the input data using the Cloglog link. Uses the glm method from the GLM package under the hood. Returns an object of type FrequentistRegression{:LogisticRegression}.\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::Cauchit)\n\nFit a Logistic Regression model on the input data using the Cauchit link. Uses the glm method from the GLM package under the hood. Returns an object of type FrequentistRegression{:LogisticRegression}.\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::NegBinomRegression)\n\nFit a Negative Binomial Regression model on the input data (with the default link function being the Log link). Uses the glm method from the GLM package under the hood. Returns an object of type FrequentistRegression{:NegativeBinomialRegression}.\n\nExample\n\njulia> using CRRao, RDatasets, StatsModels\njulia> sanction = dataset(\"Zelig\", \"sanction\")\n78×8 DataFrame\n Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         \n     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          \n─────┼───────────────────────────────────────────────────────────────────\n   1 │     1      4       3       1       1      4     15  major loss\n   2 │     0      2       3       0       1      3      4  modest loss\n   3 │     0      1       3       1       0      2      1  little effect\n   4 │     1      1       3       1       1      2      1  little effect\n   5 │     0      1       3       1       1      2      1  little effect\n   6 │     0      1       3       0       1      2      1  little effect\n  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮\n  73 │     1      3       1       1       1      2     14  little effect\n  74 │     0      2       1       0       0      1      2  net gain\n  75 │     0      1       3       0       1      2      1  little effect\n  76 │     0      4       3       1       0      2     13  little effect\n  77 │     0      1       2       0       0      1      1  net gain\n  78 │     1      3       1       1       1      2     10  little effect\n                                                          66 rows omitted\njulia> container = fit(@formula(Num ~ Target + Coop + NCost), sanction, NegBinomRegression())\nModel Class: Count Regression\nLikelihood Mode: Negative Binomial\nLink Function: Log\nComputing Method: Optimization\n─────────────────────────────────────────────────────────────────────────────────\n                         Coef.  Std. Error      z  Pr(>|z|)  Lower 95%  Upper 95%\n─────────────────────────────────────────────────────────────────────────────────\n(Intercept)         -1.10939      0.459677  -2.41    0.0158  -2.01034   -0.208444\nTarget               0.0117398    0.142779   0.08    0.9345  -0.268101   0.291581\nCoop                 1.0506       0.111556   9.42    <1e-20   0.831949   1.26924\nNCost: major loss   -0.204244     0.508156  -0.40    0.6877  -1.20021    0.791723\nNCost: modest loss   1.27142      0.290427   4.38    <1e-04   0.702197   1.84065\nNCost: net gain      0.176797     0.254291   0.70    0.4869  -0.321604   0.675197\n─────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::PoissonRegression)\n\nFit a Poisson Regression model on the input data (with the default link function being the Log link). Uses the glm method from the GLM package under the hood. Returns an object of type FrequentistRegression{:PoissonRegression}.\n\nExample\n\njulia> using CRRao, RDatasets, StatsModels\njulia> sanction = dataset(\"Zelig\", \"sanction\")\n78×8 DataFrame\n Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         \n     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          \n─────┼───────────────────────────────────────────────────────────────────\n   1 │     1      4       3       1       1      4     15  major loss\n   2 │     0      2       3       0       1      3      4  modest loss\n   3 │     0      1       3       1       0      2      1  little effect\n   4 │     1      1       3       1       1      2      1  little effect\n   5 │     0      1       3       1       1      2      1  little effect\n   6 │     0      1       3       0       1      2      1  little effect\n  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮\n  73 │     1      3       1       1       1      2     14  little effect\n  74 │     0      2       1       0       0      1      2  net gain\n  75 │     0      1       3       0       1      2      1  little effect\n  76 │     0      4       3       1       0      2     13  little effect\n  77 │     0      1       2       0       0      1      1  net gain\n  78 │     1      3       1       1       1      2     10  little effect\n                                                          66 rows omitted\njulia> container = fit(@formula(Num ~ Target + Coop + NCost), sanction, PoissonRegression())\nModel Class: Poisson Regression\nLikelihood Mode: Poison\nLink Function: Log\nComputing Method: Optimization\n─────────────────────────────────────────────────────────────────────────────────\n                        Coef.  Std. Error      z  Pr(>|z|)   Lower 95%  Upper 95%\n─────────────────────────────────────────────────────────────────────────────────\n(Intercept)         -1.91392    0.261667   -7.31    <1e-12  -2.42678    -1.40106\nTarget               0.157769   0.0653822   2.41    0.0158   0.0296218   0.285915\nCoop                 1.15127    0.0561861  20.49    <1e-92   1.04114     1.26139\nNCost: major loss   -0.324051   0.230055   -1.41    0.1590  -0.774951    0.126848\nNCost: modest loss   1.71973    0.100518   17.11    <1e-64   1.52272     1.91674\nNCost: net gain      0.463907   0.16992     2.73    0.0063   0.13087     0.796944\n─────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression, prior::Prior_Ridge, h::Float64 = 0.01, sim_size::Int64 = 1000)\n\nFit a Bayesian Linear Regression model on the input data with a Ridge prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> df = dataset(\"datasets\", \"mtcars\")\n32×12 DataFrame\n Row │ Model              MPG      Cyl    Disp     HP     DRat     WT       QSec     VS     AM     Gear   Carb  \n     │ String31           Float64  Int64  Float64  Int64  Float64  Float64  Float64  Int64  Int64  Int64  Int64 \n─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ Mazda RX4             21.0      6    160.0    110     3.9     2.62     16.46      0      1      4      4\n   2 │ Mazda RX4 Wag         21.0      6    160.0    110     3.9     2.875    17.02      0      1      4      4\n   3 │ Datsun 710            22.8      4    108.0     93     3.85    2.32     18.61      1      1      4      1\n   4 │ Hornet 4 Drive        21.4      6    258.0    110     3.08    3.215    19.44      1      0      3      1\n  ⋮  │         ⋮             ⋮       ⋮       ⋮       ⋮       ⋮        ⋮        ⋮       ⋮      ⋮      ⋮      ⋮\n  30 │ Ferrari Dino          19.7      6    145.0    175     3.62    2.77     15.5       0      1      5      6\n  31 │ Maserati Bora         15.0      8    301.0    335     3.54    3.57     14.6       0      1      5      8\n  32 │ Volvo 142E            21.4      4    121.0    109     4.11    2.78     18.6       1      1      4      2\n                                                                                                 25 rows omitted\njulia> container = fit(@formula(MPG ~ HP + WT + Gear), df, LinearRegression(), Prior_Ridge())\n┌ Info: Found initial step size\n└   ϵ = 0.00078125\nChains MCMC chain (10000×18×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 30.44 seconds\nCompute duration  = 30.44 seconds\nparameters        = v, σ, α, β[1], β[2], β[3]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           v    6.9097    3.7793     0.0378    0.0609   3848.1626    0.9999      126.4013\n           σ    2.6726    0.3878     0.0039    0.0061   3787.1472    1.0000      124.3972\n           α   28.6866    5.4205     0.0542    0.1106   2431.5304    1.0001       79.8690\n        β[1]   -0.0395    0.0106     0.0001    0.0002   4057.7267    0.9999      133.2849\n        β[2]   -2.7056    0.9635     0.0096    0.0176   2897.6230    1.0001       95.1788\n        β[3]    1.5912    0.9825     0.0098    0.0198   2538.0548    1.0001       83.3680\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           v    2.5200    4.5014    6.0397    8.2382   16.6220\n           σ    2.0519    2.4030    2.6297    2.8942    3.5482\n           α   17.6034   25.2623   28.9229   32.3360   38.7343\n        β[1]   -0.0612   -0.0464   -0.0393   -0.0325   -0.0191\n        β[2]   -4.5163   -3.3443   -2.7385   -2.1041   -0.7211\n        β[3]   -0.2205    0.9158    1.5520    2.2028    3.6202\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression, prior::Prior_Laplace, h::Float64 = 0.01, sim_size::Int64 = 1000)\n\nFit a Bayesian Linear Regression model on the input data with a Laplace prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> df = dataset(\"datasets\", \"mtcars\")\n32×12 DataFrame\n Row │ Model              MPG      Cyl    Disp     HP     DRat     WT       QSec     VS     AM     Gear   Carb  \n     │ String31           Float64  Int64  Float64  Int64  Float64  Float64  Float64  Int64  Int64  Int64  Int64 \n─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ Mazda RX4             21.0      6    160.0    110     3.9     2.62     16.46      0      1      4      4\n   2 │ Mazda RX4 Wag         21.0      6    160.0    110     3.9     2.875    17.02      0      1      4      4\n   3 │ Datsun 710            22.8      4    108.0     93     3.85    2.32     18.61      1      1      4      1\n   4 │ Hornet 4 Drive        21.4      6    258.0    110     3.08    3.215    19.44      1      0      3      1\n  ⋮  │         ⋮             ⋮       ⋮       ⋮       ⋮       ⋮        ⋮        ⋮       ⋮      ⋮      ⋮      ⋮\n  30 │ Ferrari Dino          19.7      6    145.0    175     3.62    2.77     15.5       0      1      5      6\n  31 │ Maserati Bora         15.0      8    301.0    335     3.54    3.57     14.6       0      1      5      8\n  32 │ Volvo 142E            21.4      4    121.0    109     4.11    2.78     18.6       1      1      4      2\n                                                                                                 25 rows omitted\njulia> container = fit(@formula(MPG ~ HP + WT + Gear), df, LinearRegression(), Prior_Laplace())\n┌ Info: Found initial step size\n└   ϵ = 0.00078125\nChains MCMC chain (10000×18×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 28.55 seconds\nCompute duration  = 28.55 seconds\nparameters        = v, σ, α, β[1], β[2], β[3]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           v    4.2213    3.0653     0.0307    0.0506   3799.4211    0.9999      133.0609\n           σ    2.6713    0.3829     0.0038    0.0068   3782.5307    1.0001      132.4694\n           α   29.0523    5.2589     0.0526    0.1032   3144.5864    1.0004      110.1277\n        β[1]   -0.0398    0.0106     0.0001    0.0002   4429.6471    1.0005      155.1323\n        β[2]   -2.7161    0.9506     0.0095    0.0182   3299.1828    1.0009      115.5419\n        β[3]    1.5129    0.9530     0.0095    0.0180   3383.7096    1.0002      118.5021\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           v    1.2438    2.3788    3.4110    5.1138   11.7423\n           σ    2.0692    2.4016    2.6226    2.8897    3.5602\n           α   17.8056   25.8001   29.2866   32.5385   38.8889\n        β[1]   -0.0614   -0.0466   -0.0395   -0.0326   -0.0194\n        β[2]   -4.5559   -3.3384   -2.7407   -2.1204   -0.7254\n        β[3]   -0.2790    0.8794    1.4691    2.1092    3.5245\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression, prior::Prior_Cauchy, sim_size::Int64 = 1000)\n\nFit a Bayesian Linear Regression model on the input data with a Cauchy prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> df = dataset(\"datasets\", \"mtcars\")\n32×12 DataFrame\n Row │ Model              MPG      Cyl    Disp     HP     DRat     WT       QSec     VS     AM     Gear   Carb  \n     │ String31           Float64  Int64  Float64  Int64  Float64  Float64  Float64  Int64  Int64  Int64  Int64 \n─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ Mazda RX4             21.0      6    160.0    110     3.9     2.62     16.46      0      1      4      4\n   2 │ Mazda RX4 Wag         21.0      6    160.0    110     3.9     2.875    17.02      0      1      4      4\n   3 │ Datsun 710            22.8      4    108.0     93     3.85    2.32     18.61      1      1      4      1\n   4 │ Hornet 4 Drive        21.4      6    258.0    110     3.08    3.215    19.44      1      0      3      1\n  ⋮  │         ⋮             ⋮       ⋮       ⋮       ⋮       ⋮        ⋮        ⋮       ⋮      ⋮      ⋮      ⋮\n  30 │ Ferrari Dino          19.7      6    145.0    175     3.62    2.77     15.5       0      1      5      6\n  31 │ Maserati Bora         15.0      8    301.0    335     3.54    3.57     14.6       0      1      5      8\n  32 │ Volvo 142E            21.4      4    121.0    109     4.11    2.78     18.6       1      1      4      2\n                                                                                                 25 rows omitted\njulia> container = fit(@formula(MPG ~ HP + WT + Gear), df, LinearRegression(), Prior_Cauchy(), 20000)\n┌ Info: Found initial step size\n└   ϵ = 0.000390625\nChains MCMC chain (20000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:21000\nNumber of chains  = 1\nSamples per chain = 20000\nWall duration     = 34.1 seconds\nCompute duration  = 34.1 seconds\nparameters        = σ, α, β[1], β[2], β[3]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           σ    2.5891    0.3413     0.0024    0.0036   8611.3030    1.0001      252.5087\n           α   30.2926    4.6666     0.0330    0.0590   5600.5552    1.0013      164.2247\n        β[1]   -0.0394    0.0100     0.0001    0.0001   7985.0944    1.0009      234.1464\n        β[2]   -2.8393    0.8638     0.0061    0.0106   6031.2854    1.0012      176.8550\n        β[3]    1.2738    0.8524     0.0060    0.0107   5814.5026    1.0014      170.4983\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           σ    2.0266    2.3485    2.5547    2.7908    3.3512\n           α   20.8140   27.3265   30.3854   33.4168   39.1369\n        β[1]   -0.0595   -0.0458   -0.0393   -0.0328   -0.0197\n        β[2]   -4.5172   -3.4069   -2.8485   -2.2786   -1.1244\n        β[3]   -0.3576    0.7039    1.2568    1.8199    3.0201\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression, prior::Prior_TDist, h::Float64 = 2.0, sim_size::Int64 = 1000)\n\nFit a Bayesian Linear Regression model on the input data with a t(ν) distributed prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsPlots, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> df = dataset(\"datasets\", \"mtcars\")\n32×12 DataFrame\n Row │ Model              MPG      Cyl    Disp     HP     DRat     WT       QSec     VS     AM     Gear   Carb  \n     │ String31           Float64  Int64  Float64  Int64  Float64  Float64  Float64  Int64  Int64  Int64  Int64 \n─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ Mazda RX4             21.0      6    160.0    110     3.9     2.62     16.46      0      1      4      4\n   2 │ Mazda RX4 Wag         21.0      6    160.0    110     3.9     2.875    17.02      0      1      4      4\n   3 │ Datsun 710            22.8      4    108.0     93     3.85    2.32     18.61      1      1      4      1\n   4 │ Hornet 4 Drive        21.4      6    258.0    110     3.08    3.215    19.44      1      0      3      1\n  ⋮  │         ⋮             ⋮       ⋮       ⋮       ⋮       ⋮        ⋮        ⋮       ⋮      ⋮      ⋮      ⋮\n  30 │ Ferrari Dino          19.7      6    145.0    175     3.62    2.77     15.5       0      1      5      6\n  31 │ Maserati Bora         15.0      8    301.0    335     3.54    3.57     14.6       0      1      5      8\n  32 │ Volvo 142E            21.4      4    121.0    109     4.11    2.78     18.6       1      1      4      2\n                                                                                                 25 rows omitted\njulia> container = fit(@formula(MPG ~ HP + WT + Gear), df, LinearRegression(), Prior_TDist())\n┌ Info: Found initial step size\n└   ϵ = 1.1920928955078126e-8\nChains MCMC chain (10000×18×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 41.09 seconds\nCompute duration  = 41.09 seconds\nparameters        = ν, σ, α, β[1], β[2], β[3]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           ν    1.0538    0.5576     0.0056    0.0143   1340.7091    0.9999       32.6318\n           σ    2.6251    0.3559     0.0036    0.0043   6374.0312    0.9999      155.1388\n           α   30.1859    4.7935     0.0479    0.0605   5361.7257    1.0006      130.5001\n        β[1]   -0.0396    0.0103     0.0001    0.0001   5835.9959    1.0003      142.0434\n        β[2]   -2.8099    0.8772     0.0088    0.0114   5301.0033    1.0010      129.0221\n        β[3]    1.2856    0.8699     0.0087    0.0106   5752.1640    1.0003      140.0030\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           ν    0.3670    0.6600    0.9301    1.2961    2.4821\n           σ    2.0327    2.3758    2.5885    2.8442    3.4393\n           α   20.4816   27.0685   30.2787   33.4481   39.3462\n        β[1]   -0.0599   -0.0464   -0.0396   -0.0326   -0.0198\n        β[2]   -4.4924   -3.3902   -2.8250   -2.2351   -1.0257\n        β[3]   -0.3642    0.7021    1.2642    1.8397    3.0849\njulia> plot(container.chain)\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression, prior::Prior_Uniform, h::Float64 = 0.01, sim_size::Int64 = 1000)\n\nFit a Bayesian Linear Regression model on the input data with a Uniform prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsPlots, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> df = dataset(\"datasets\", \"mtcars\")\n32×12 DataFrame\n Row │ Model              MPG      Cyl    Disp     HP     DRat     WT       QSec     VS     AM     Gear   Carb  \n     │ String31           Float64  Int64  Float64  Int64  Float64  Float64  Float64  Int64  Int64  Int64  Int64 \n─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────\n   1 │ Mazda RX4             21.0      6    160.0    110     3.9     2.62     16.46      0      1      4      4\n   2 │ Mazda RX4 Wag         21.0      6    160.0    110     3.9     2.875    17.02      0      1      4      4\n   3 │ Datsun 710            22.8      4    108.0     93     3.85    2.32     18.61      1      1      4      1\n   4 │ Hornet 4 Drive        21.4      6    258.0    110     3.08    3.215    19.44      1      0      3      1\n  ⋮  │         ⋮             ⋮       ⋮       ⋮       ⋮       ⋮        ⋮        ⋮       ⋮      ⋮      ⋮      ⋮\n  30 │ Ferrari Dino          19.7      6    145.0    175     3.62    2.77     15.5       0      1      5      6\n  31 │ Maserati Bora         15.0      8    301.0    335     3.54    3.57     14.6       0      1      5      8\n  32 │ Volvo 142E            21.4      4    121.0    109     4.11    2.78     18.6       1      1      4      2\n                                                                                                 25 rows omitted\njulia> container = fit(@formula(MPG ~ HP + WT + Gear), df, LinearRegression(), Prior_Uniform())\n┌ Info: Found initial step size\n└   ϵ = 0.00078125\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 34.62 seconds\nCompute duration  = 34.62 seconds\nparameters        = σ, α, β[1], β[2], β[3]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           σ    2.7117    0.3850     0.0039    0.0065   3581.7504    1.0006      103.4590\n           α   31.9599    4.8963     0.0490    0.0866   2347.8428    1.0000       67.8175\n        β[1]   -0.0369    0.0106     0.0001    0.0002   4837.8122    0.9999      139.7404\n        β[2]   -3.1811    0.9042     0.0090    0.0162   2643.2557    0.9999       76.3505\n        β[3]    1.0252    0.9053     0.0091    0.0157   2529.3416    1.0002       73.0601\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           σ    2.0984    2.4368    2.6656    2.9354    3.5799\n           α   22.3918   28.7053   31.9294   35.2194   41.3685\n        β[1]   -0.0580   -0.0438   -0.0370   -0.0299   -0.0161\n        β[2]   -4.9551   -3.7839   -3.1659   -2.6058   -1.3929\n        β[3]   -0.7644    0.4230    1.0254    1.6245    2.8040\njulia> plot(container.chain)\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression, prior::Prior_Gauss,alpha_prior_mean::Float64 = 0.0, beta_prior_mean::Float64, sim_size::Int64 = 1000, h::Float64 = 0.1)\n\nFit a Bayesian Linear Regression model on the input data with a Gaussian prior with user specific prior mean for α and β. User doesnot have     idea about the prior sd of α and β. User ignore the specification for sd of α and β.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123));\njulia> df = dataset(\"datasets\", \"mtcars\");                                                                                                \njulia> container = fit(@formula(MPG ~ HP + WT + Gear), df, LinearRegression(), Prior_Gauss(),0.0,[0.0,-3.0,1.0],1000)\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression, prior::Prior_Gauss, alpha_prior_mean::Float64, alpha_prior_sd::Float64, beta_prior_mean::Vector{Float64}, beta_prior_sd::Vector{Float64}, sim_size::Int64 = 1000)\n\nFit a Bayesian Linear Regression model on the input data with a Gaussian prior with user specific prior mean and sd for α and β. \n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123));\njulia> df = dataset(\"datasets\", \"mtcars\");                                                                                                \njulia> container = fit(@formula(MPG ~ HP + WT + Gear), df, LinearRegression(), Prior_Gauss(),30.0,10.0,[0.0,-3.0,1.0],[0.1,1.0,1.0],1000)\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::CRRaoLink, prior::Prior_Ridge, h::Float64 = 0.1, level::Float64 = 0.95, sim_size::Int64 = 1000)\n\nFit a Bayesian Logistic Regression model on the input data with a Ridge prior with the provided Link function.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> turnout = dataset(\"Zelig\", \"turnout\")\n2000×5 DataFrame\n  Row │ Race   Age    Educate  Income   Vote  \n      │ Cat…   Int32  Float64  Float64  Int32 \n──────┼───────────────────────────────────────\n    1 │ white     60     14.0   3.3458      1\n    2 │ white     51     10.0   1.8561      0\n    3 │ white     24     12.0   0.6304      0\n    4 │ white     38      8.0   3.4183      1\n  ⋮   │   ⋮      ⋮       ⋮        ⋮       ⋮\n 1998 │ white     51     16.0   7.8949      1\n 1999 │ white     22     10.0   2.4811      0\n 2000 │ white     59     10.0   0.5523      0\n                             1993 rows omitted\njulia> container_logit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Logit(), Prior_Ridge())\n┌ Warning: The current proposal will be rejected due to numerical error(s).\n│   isfinite.((θ, r, ℓπ, ℓκ)) = (true, false, false, false)\n└ @ AdvancedHMC ~/.julia/packages/AdvancedHMC/kB7Xa/src/hamiltonian.jl:47\nChains MCMC chain (1000×18×1 Array{Float64, 3}):\n\nIterations        = 501:1:1500\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 6.98 seconds\nCompute duration  = 6.98 seconds\nparameters        = λ, α, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse        ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64    Float64   Float64       Float64 \n\n           λ    2.3722    0.0007     0.0000    0.0001     5.1273    1.1072        0.7346\n           α    0.7872    0.0003     0.0000    0.0001     2.9594    1.5660        0.4240\n        β[1]    0.4843    0.0000     0.0000    0.0000   111.6465    1.0067       15.9952\n        β[2]    0.6183    0.0004     0.0000    0.0001     2.3393    2.2610        0.3351\n        β[3]   -1.4043    0.0003     0.0000    0.0001     3.1790    1.3848        0.4554\n        β[4]   -3.0656    0.0007     0.0000    0.0001     2.4692    1.9479        0.3538\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    2.3706    2.3721    2.3724    2.3726    2.3731\n           α    0.7865    0.7872    0.7873    0.7874    0.7876\n        β[1]    0.4842    0.4843    0.4843    0.4843    0.4843\n        β[2]    0.6178    0.6180    0.6181    0.6186    0.6192\n        β[3]   -1.4046   -1.4045   -1.4044   -1.4041   -1.4036\n        β[4]   -3.0669   -3.0659   -3.0655   -3.0653   -3.0645\n\njulia> container_probit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Probit(), Prior_Ridge())\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 177.96 seconds\nCompute duration  = 177.96 seconds\nparameters        = λ, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    0.0867    0.0533     0.0005    0.0008   3682.1660    0.9999       20.6909\n        β[1]    0.0033    0.0013     0.0000    0.0000   9024.2636    0.9999       50.7092\n        β[2]   -0.0162    0.0577     0.0006    0.0007   5712.7441    1.0000       32.1011\n        β[3]    0.0902    0.0137     0.0001    0.0002   6239.2706    1.0004       35.0598\n        β[4]    0.0220    0.0068     0.0001    0.0001   6004.4582    0.9999       33.7403\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.0354    0.0556    0.0733    0.1018    0.2139\n        β[1]    0.0007    0.0024    0.0033    0.0041    0.0058\n        β[2]   -0.1353   -0.0525   -0.0157    0.0218    0.0958\n        β[3]    0.0634    0.0808    0.0901    0.0992    0.1179\n        β[4]    0.0086    0.0174    0.0221    0.0265    0.0354\njulia> container_cloglog = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cloglog(), Prior_Ridge())\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 94.56 seconds\nCompute duration  = 94.56 seconds\nparameters        = λ, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse       ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64   Float64   Float64       Float64 \n\n           λ    0.4868    0.0003     0.0000    0.0000   20.6437    1.8077        0.2183\n        β[1]   -0.1684    0.0026     0.0000    0.0003   20.2642    2.5746        0.2143\n        β[2]    0.4824    0.0008     0.0000    0.0001   20.4744    2.2619        0.2165\n        β[3]    0.9618    0.0058     0.0001    0.0006   20.2614    2.5797        0.2143\n        β[4]   -0.3887    0.0004     0.0000    0.0000   21.1046    1.7123        0.2232\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.4861    0.4865    0.4869    0.4870    0.4872\n        β[1]   -0.1725   -0.1706   -0.1687   -0.1661   -0.1642\n        β[2]    0.4813    0.4817    0.4823    0.4831    0.4840\n        β[3]    0.9521    0.9566    0.9626    0.9664    0.9707\n        β[4]   -0.3892   -0.3890   -0.3888   -0.3882   -0.3878\njulia> container_cauchit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cauchit(), Prior_Ridge())\n┌ Info: Found initial step size\n└   ϵ = 0.003125\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 153.65 seconds\nCompute duration  = 153.65 seconds\nparameters        = λ, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    0.1737    0.1033     0.0010    0.0015   3731.6701    0.9999       24.2865\n        β[1]    0.0033    0.0024     0.0000    0.0000   8722.9987    1.0000       56.7711\n        β[2]   -0.0598    0.1193     0.0012    0.0017   5364.6587    1.0008       34.9143\n        β[3]    0.2191    0.0365     0.0004    0.0005   5826.8422    0.9999       37.9223\n        β[4]    0.0204    0.0128     0.0001    0.0002   5304.7531    0.9999       34.5245\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.0702    0.1130    0.1483    0.2052    0.4066\n        β[1]   -0.0013    0.0016    0.0032    0.0049    0.0083\n        β[2]   -0.3183   -0.1325   -0.0507    0.0204    0.1532\n        β[3]    0.1488    0.1942    0.2187    0.2429    0.2919\n        β[4]   -0.0046    0.0117    0.0203    0.0292    0.0459\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::CRRaoLink, prior::Prior_Laplace, h::Float64 = 0.1, level::Float64 = 0.95, sim_size::Int64 = 1000)\n\nFit a Bayesian Logistic Regression model on the input data with a Laplace prior with the provided Link function.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> turnout = dataset(\"Zelig\", \"turnout\")\n2000×5 DataFrame\n  Row │ Race   Age    Educate  Income   Vote  \n      │ Cat…   Int32  Float64  Float64  Int32 \n──────┼───────────────────────────────────────\n    1 │ white     60     14.0   3.3458      1\n    2 │ white     51     10.0   1.8561      0\n    3 │ white     24     12.0   0.6304      0\n    4 │ white     38      8.0   3.4183      1\n  ⋮   │   ⋮      ⋮       ⋮        ⋮       ⋮\n 1998 │ white     51     16.0   7.8949      1\n 1999 │ white     22     10.0   2.4811      0\n 2000 │ white     59     10.0   0.5523      0\n                             1993 rows omitted\njulia> container_logit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Logit(), Prior_Laplace())\n┌ Info: Found initial step size\n└   ϵ = 0.003125\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 143.44 seconds\nCompute duration  = 143.44 seconds\nparameters        = λ, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    0.1178    0.0826     0.0008    0.0012   4670.5185    0.9999       32.5619\n        β[1]    0.0051    0.0022     0.0000    0.0000   9160.7544    1.0001       63.8669\n        β[2]   -0.0228    0.0890     0.0009    0.0013   4963.2154    1.0002       34.6025\n        β[3]    0.1628    0.0254     0.0003    0.0004   5795.2458    1.0000       40.4033\n        β[4]    0.0321    0.0118     0.0001    0.0002   5366.5589    1.0006       37.4146\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.0380    0.0677    0.0958    0.1410    0.3341\n        β[1]    0.0007    0.0036    0.0051    0.0066    0.0095\n        β[2]   -0.2299   -0.0690   -0.0133    0.0273    0.1522\n        β[3]    0.1145    0.1454    0.1624    0.1796    0.2133\n        β[4]    0.0090    0.0240    0.0323    0.0400    0.0549\njulia> container_probit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Probit(), Prior_Laplace())\n┌ Info: Found initial step size\n└   ϵ = 0.00078125\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 171.43 seconds\nCompute duration  = 171.43 seconds\nparameters        = λ, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    0.0821    0.0551     0.0006    0.0008   4512.1853    1.0003       26.3206\n        β[1]    0.0033    0.0013     0.0000    0.0000   8915.4805    0.9999       52.0059\n        β[2]   -0.0138    0.0553     0.0006    0.0008   5240.6484    1.0000       30.5698\n        β[3]    0.0916    0.0141     0.0001    0.0002   6402.4324    1.0001       37.3468\n        β[4]    0.0212    0.0070     0.0001    0.0001   5508.5643    1.0000       32.1326\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.0275    0.0477    0.0667    0.0988    0.2270\n        β[1]    0.0008    0.0024    0.0033    0.0042    0.0059\n        β[2]   -0.1346   -0.0444   -0.0088    0.0192    0.0907\n        β[3]    0.0641    0.0820    0.0913    0.1011    0.1195\n        β[4]    0.0074    0.0165    0.0213    0.0260    0.0349\njulia> container_cloglog = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cloglog(), Prior_Laplace())\n┌ Info: Found initial step size\n└   ϵ = 0.0015625\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 194.12 seconds\nCompute duration  = 194.12 seconds\nparameters        = λ, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    0.0731    0.0509     0.0005    0.0007   4981.1484    1.0002       25.6608\n        β[1]    0.0008    0.0012     0.0000    0.0000   9615.6483    1.0003       49.5358\n        β[2]   -0.0266    0.0521     0.0005    0.0007   4812.7260    1.0001       24.7932\n        β[3]    0.0759    0.0114     0.0001    0.0002   5448.6076    0.9999       28.0690\n        β[4]    0.0069    0.0060     0.0001    0.0001   4591.7360    0.9999       23.6547\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.0242    0.0421    0.0600    0.0873    0.2035\n        β[1]   -0.0015    0.0000    0.0008    0.0016    0.0031\n        β[2]   -0.1478   -0.0559   -0.0199    0.0063    0.0647\n        β[3]    0.0538    0.0682    0.0760    0.0836    0.0983\n        β[4]   -0.0045    0.0027    0.0068    0.0111    0.0188\njulia> container_cauchit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cauchit(), Prior_Laplace())\n┌ Info: Found initial step size\n└   ϵ = 0.0330078125\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 151.32 seconds\nCompute duration  = 151.32 seconds\nparameters        = λ, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    0.1426    0.1158     0.0012    0.0019   4085.6946    1.0002       27.0013\n        β[1]    0.0032    0.0024     0.0000    0.0000   6944.6444    1.0001       45.8953\n        β[2]   -0.0474    0.1145     0.0011    0.0016   4859.5428    1.0001       32.1154\n        β[3]    0.2237    0.0363     0.0004    0.0005   4613.9690    0.9999       30.4925\n        β[4]    0.0185    0.0126     0.0001    0.0002   4725.1966    0.9999       31.2275\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.0446    0.0790    0.1127    0.1686    0.4193\n        β[1]   -0.0013    0.0016    0.0032    0.0047    0.0081\n        β[2]   -0.3124   -0.1050   -0.0278    0.0204    0.1493\n        β[3]    0.1551    0.1986    0.2228    0.2472    0.2979\n        β[4]   -0.0055    0.0097    0.0182    0.0269    0.0439\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::CRRaoLink, prior::Prior_Cauchy, h::Float64 = 0.1, level::Float64 = 0.95, sim_size::Int64 = 1000)\n\nFit a Bayesian Logistic Regression model on the input data with a Cauchy prior with the provided Link function.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> turnout = dataset(\"Zelig\", \"turnout\")\n2000×5 DataFrame\n  Row │ Race   Age    Educate  Income   Vote  \n      │ Cat…   Int32  Float64  Float64  Int32 \n──────┼───────────────────────────────────────\n    1 │ white     60     14.0   3.3458      1\n    2 │ white     51     10.0   1.8561      0\n    3 │ white     24     12.0   0.6304      0\n    4 │ white     38      8.0   3.4183      1\n  ⋮   │   ⋮      ⋮       ⋮        ⋮       ⋮\n 1998 │ white     51     16.0   7.8949      1\n 1999 │ white     22     10.0   2.4811      0\n 2000 │ white     59     10.0   0.5523      0\n                             1993 rows omitted\njulia> container_logit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Logit(), Prior_Cauchy())\n┌ Info: Found initial step size\n└   ϵ = 0.003125\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 152.61 seconds\nCompute duration  = 152.61 seconds\nparameters        = λ, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    0.0792    0.0857     0.0009    0.0012   4663.0871    0.9999       30.5560\n        β[1]    0.0052    0.0022     0.0000    0.0000   7200.5641    0.9999       47.1834\n        β[2]   -0.0205    0.0797     0.0008    0.0013   4355.2582    0.9999       28.5389\n        β[3]    0.1653    0.0256     0.0003    0.0003   4895.3528    1.0004       32.0780\n        β[4]    0.0306    0.0117     0.0001    0.0002   3982.8457    1.0001       26.0985\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.0092    0.0297    0.0545    0.0981    0.2908\n        β[1]    0.0010    0.0038    0.0052    0.0067    0.0094\n        β[2]   -0.2124   -0.0533   -0.0088    0.0194    0.1293\n        β[3]    0.1153    0.1481    0.1652    0.1822    0.2164\n        β[4]    0.0080    0.0227    0.0304    0.0384    0.0537\njulia> container_probit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Probit(), Prior_Cauchy())\n┌ Info: Found initial step size\n└   ϵ = 0.00078125\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 188.49 seconds\nCompute duration  = 188.49 seconds\nparameters        = λ, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    0.0484    0.0512     0.0005    0.0007   5054.3994    1.0000       26.8155\n        β[1]    0.0034    0.0013     0.0000    0.0000   8376.0026    0.9999       44.4379\n        β[2]   -0.0101    0.0470     0.0005    0.0007   3497.1991    1.0000       18.5540\n        β[3]    0.0927    0.0142     0.0001    0.0002   5007.2301    1.0000       26.5652\n        β[4]    0.0202    0.0070     0.0001    0.0001   4277.4390    0.9999       22.6934\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.0057    0.0185    0.0335    0.0599    0.1824\n        β[1]    0.0009    0.0025    0.0034    0.0042    0.0059\n        β[2]   -0.1236   -0.0297   -0.0045    0.0135    0.0783\n        β[3]    0.0649    0.0830    0.0927    0.1021    0.1207\n        β[4]    0.0068    0.0155    0.0202    0.0249    0.0343\njulia> container_cloglog = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cloglog(), Prior_Cauchy())\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 121.33 seconds\nCompute duration  = 121.33 seconds\nparameters        = λ, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse       ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64   Float64   Float64       Float64 \n\n           λ    0.2562    0.0001     0.0000    0.0000   24.5965    1.0444        0.2027\n        β[1]    0.0457    0.0000     0.0000    0.0000   30.9386    1.3997        0.2550\n        β[2]    0.3084    0.0008     0.0000    0.0001   20.7337    1.6095        0.1709\n        β[3]    0.0931    0.0023     0.0000    0.0002   20.2943    2.5468        0.1673\n        β[4]   -1.3797    0.0072     0.0001    0.0007   20.2381    2.7801        0.1668\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.2561    0.2562    0.2562    0.2563    0.2564\n        β[1]    0.0456    0.0457    0.0457    0.0457    0.0457\n        β[2]    0.3069    0.3077    0.3083    0.3089    0.3100\n        β[3]    0.0893    0.0912    0.0933    0.0948    0.0971\n        β[4]   -1.3910   -1.3863   -1.3800   -1.3733   -1.3681\njulia> container_cauchit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cauchit(), Prior_Cauchy())\n┌ Info: Found initial step size\n└   ϵ = 0.05\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 169.7 seconds\nCompute duration  = 169.7 seconds\nparameters        = λ, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    0.0759    0.0909     0.0009    0.0014   3905.2459    0.9999       23.0129\n        β[1]    0.0033    0.0023     0.0000    0.0000   5465.8674    1.0000       32.2094\n        β[2]   -0.0318    0.0969     0.0010    0.0015   4113.9104    0.9999       24.2425\n        β[3]    0.2285    0.0364     0.0004    0.0007   2716.3177    1.0006       16.0068\n        β[4]    0.0158    0.0124     0.0001    0.0002   3167.8247    1.0000       18.6674\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.0050    0.0211    0.0451    0.0970    0.3174\n        β[1]   -0.0011    0.0018    0.0033    0.0048    0.0078\n        β[2]   -0.2849   -0.0621   -0.0083    0.0151    0.1277\n        β[3]    0.1586    0.2033    0.2276    0.2529    0.3004\n        β[4]   -0.0067    0.0069    0.0153    0.0239    0.0415\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::CRRaoLink, prior::Prior_TDist, h::Float64 = 1.0, level::Float64 = 0.95, sim_size::Int64 = 1000)\n\nFit a Bayesian Logistic Regression model on the input data with a T-Dist prior with the provided Link function.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> turnout = dataset(\"Zelig\", \"turnout\")\n2000×5 DataFrame\n  Row │ Race   Age    Educate  Income   Vote  \n      │ Cat…   Int32  Float64  Float64  Int32 \n──────┼───────────────────────────────────────\n    1 │ white     60     14.0   3.3458      1\n    2 │ white     51     10.0   1.8561      0\n    3 │ white     24     12.0   0.6304      0\n    4 │ white     38      8.0   3.4183      1\n  ⋮   │   ⋮      ⋮       ⋮        ⋮       ⋮\n 1998 │ white     51     16.0   7.8949      1\n 1999 │ white     22     10.0   2.4811      0\n 2000 │ white     59     10.0   0.5523      0\n                             1993 rows omitted\njulia> container_logit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Logit(), Prior_TDist())\n┌ Info: Found initial step size\n└   ϵ = 0.003125\nChains MCMC chain (10000×18×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 191.64 seconds\nCompute duration  = 191.64 seconds\nparameters        = λ, ν, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean        std   naive_se      mcse          ess      rhat   ess_per_sec \n      Symbol   Float64    Float64    Float64   Float64      Float64   Float64       Float64 \n\n           λ    0.3043     0.1678     0.0017    0.0022    5712.7538    1.0000       29.8104\n           ν   27.2612   549.4979     5.4950    7.5617    5110.0078    1.0002       26.6652\n        β[1]    0.0052     0.0023     0.0000    0.0000   11260.6563    1.0004       58.7607\n        β[2]   -0.0589     0.1247     0.0012    0.0012    8832.9112    1.0000       46.0921\n        β[3]    0.1667     0.0255     0.0003    0.0003    8308.4832    1.0000       43.3555\n        β[4]    0.0333     0.0123     0.0001    0.0001    7630.2720    0.9999       39.8165\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.1211    0.1958    0.2627    0.3635    0.7244\n           ν    0.5077    1.4402    2.9883    7.2135   79.1950\n        β[1]    0.0008    0.0037    0.0052    0.0067    0.0097\n        β[2]   -0.3138   -0.1399   -0.0546    0.0244    0.1796\n        β[3]    0.1177    0.1491    0.1665    0.1841    0.2177\n        β[4]    0.0094    0.0251    0.0334    0.0414    0.0575\njulia> container_probit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Probit(), Prior_TDist())\n┌ Info: Found initial step size\n└   ϵ = 0.00078125\nChains MCMC chain (10000×18×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 262.37 seconds\nCompute duration  = 262.37 seconds\nparameters        = λ, ν, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean        std   naive_se      mcse          ess      rhat   ess_per_sec \n      Symbol   Float64    Float64    Float64   Float64      Float64   Float64       Float64 \n\n           λ    0.2694     0.1551     0.0016    0.0020    5091.0519    1.0002       19.4040\n           ν   21.1549   329.1679     3.2917    4.5051    5189.9802    1.0000       19.7811\n        β[1]    0.0034     0.0013     0.0000    0.0000   12584.4962    0.9999       47.9645\n        β[2]   -0.0331     0.0779     0.0008    0.0009    6959.6501    0.9999       26.5260\n        β[3]    0.0936     0.0138     0.0001    0.0002    6043.0747    1.0012       23.0326\n        β[4]    0.0218     0.0072     0.0001    0.0001    5938.2251    1.0010       22.6329\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.1104    0.1761    0.2314    0.3177    0.6395\n           ν    0.4766    1.3862    2.9031    6.9434   79.8040\n        β[1]    0.0008    0.0025    0.0034    0.0043    0.0060\n        β[2]   -0.1901   -0.0841   -0.0325    0.0184    0.1189\n        β[3]    0.0671    0.0843    0.0935    0.1027    0.1210\n        β[4]    0.0077    0.0169    0.0217    0.0267    0.0361\njulia> container_cloglog = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cloglog(), Prior_TDist())\n┌ Info: Found initial step size\n└   ϵ = 0.0015625\nChains MCMC chain (10000×18×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 241.88 seconds\nCompute duration  = 241.88 seconds\nparameters        = λ, ν, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean        std   naive_se      mcse          ess      rhat   ess_per_sec \n      Symbol   Float64    Float64    Float64   Float64      Float64   Float64       Float64 \n\n           λ    0.2705     0.1490     0.0015    0.0018    6373.8367    1.0001       26.3518\n           ν   25.1429   513.9686     5.1397    7.9414    4083.5546    1.0000       16.8829\n        β[1]    0.0010     0.0012     0.0000    0.0000   11899.8637    0.9999       49.1984\n        β[2]   -0.0562     0.0693     0.0007    0.0009    6611.6159    0.9999       27.3348\n        β[3]    0.0774     0.0115     0.0001    0.0001    6350.5188    0.9999       26.2554\n        β[4]    0.0081     0.0066     0.0001    0.0001    5974.2918    1.0000       24.6999\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.1079    0.1732    0.2318    0.3236    0.6667\n           ν    0.4629    1.3747    2.7811    6.9236   88.1070\n        β[1]   -0.0014    0.0001    0.0010    0.0018    0.0034\n        β[2]   -0.1960   -0.1021   -0.0563   -0.0089    0.0781\n        β[3]    0.0549    0.0696    0.0775    0.0851    0.0996\n        β[4]   -0.0050    0.0036    0.0081    0.0126    0.0209\njulia> container_cauchit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cauchit(), Prior_TDist())\n┌ Info: Found initial step size\n└   ϵ = 0.009375000000000001\nChains MCMC chain (10000×18×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 224.46 seconds\nCompute duration  = 224.46 seconds\nparameters        = λ, ν, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean        std   naive_se      mcse          ess      rhat   ess_per_sec \n      Symbol   Float64    Float64    Float64   Float64      Float64   Float64       Float64 \n\n           λ    0.3293     0.1847     0.0018    0.0022    6475.4472    1.0000       28.8487\n           ν   16.2524   148.5657     1.4857    2.0404    5495.7033    1.0003       24.4839\n        β[1]    0.0036     0.0025     0.0000    0.0000   10948.8161    0.9999       48.7780\n        β[2]   -0.1076     0.1519     0.0015    0.0016    7435.6058    1.0000       33.1263\n        β[3]    0.2321     0.0361     0.0004    0.0004    7391.3297    0.9999       32.9291\n        β[4]    0.0198     0.0133     0.0001    0.0002    6891.4114    1.0000       30.7019\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.1305    0.2117    0.2828    0.3905    0.7982\n           ν    0.5166    1.4512    2.9337    6.9187   79.9496\n        β[1]   -0.0011    0.0019    0.0036    0.0052    0.0087\n        β[2]   -0.4347   -0.2049   -0.0983   -0.0031    0.1670\n        β[3]    0.1629    0.2075    0.2313    0.2560    0.3057\n        β[4]   -0.0058    0.0106    0.0196    0.0287    0.0461\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::CRRaoLink, prior::Prior_Uniform, h::Float64 = 0.01, level::Float64 = 0.95, sim_size::Int64 = 1000)\n\nFit a Bayesian Logistic Regression model on the input data with a Uniform prior with the provided Link function.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> turnout = dataset(\"Zelig\", \"turnout\")\n2000×5 DataFrame\n  Row │ Race   Age    Educate  Income   Vote  \n      │ Cat…   Int32  Float64  Float64  Int32 \n──────┼───────────────────────────────────────\n    1 │ white     60     14.0   3.3458      1\n    2 │ white     51     10.0   1.8561      0\n    3 │ white     24     12.0   0.6304      0\n    4 │ white     38      8.0   3.4183      1\n  ⋮   │   ⋮      ⋮       ⋮        ⋮       ⋮\n 1998 │ white     51     16.0   7.8949      1\n 1999 │ white     22     10.0   2.4811      0\n 2000 │ white     59     10.0   0.5523      0\n                             1993 rows omitted\njulia> container_logit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Logit(), Prior_Uniform())\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 189.43 seconds\nCompute duration  = 189.43 seconds\nparameters        = v, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters                                                                                                             ⋯\n      Symbol                                                                                                             ⋯\n\n           v   675917680092296823408089342391941239673271191525001008413719254637203390096714276526402253793438558348179 ⋯\n        β[1]                                                                                                             ⋯\n        β[2]                                                                                                             ⋯\n        β[3]                                                                                                             ⋯\n        β[4]                                                                                                             ⋯\n                                                                                                         7 columns omitted\n\nQuantiles\n  parameters      2.5%                25.0%                                  50.0%                                       ⋯\n      Symbol   Float64              Float64                                Float64                                       ⋯\n\n           v   10.0068   6671382021570.9727   2875917206819862279706875265024.0000   10765098578457618185304163237787764 ⋯\n        β[1]    0.0025               0.0052                                 0.0066                                       ⋯\n        β[2]   -0.2792              -0.2792                                -0.2792                                       ⋯\n        β[3]    0.1295               0.1615                                 0.1791                                       ⋯\n        β[4]    0.0180               0.0327                                 0.0399                                       ⋯\n                                                                                                         2 columns omitted\njulia> container_probit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Probit(), Prior_Uniform())\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 271.82 seconds\nCompute duration  = 271.82 seconds\nparameters        = v, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters                                                                                                             ⋯\n      Symbol                                                                                                             ⋯\n\n           v   315982558180625088687517999180569534700873081964494331646387645413807923241067841219325555242963795532234 ⋯\n        β[1]                                                                                                             ⋯\n        β[2]                                                                                                             ⋯\n        β[3]                                                                                                             ⋯\n        β[4]                                                                                                             ⋯\n                                                                                                         7 columns omitted\n\nQuantiles\n  parameters      2.5%               25.0%                                 50.0%                                         ⋯\n      Symbol   Float64             Float64                               Float64                                         ⋯\n\n           v    2.9888   500027163480.9627   114267416620826600088306450432.0000   8731986448381257740408362721640235131 ⋯\n        β[1]    0.0008              0.0025                                0.0033                                         ⋯\n        β[2]   -0.2059             -0.0982                               -0.0400                                         ⋯\n        β[3]    0.0668              0.0850                                0.0941                                         ⋯\n        β[4]    0.0078              0.0169                                0.0220                                         ⋯\n                                                                                                         2 columns omitted\njulia> container_cloglog = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cloglog(), Prior_Uniform())\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 221.98 seconds\nCompute duration  = 221.98 seconds\nparameters        = v, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters                                                                                                             ⋯\n      Symbol                                                                                                             ⋯\n\n           v   997108327601492157485369149438354840181147942886188650973018774473864498882782024293920677223471145239174 ⋯\n        β[1]                                                                                                             ⋯\n        β[2]                                                                                                             ⋯\n        β[3]                                                                                                             ⋯\n        β[4]                                                                                                             ⋯\n                                                                                                         7 columns omitted\n\nQuantiles\n  parameters      2.5%                25.0%                                50.0%                                         ⋯\n      Symbol   Float64              Float64                              Float64                                         ⋯\n\n           v    2.9344   1617934162465.7087   79417083014024744675909304320.0000   6335481071385452850562280409696628675 ⋯\n        β[1]   -0.0013               0.0002                               0.0010                                         ⋯\n        β[2]   -0.2056              -0.1137                              -0.0639                                         ⋯\n        β[3]    0.0555               0.0705                               0.0779                                         ⋯\n        β[4]   -0.0042               0.0036                               0.0079                                         ⋯\n                                                                                                         2 columns omitted\njulia> container_cauchit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cauchit(), Prior_Uniform())\nChains MCMC chain (10000×17×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 250.81 seconds\nCompute duration  = 250.81 seconds\nparameters        = v, β[1], β[2], β[3], β[4]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters                                                                                                             ⋯\n      Symbol                                                                                                             ⋯\n\n           v   145105825023746239211260804740935487372396420958652923206621612953582232010367942920611301369886999826663 ⋯\n        β[1]                                                                                                             ⋯\n        β[2]                                                                                                             ⋯\n        β[3]                                                                                                             ⋯\n        β[4]                                                                                                             ⋯\n                                                                                                         7 columns omitted\n\nQuantiles\n  parameters      2.5%               25.0%                                50.0%                                          ⋯\n      Symbol   Float64             Float64                              Float64                                          ⋯\n\n           v   10.8874   371822401390.3905   15665245298723267168052445184.0000   97091064597143721776615147479481412134 ⋯\n        β[1]   -0.0009              0.0022                               0.0039                                          ⋯\n        β[2]   -0.5461             -0.2897                              -0.1591                                          ⋯\n        β[3]    0.1679              0.2138                               0.2371                                          ⋯\n        β[4]   -0.0047              0.0122                               0.0212                                          ⋯\n                                                                                                         2 columns omitted\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::CRRaoLink, prior::Prior_HorseShoe, h::Float64 = 0.01, level::Float64 = 0.95, sim_size::Int64 = 1000)\n\nFit a Bayesian Logistic Regression model on the input data with a HorseShoe prior with the provided Link function.\n\nExample\n\n```julia-repl julia> using CRRao, RDatasets, StableRNGs, StatsModels julia> CRRao.setrng(StableRNG(123)) julia> turnout = dataset(\"Zelig\", \"turnout\"); julia> containerlogit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Logit(), PriorHorseShoe()) julia> containerprobit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Probit(), PriorHorseShoe()) julia> containercloglog = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cloglog(), PriorHorseShoe()) julia> containercauchit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cauchit(), Prior_HorseShoe())\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::NegBinomRegression, prior::Prior_Ridge, h::Float64 = 0.1, sim_size::Int64 = 1000)\n\nFit a Bayesian Negative Binomial Regression model on the input data with a Ridge prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> sanction = dataset(\"Zelig\", \"sanction\")\n78×8 DataFrame\n Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         \n     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          \n─────┼───────────────────────────────────────────────────────────────────\n   1 │     1      4       3       1       1      4     15  major loss\n   2 │     0      2       3       0       1      3      4  modest loss\n   3 │     0      1       3       1       0      2      1  little effect\n   4 │     1      1       3       1       1      2      1  little effect\n  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮\n  76 │     0      4       3       1       0      2     13  little effect\n  77 │     0      1       2       0       0      1      1  net gain\n  78 │     1      3       1       1       1      2     10  little effect\n                                                          71 rows omitted\njulia> container = fit(@formula(Num ~ Target + Coop + NCost), sanction, NegBinomRegression(), Prior_Ridge())\n┌ Info: Found initial step size\n└   ϵ = 0.025\nChains MCMC chain (10000×19×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 26.52 seconds\nCompute duration  = 26.52 seconds\nparameters        = λ, α, β[1], β[2], β[3], β[4], β[5]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    2.0416    0.4460     0.0045    0.0045   8499.3498    0.9999      320.5246\n           α   -1.0792    0.5148     0.0051    0.0089   3405.4069    1.0010      128.4235\n        β[1]   -0.0049    0.1614     0.0016    0.0023   4627.1117    1.0009      174.4960\n        β[2]    1.0615    0.1319     0.0013    0.0020   5046.9022    1.0001      190.3270\n        β[3]   -0.1757    0.5563     0.0056    0.0063   8056.2338    1.0001      303.8139\n        β[4]    1.2810    0.3214     0.0032    0.0035   6779.1552    0.9999      255.6532\n        β[5]    0.1493    0.2799     0.0028    0.0036   6164.9114    1.0004      232.4890\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    1.3159    1.7243    1.9928    2.3049    3.0445\n           α   -2.0865   -1.4300   -1.0908   -0.7306   -0.0721\n        β[1]   -0.3180   -0.1136   -0.0044    0.1053    0.3146\n        β[2]    0.8046    0.9738    1.0594    1.1483    1.3262\n        β[3]   -1.2332   -0.5561   -0.1992    0.2020    0.9502\n        β[4]    0.6571    1.0654    1.2744    1.4900    1.9274\n        β[5]   -0.4064   -0.0370    0.1501    0.3388    0.6903\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::NegBinomRegression, prior::Prior_Laplace, h::Float64 = 0.01, sim_size::Int64 = 1000)\n\nFit a Bayesian Negative Binomial Regression model on the input data with a Laplace prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> sanction = dataset(\"Zelig\", \"sanction\")\n78×8 DataFrame\n Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         \n     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          \n─────┼───────────────────────────────────────────────────────────────────\n   1 │     1      4       3       1       1      4     15  major loss\n   2 │     0      2       3       0       1      3      4  modest loss\n   3 │     0      1       3       1       0      2      1  little effect\n   4 │     1      1       3       1       1      2      1  little effect\n  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮\n  76 │     0      4       3       1       0      2     13  little effect\n  77 │     0      1       2       0       0      1      1  net gain\n  78 │     1      3       1       1       1      2     10  little effect\n                                                          71 rows omitted\njulia> container = fit(@formula(Num ~ Target + Coop + NCost), sanction, NegBinomRegression(), Prior_Laplace())\n┌ Info: Found initial step size\n└   ϵ = 0.05\nChains MCMC chain (10000×19×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 26.96 seconds\nCompute duration  = 26.96 seconds\nparameters        = λ, α, β[1], β[2], β[3], β[4], β[5]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    2.1058    0.4611     0.0046    0.0052   8213.6672    0.9999      304.6048\n           α   -1.0014    0.5020     0.0050    0.0084   3465.0499    1.0000      128.5018\n        β[1]   -0.0207    0.1583     0.0016    0.0021   5223.4434    0.9999      193.7120\n        β[2]    1.0465    0.1301     0.0013    0.0017   5029.9415    1.0000      186.5359\n        β[3]   -0.1426    0.4996     0.0050    0.0057   7487.9201    0.9999      277.6903\n        β[4]    1.2832    0.3245     0.0032    0.0035   6912.6238    0.9999      256.3554\n        β[5]    0.1198    0.2656     0.0027    0.0039   5505.7699    1.0000      204.1821\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    1.3431    1.7782    2.0523    2.3788    3.1662\n           α   -2.0082   -1.3266   -1.0000   -0.6730   -0.0202\n        β[1]   -0.3373   -0.1240   -0.0190    0.0823    0.2921\n        β[2]    0.7927    0.9595    1.0454    1.1337    1.3056\n        β[3]   -1.1412   -0.4702   -0.1379    0.1801    0.8557\n        β[4]    0.6480    1.0707    1.2824    1.4966    1.9203\n        β[5]   -0.4026   -0.0558    0.1158    0.2980    0.6499\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::NegBinomRegression, prior::Prior_Cauchy, h::Float64 = 1.0, sim_size::Int64 = 1000)\n\nFit a Bayesian Negative Binomial Regression model on the input data with a Cauchy prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> sanction = dataset(\"Zelig\", \"sanction\")\n78×8 DataFrame\n Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         \n     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          \n─────┼───────────────────────────────────────────────────────────────────\n   1 │     1      4       3       1       1      4     15  major loss\n   2 │     0      2       3       0       1      3      4  modest loss\n   3 │     0      1       3       1       0      2      1  little effect\n   4 │     1      1       3       1       1      2      1  little effect\n  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮\n  76 │     0      4       3       1       0      2     13  little effect\n  77 │     0      1       2       0       0      1      1  net gain\n  78 │     1      3       1       1       1      2     10  little effect\n                                                          71 rows omitted\njulia> container = fit(@formula(Num ~ Target + Coop + NCost), sanction, NegBinomRegression(), Prior_Cauchy())\n┌ Info: Found initial step size\n└   ϵ = 0.2\nChains MCMC chain (10000×19×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 27.58 seconds\nCompute duration  = 27.58 seconds\nparameters        = λ, α, β[1], β[2], β[3], β[4], β[5]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    2.0219    0.4304     0.0043    0.0047   7839.1614    1.0001      284.1923\n           α   -1.0233    0.5192     0.0052    0.0091   3193.5541    1.0010      115.7756\n        β[1]   -0.0192    0.1632     0.0016    0.0025   4320.9927    1.0006      156.6485\n        β[2]    1.0535    0.1327     0.0013    0.0021   4739.9448    1.0008      171.8367\n        β[3]   -0.1552    0.5453     0.0055    0.0069   7763.7273    1.0002      281.4576\n        β[4]    1.2743    0.3250     0.0032    0.0041   6655.6093    1.0008      241.2851\n        β[5]    0.1298    0.2822     0.0028    0.0036   5253.2578    1.0000      190.4458\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    1.3226    1.7126    1.9731    2.2757    2.9804\n           α   -2.0538   -1.3647   -1.0180   -0.6733   -0.0207\n        β[1]   -0.3375   -0.1285   -0.0189    0.0881    0.3042\n        β[2]    0.8001    0.9647    1.0516    1.1418    1.3138\n        β[3]   -1.1825   -0.5301   -0.1676    0.2010    0.9589\n        β[4]    0.6478    1.0553    1.2704    1.4870    1.9319\n        β[5]   -0.4131   -0.0613    0.1305    0.3166    0.6901\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::NegBinomRegression, prior::Prior_TDist, h::Float64 = 1.0, sim_size::Int64 = 1000)\n\nFit a Bayesian Negative Binomial Regression model on the input data with a t(ν) distributed prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> sanction = dataset(\"Zelig\", \"sanction\")\n78×8 DataFrame\n Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         \n     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          \n─────┼───────────────────────────────────────────────────────────────────\n   1 │     1      4       3       1       1      4     15  major loss\n   2 │     0      2       3       0       1      3      4  modest loss\n   3 │     0      1       3       1       0      2      1  little effect\n   4 │     1      1       3       1       1      2      1  little effect\n  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮\n  76 │     0      4       3       1       0      2     13  little effect\n  77 │     0      1       2       0       0      1      1  net gain\n  78 │     1      3       1       1       1      2     10  little effect\n                                                          71 rows omitted\njulia> container = fit(@formula(Num ~ Target + Coop + NCost), sanction, NegBinomRegression(), Prior_TDist())\n┌ Info: Found initial step size\n└   ϵ = 0.05\nChains MCMC chain (10000×20×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 32.4 seconds\nCompute duration  = 32.4 seconds\nparameters        = λ, ν, α, β[1], β[2], β[3], β[4], β[5]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean        std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64    Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    2.0021     0.4262     0.0043    0.0048   7946.6182    0.9999      245.2887\n           ν   20.4978   213.5274     2.1353    2.7473   6455.1193    0.9999      199.2505\n           α   -1.0562     0.5154     0.0052    0.0076   4162.0565    1.0010      128.4704\n        β[1]   -0.0096     0.1617     0.0016    0.0022   5232.3275    1.0005      161.5065\n        β[2]    1.0581     0.1308     0.0013    0.0016   5850.3314    1.0004      180.5825\n        β[3]   -0.1725     0.5396     0.0054    0.0056   7961.1718    0.9999      245.7379\n        β[4]    1.2762     0.3222     0.0032    0.0036   7541.0855    0.9999      232.7711\n        β[5]    0.1400     0.2822     0.0028    0.0037   6538.0847    1.0009      201.8114\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%      97.5% \n      Symbol   Float64   Float64   Float64   Float64    Float64 \n\n           λ    1.2972    1.6983    1.9536    2.2528     2.9704\n           ν    0.6529    1.9144    3.8831    9.7303   108.9970\n           α   -2.0694   -1.4014   -1.0565   -0.7076    -0.0696\n        β[1]   -0.3338   -0.1176   -0.0090    0.0979     0.3056\n        β[2]    0.8046    0.9695    1.0576    1.1482     1.3157\n        β[3]   -1.2170   -0.5366   -0.1879    0.1806     0.9140\n        β[4]    0.6519    1.0551    1.2762    1.4910     1.9133\n        β[5]   -0.4045   -0.0511    0.1377    0.3314     0.6957\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::NegBinomRegression, prior::Prior_Uniform, h::Float64 = 0.1, sim_size::Int64 = 1000)\n\nFit a Bayesian Negative Binomial Regression model on the input data with a Uniform prior. Ibrahim and Laud (JASA, 1990) showed that the uniform flat priors for GLMs can lead to improper posterior distributions thus making them undesirable. In such cases, the Markov Chain struggles to converge. Even if it converges, results are unreliable.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> sanction = dataset(\"Zelig\", \"sanction\")\n78×8 DataFrame\n Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         \n     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          \n─────┼───────────────────────────────────────────────────────────────────\n   1 │     1      4       3       1       1      4     15  major loss\n   2 │     0      2       3       0       1      3      4  modest loss\n   3 │     0      1       3       1       0      2      1  little effect\n   4 │     1      1       3       1       1      2      1  little effect\n  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮\n  76 │     0      4       3       1       0      2     13  little effect\n  77 │     0      1       2       0       0      1      1  net gain\n  78 │     1      3       1       1       1      2     10  little effect\n                                                          71 rows omitted\njulia> container = fit(@formula(Num ~ Target + Coop + NCost), sanction, NegBinomRegression(), Prior_Uniform())\nChains MCMC chain (10000×19×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 294.79 seconds\nCompute duration  = 294.79 seconds\nparameters        = λ, α, β[1], β[2], β[3], β[4], β[5]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    0.9182    0.1461     0.0015    0.0016   9715.0842    1.0003       32.9562\n           α    0.2792    0.0000     0.0000    0.0000     20.5530    0.9999        0.0697\n        β[1]    0.2792    0.0000     0.0000    0.0000     20.5530    0.9999        0.0697\n        β[2]    0.2792    0.0000     0.0000    0.0000     20.5530    0.9999        0.0697\n        β[3]   -0.1238    0.2502     0.0025    0.0234     24.2396    1.5013        0.0822\n        β[4]    0.2792    0.0000     0.0000    0.0000     20.5530    0.9999        0.0697\n        β[5]   -0.2643    0.0899     0.0009    0.0075     81.7627    1.0276        0.2774\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.6696    0.8140    0.9070    1.0073    1.2411\n           α    0.2792    0.2792    0.2792    0.2792    0.2792\n        β[1]    0.2792    0.2792    0.2792    0.2792    0.2792\n        β[2]    0.2792    0.2792    0.2792    0.2792    0.2792\n        β[3]   -0.2792   -0.2792   -0.2792    0.2792    0.2792\n        β[4]    0.2792    0.2792    0.2792    0.2792    0.2792\n        β[5]   -0.2792   -0.2792   -0.2792   -0.2792    0.2792\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::NegBinomRegression, prior::Prior_HorseShoe, sim_size::Int64 = 1000)\n\nFit a Bayesian Negative Binomial Regression model on the input data with a HorseShoe prior. \n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsPlots, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\njulia> sanction = dataset(\"Zelig\", \"sanction\");\njulia> container = fit(@formula(Num ~ Target + Coop + NCost), sanction, NegBinomRegression(), Prior_HorseShoe())\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::PoissonRegression, prior::Prior_Ridge, h::Float64 = 0.1, sim_size::Int64 = 1000)\n\nFit a Bayesian Poisson Regression model on the input data with a Ridge prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> sanction = dataset(\"Zelig\", \"sanction\")\n78×8 DataFrame\n Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         \n     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          \n─────┼───────────────────────────────────────────────────────────────────\n   1 │     1      4       3       1       1      4     15  major loss\n   2 │     0      2       3       0       1      3      4  modest loss\n   3 │     0      1       3       1       0      2      1  little effect\n   4 │     1      1       3       1       1      2      1  little effect\n  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮\n  76 │     0      4       3       1       0      2     13  little effect\n  77 │     0      1       2       0       0      1      1  net gain\n  78 │     1      3       1       1       1      2     10  little effect\n                                                          71 rows omitted\njulia> container = fit(@formula(Num ~ Target + Coop + NCost), sanction, PoissonRegression(), Prior_Ridge())\n┌ Info: Found initial step size\n└   ϵ = 0.025\nChains MCMC chain (10000×19×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 28.3 seconds\nCompute duration  = 28.3 seconds\nparameters        = λ, α, β[1], β[2], β[3], β[4], β[5]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    1.3118    0.4894     0.0049    0.0066   5733.9828    1.0000      202.6214\n           α   -1.8003    0.2607     0.0026    0.0038   4247.2367    1.0000      150.0843\n        β[1]    0.1392    0.0656     0.0007    0.0008   5949.9827    1.0000      210.2542\n        β[2]    1.1334    0.0563     0.0006    0.0007   5344.6101    1.0003      188.8622\n        β[3]   -0.3259    0.2281     0.0023    0.0026   7065.4440    0.9999      249.6712\n        β[4]    1.6983    0.0988     0.0010    0.0012   6534.2641    1.0001      230.9009\n        β[5]    0.4053    0.1688     0.0017    0.0023   5330.2762    1.0006      188.3556\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.7113    0.9826    1.2040    1.5098    2.5620\n           α   -2.3202   -1.9764   -1.7971   -1.6229   -1.3003\n        β[1]    0.0115    0.0950    0.1399    0.1825    0.2690\n        β[2]    1.0246    1.0950    1.1331    1.1712    1.2451\n        β[3]   -0.7923   -0.4776   -0.3205   -0.1703    0.1022\n        β[4]    1.5095    1.6308    1.6977    1.7645    1.8936\n        β[5]    0.0755    0.2930    0.4068    0.5190    0.7331\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::PoissonRegression, prior::Prior_Laplace, h::Float64 = 0.1, sim_size::Int64 = 1000)\n\nFit a Bayesian Poisson Regression model on the input data with a Laplace prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> sanction = dataset(\"Zelig\", \"sanction\")\n78×8 DataFrame\n Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         \n     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          \n─────┼───────────────────────────────────────────────────────────────────\n   1 │     1      4       3       1       1      4     15  major loss\n   2 │     0      2       3       0       1      3      4  modest loss\n   3 │     0      1       3       1       0      2      1  little effect\n   4 │     1      1       3       1       1      2      1  little effect\n  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮\n  76 │     0      4       3       1       0      2     13  little effect\n  77 │     0      1       2       0       0      1      1  net gain\n  78 │     1      3       1       1       1      2     10  little effect\n                                                          71 rows omitted\njulia> container = fit(@formula(Num ~ Target + Coop + NCost), sanction, PoissonRegression(), Prior_Laplace())\n┌ Info: Found initial step size\n└   ϵ = 0.025\nChains MCMC chain (10000×19×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 26.38 seconds\nCompute duration  = 26.38 seconds\nparameters        = λ, α, β[1], β[2], β[3], β[4], β[5]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    1.1036    0.5676     0.0057    0.0086   5101.9856    1.0003      193.4109\n           α   -1.7912    0.2625     0.0026    0.0041   4611.2398    1.0002      174.8072\n        β[1]    0.1360    0.0649     0.0006    0.0008   6345.1220    0.9999      240.5369\n        β[2]    1.1324    0.0561     0.0006    0.0008   6267.6347    1.0006      237.5994\n        β[3]   -0.2965    0.2234     0.0022    0.0027   7304.0984    1.0001      276.8906\n        β[4]    1.7010    0.1012     0.0010    0.0013   7420.3061    0.9999      281.2960\n        β[5]    0.3928    0.1730     0.0017    0.0021   6264.6983    0.9999      237.4881\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.4544    0.7383    0.9620    1.3049    2.5869\n           α   -2.3130   -1.9684   -1.7862   -1.6133   -1.2838\n        β[1]    0.0093    0.0924    0.1354    0.1801    0.2627\n        β[2]    1.0241    1.0943    1.1313    1.1698    1.2448\n        β[3]   -0.7542   -0.4437   -0.2889   -0.1370    0.1132\n        β[4]    1.5029    1.6331    1.6994    1.7690    1.9002\n        β[5]    0.0581    0.2740    0.3946    0.5113    0.7309\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression, prior::Prior_Cauchy, h::Float64 = 1.0, sim_size::Int64 = 1000)\n\nFit a Bayesian Poisson Regression model on the input data with a Cauchy prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> sanction = dataset(\"Zelig\", \"sanction\")\n78×8 DataFrame\n Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         \n     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          \n─────┼───────────────────────────────────────────────────────────────────\n   1 │     1      4       3       1       1      4     15  major loss\n   2 │     0      2       3       0       1      3      4  modest loss\n   3 │     0      1       3       1       0      2      1  little effect\n   4 │     1      1       3       1       1      2      1  little effect\n  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮\n  76 │     0      4       3       1       0      2     13  little effect\n  77 │     0      1       2       0       0      1      1  net gain\n  78 │     1      3       1       1       1      2     10  little effect\n                                                          71 rows omitted\njulia> container = fit(@formula(Num ~ Target + Coop + NCost), sanction, PoissonRegression(), Prior_Cauchy())\n┌ Info: Found initial step size\n└   ϵ = 0.025\nChains MCMC chain (10000×19×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 27.23 seconds\nCompute duration  = 27.23 seconds\nparameters        = λ, α, β[1], β[2], β[3], β[4], β[5]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    0.8558    0.4620     0.0046    0.0050   7120.2358    0.9999      261.5138\n           α   -1.7984    0.2622     0.0026    0.0038   4736.5277    0.9999      173.9644\n        β[1]    0.1383    0.0649     0.0006    0.0008   6989.3372    1.0001      256.7061\n        β[2]    1.1322    0.0573     0.0006    0.0008   5442.3181    0.9999      199.8868\n        β[3]   -0.2928    0.2169     0.0022    0.0025   6830.7146    1.0000      250.8802\n        β[4]    1.7040    0.0974     0.0010    0.0011   6738.4680    0.9999      247.4921\n        β[5]    0.3945    0.1673     0.0017    0.0023   5730.9957    0.9999      210.4894\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.2927    0.5424    0.7551    1.0504    1.9964\n           α   -2.3125   -1.9749   -1.7957   -1.6220   -1.2893\n        β[1]    0.0112    0.0950    0.1366    0.1813    0.2677\n        β[2]    1.0198    1.0937    1.1315    1.1709    1.2457\n        β[3]   -0.7403   -0.4351   -0.2887   -0.1398    0.1058\n        β[4]    1.5135    1.6384    1.7053    1.7704    1.8926\n        β[5]    0.0677    0.2823    0.3952    0.5066    0.7253\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::PoissonRegression, prior::Prior_TDist, h::Float64 = 2.0, sim_size::Int64 = 1000)\n\nFit a Bayesian Poisson Regression model on the input data with a t(ν) distributed prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> sanction = dataset(\"Zelig\", \"sanction\")\n78×8 DataFrame\n Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         \n     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          \n─────┼───────────────────────────────────────────────────────────────────\n   1 │     1      4       3       1       1      4     15  major loss\n   2 │     0      2       3       0       1      3      4  modest loss\n   3 │     0      1       3       1       0      2      1  little effect\n   4 │     1      1       3       1       1      2      1  little effect\n  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮\n  76 │     0      4       3       1       0      2     13  little effect\n  77 │     0      1       2       0       0      1      1  net gain\n  78 │     1      3       1       1       1      2     10  little effect\n                                                          71 rows omitted\njulia> container = fit(@formula(Num ~ Target + Coop + NCost), sanction, PoissonRegression(), Prior_TDist())\n┌ Info: Found initial step size\n└   ϵ = 0.0125\nChains MCMC chain (10000×20×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 28.44 seconds\nCompute duration  = 28.44 seconds\nparameters        = λ, ν, α, β[1], β[2], β[3], β[4], β[5]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 \n\n           λ    0.9887    0.4187     0.0042    0.0046   6943.4330    1.0001      244.1861\n           ν    3.0837    7.9963     0.0800    0.1140   4422.8043    1.0000      155.5409\n           α   -1.8065    0.2648     0.0026    0.0042   3384.5428    0.9999      119.0274\n        β[1]    0.1399    0.0656     0.0007    0.0009   5242.0449    1.0001      184.3518\n        β[2]    1.1339    0.0565     0.0006    0.0009   4397.9611    1.0004      154.6672\n        β[3]   -0.3097    0.2208     0.0022    0.0029   5930.8888    1.0000      208.5771\n        β[4]    1.7026    0.1000     0.0010    0.0012   5706.3129    0.9999      200.6792\n        β[5]    0.4025    0.1701     0.0017    0.0024   4239.8288    0.9999      149.1060\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.4024    0.7020    0.9159    1.1889    2.0151\n           ν    0.5755    1.1693    1.8160    3.0737   12.7800\n           α   -2.3383   -1.9843   -1.8024   -1.6222   -1.3155\n        β[1]    0.0116    0.0952    0.1392    0.1836    0.2704\n        β[2]    1.0255    1.0953    1.1331    1.1717    1.2464\n        β[3]   -0.7635   -0.4518   -0.3017   -0.1559    0.1005\n        β[4]    1.5112    1.6334    1.7023    1.7700    1.9025\n        β[5]    0.0680    0.2864    0.4016    0.5174    0.7395\n\n\n\n\n\nfit(formula::FormulaTerm, data::DataFrame, modelClass::PoissonRegression, prior::Prior_Uniform, h::Float64 = 1.0, sim_size::Int64 = 1000)\n\nFit a Bayesian Poisson Regression model on the input data with a Uniform prior.\n\nExample\n\njulia> using CRRao, RDatasets, StableRNGs, StatsModels\njulia> CRRao.set_rng(StableRNG(123))\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)\njulia> sanction = dataset(\"Zelig\", \"sanction\")\n78×8 DataFrame\n Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         \n     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          \n─────┼───────────────────────────────────────────────────────────────────\n   1 │     1      4       3       1       1      4     15  major loss\n   2 │     0      2       3       0       1      3      4  modest loss\n   3 │     0      1       3       1       0      2      1  little effect\n   4 │     1      1       3       1       1      2      1  little effect\n  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮\n  76 │     0      4       3       1       0      2     13  little effect\n  77 │     0      1       2       0       0      1      1  net gain\n  78 │     1      3       1       1       1      2     10  little effect\n                                                          71 rows omitted\njulia> container = fit(@formula(Num ~ Target + Coop + NCost), sanction, PoissonRegression(), Prior_Uniform())\nChains MCMC chain (10000×19×1 Array{Float64, 3}):\n\nIterations        = 1001:1:11000\nNumber of chains  = 1\nSamples per chain = 10000\nWall duration     = 226.71 seconds\nCompute duration  = 226.71 seconds\nparameters        = λ, α, β[1], β[2], β[3], β[4], β[5]\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse          ess      rhat   ess_per_sec \n      Symbol   Float64   Float64    Float64   Float64      Float64   Float64       Float64 \n\n           λ    6.8751   14.2909     0.1429    0.9915     139.5154    1.0000        0.6154\n           α    0.2792    0.0000     0.0000    0.0000      20.5530    0.9999        0.0907\n        β[1]    0.2792    0.0000     0.0000    0.0000      20.5530    0.9999        0.0907\n        β[2]    0.2792    0.0000     0.0000    0.0000      20.5530    0.9999        0.0907\n        β[3]    0.2791    0.0056     0.0001    0.0001   10004.0032    1.0000       44.1261\n        β[4]    0.2792    0.0000     0.0000    0.0000      20.5530    0.9999        0.0907\n        β[5]   -0.2792    0.0000     0.0000    0.0000      20.5530    0.9999        0.0907\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           λ    0.6280    1.3666    2.6401    6.3038   39.3119\n           α    0.2792    0.2792    0.2792    0.2792    0.2792\n        β[1]    0.2792    0.2792    0.2792    0.2792    0.2792\n        β[2]    0.2792    0.2792    0.2792    0.2792    0.2792\n        β[3]    0.2792    0.2792    0.2792    0.2792    0.2792\n        β[4]    0.2792    0.2792    0.2792    0.2792    0.2792\n        β[5]   -0.2792   -0.2792   -0.2792   -0.2792   -0.2792\n\n\n\n\n\n","category":"function"},{"location":"api/interface/#Model-Classes","page":"General Interface","title":"Model Classes","text":"","category":"section"},{"location":"api/interface/","page":"General Interface","title":"General Interface","text":"LinearRegression\nLogisticRegression\nNegBinomRegression\nPoissonRegression","category":"page"},{"location":"api/interface/#CRRao.LinearRegression","page":"General Interface","title":"CRRao.LinearRegression","text":"LinearRegression\n\nType representing the Linear Regression model class.\n\n\n\n\n\n","category":"type"},{"location":"api/interface/#CRRao.LogisticRegression","page":"General Interface","title":"CRRao.LogisticRegression","text":"LogisticRegression\n\nType representing the Logistic Regression model class.\n\n\n\n\n\n","category":"type"},{"location":"api/interface/#CRRao.NegBinomRegression","page":"General Interface","title":"CRRao.NegBinomRegression","text":"NegBinomRegression\n\nType representing the Negative Binomial Regression model class.\n\n\n\n\n\n","category":"type"},{"location":"api/interface/#CRRao.PoissonRegression","page":"General Interface","title":"CRRao.PoissonRegression","text":"PoissonRegression\n\nType representing the Poisson Regression model class.\n\n\n\n\n\n","category":"type"},{"location":"api/interface/#Link-functions.","page":"General Interface","title":"Link functions.","text":"","category":"section"},{"location":"api/interface/","page":"General Interface","title":"General Interface","text":"CRRaoLink\nLogit\nProbit\nCloglog\nCauchit","category":"page"},{"location":"api/interface/#CRRao.CRRaoLink","page":"General Interface","title":"CRRao.CRRaoLink","text":"CRRaoLink\n\nAbstract type representing link functions which are used to dispatch to appropriate calls.\n\n\n\n\n\n","category":"type"},{"location":"api/interface/#CRRao.Logit","page":"General Interface","title":"CRRao.Logit","text":"Logit <: CRRaoLink\n\nA type representing the Logit link function, which is defined by the formula\n\nzmapsto dfrac11 + exp(-z)\n\n\n\n\n\n","category":"type"},{"location":"api/interface/#CRRao.Probit","page":"General Interface","title":"CRRao.Probit","text":"Probit <: CRRaoLink\n\nA type representing the Probit link function, which is defined by the formula\n\nzmapsto mathbbPZle z\n\nwhere Zsim textNormal(0 1).\n\n\n\n\n\n","category":"type"},{"location":"api/interface/#CRRao.Cloglog","page":"General Interface","title":"CRRao.Cloglog","text":"Cloglog <: CRRaoLink\n\nA type representing the Cloglog link function, which is defined by the formula \n\nzmapsto 1 - exp(-exp(z))\n\n\n\n\n\n","category":"type"},{"location":"api/interface/#CRRao.Cauchit","page":"General Interface","title":"CRRao.Cauchit","text":"Cauchit <: CRRaoLink\n\nA type representing the Cauchit link function, which is defined by the formula\n\nzmapsto dfrac12 + dfractextatan(z)pi\n\n\n\n\n\n","category":"type"},{"location":"api/interface/#Setting-Random-Number-Generators","page":"General Interface","title":"Setting Random Number Generators","text":"","category":"section"},{"location":"api/interface/","page":"General Interface","title":"General Interface","text":"CRRao.set_rng","category":"page"},{"location":"api/interface/#CRRao.set_rng","page":"General Interface","title":"CRRao.set_rng","text":"set_rng(rng)\n\nSet the random number generator. This is useful if you want to work with reproducible results. rng must be a random number generator.\n\nExample\n\nusing StableRNGs\nCRRao.set_rng(StableRNG(1234))\n\n\n\n\n\n","category":"function"},{"location":"man/guide/#Package-Guide","page":"Guide","title":"Package Guide","text":"","category":"section"},{"location":"man/guide/#Installation","page":"Guide","title":"Installation","text":"","category":"section"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"To install the package, type ] in the Julia REPL to enter the Pkg mode and run","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"pkg> add https://github.com/xKDR/CRRao.jl","category":"page"},{"location":"man/guide/#Tutorial:-Frequentist-Linear-Regression","page":"Guide","title":"Tutorial: Frequentist Linear Regression","text":"","category":"section"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"The goal of the CRRao package is to try to unify calling variety of statistical models under the same API. Note that this is different from what something like StatsAPI.jl is doing; instead of introducing namespaces for development of packages, CRRao tries to call those packages with a uniform API. A very similar package comes from the R world: the Zelig Project.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"To see how this API works, we will go over an example in which we'll train a linear regression model with the usual ordinary least squares method (which falls under the category of the frequentist viewpoint of statistics). For our example, we will be working with the mtcars dataset.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"We first import the required packages.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"using CRRao, RDatasets, StatsPlots, Plots, StatsModels","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"Then we import the dataset.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"mtcars = dataset(\"datasets\", \"mtcars\")","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"This dataset has 11 columns (barring the index). We want to train a linear regression model to predict MPG of a car from the information contained in the attributes HP, WT and Gear. We can represent this as a formula term of type StatsModels.formula. The formula term will look like","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"MPG ~ HP + WT + Gear","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"More information about such terms can be found in the corresponding docs. ","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"Next, we train a linear regression model.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"model = fit(@formula(MPG ~ HP + WT + Gear), mtcars, LinearRegression())","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"As we can see from the output, a table of coefficients has been printed for us. We can now infer other details of the model from the various getter functions that apply to frequentist models. So one can do the following.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"coeftable(model)\nsigma(model)","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"We can also get the predicted response of the model, along with other measures like the vector of Cook's distances using the predict and cooksdistance functions exported by CRRao. Here's a plot of the vector of Cook's distances.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"plot(cooksdistance(model))","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"To understand more about these functions and in general how frequentist models work in CRRao along with a complete set of getter functions that can be used, please visit the section of the API reference on Frequentist Regression Models.","category":"page"},{"location":"man/guide/#Tutorial:-Bayesian-Logistic-Regression","page":"Guide","title":"Tutorial: Bayesian Logistic Regression","text":"","category":"section"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"Next, let's see an example of doing bayesian statistical inference with CRRao. In this example, we will perform bayesian logistic regression on the turnout dataset from R's Zelig. Further, we will use the Logit link function with a Ridge prior (Prior_Ridge).","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"With this example, we'll also showcase how to use random number generators to get reproducible results. For this, we will use the StableRNGs package (although any random number generator can be used). So, first we import the required modules.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"using Logging\nLogging.disable_logging(Logging.Warn)","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"using CRRao, RDatasets, StableRNGs, StatsModels","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"CRRao.setprogress!(false)","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"Then, we use a StableRNG with random seed 123 as our random number generator.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"CRRao.set_rng(StableRNG(123))","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"(Check the documentation of the CRRao.set_rng method for more details). Next, let us load our dataset.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"turnout = dataset(\"Zelig\", \"turnout\")","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"And finally, we do the inference using our proposed model.","category":"page"},{"location":"man/guide/","page":"Guide","title":"Guide","text":"model = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Logit(), Prior_Ridge())","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = CRRao","category":"page"},{"location":"","page":"Home","title":"Home","text":"Documentation for CRRao.","category":"page"},{"location":"#CRRao","page":"Home","title":"CRRao","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CRRao is a package that implements statistical models. The implementation  of statistical models becomes straightforward for most Julia users  with the help of this package. This is going to be a wrapper package; leveraging the strength of wonderful Julia packages that already exist,  such as StatsBase.jl, StatsModels.jl, Distributions.jl, GLM.jl, Turing.jl, Dataframes.jl, LinearAlgebra.jl etc.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CRRao is a consistent framework through which callers interact with  a large suite of models. For the end-user, it reduces the cost and complexity  of estimating/training statistical models. It offers convenient guidelines through  which development of additional statistical models can take place in the future.","category":"page"},{"location":"","page":"Home","title":"Home","text":"We follow a framework which makes contribution to this package easy.","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nYou can read more about Prof C.R. Rao here.","category":"page"},{"location":"#Manual","page":"Home","title":"Manual","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"man/guide.md\",\n    \"man/examples.md\",\n]","category":"page"},{"location":"#API-Reference","page":"Home","title":"API Reference","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"api/interface.md\",\n    \"api/frequentist_regression.md\",\n    \"api/bayesian_regression.md\"\n]","category":"page"}]
}
