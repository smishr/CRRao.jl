<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>General Interface · CRRao.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://xKDR.github.io/CRRao.jl/api/interface/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">CRRao.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../../man/guide/">Guide</a></li></ul></li><li><span class="tocitem">API Reference</span><ul><li class="is-active"><a class="tocitem" href>General Interface</a><ul class="internal"><li><a class="tocitem" href="#Understanding-the-interface"><span>Understanding the interface</span></a></li><li><a class="tocitem" href="#Model-Classes"><span>Model Classes</span></a></li><li><a class="tocitem" href="#Link-functions."><span>Link functions.</span></a></li><li><a class="tocitem" href="#Setting-Random-Number-Generators"><span>Setting Random Number Generators</span></a></li></ul></li><li><a class="tocitem" href="../frequentist_regression/">Frequentist Regression Models</a></li><li><a class="tocitem" href="../bayesian_regression/">Bayesian Regression Models</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API Reference</a></li><li class="is-active"><a href>General Interface</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>General Interface</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/xKDR/CRRao.jl/blob/main/docs/src/api/interface.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="General-Interface"><a class="docs-heading-anchor" href="#General-Interface">General Interface</a><a id="General-Interface-1"></a><a class="docs-heading-anchor-permalink" href="#General-Interface" title="Permalink"></a></h1><h2 id="Understanding-the-interface"><a class="docs-heading-anchor" href="#Understanding-the-interface">Understanding the interface</a><a id="Understanding-the-interface-1"></a><a class="docs-heading-anchor-permalink" href="#Understanding-the-interface" title="Permalink"></a></h2><p>CRRao exports the <a href="#CRRao.fit"><code>fit</code></a> function, which is used to train all types of models supported by the package. As of now, the function supports the following signatures.</p><pre><code class="language-julia hljs">fit(formula, data, modelClass)
fit(formula, data, modelClass, link)
fit(formula, data, modelClass, prior)
fit(formula, data, modelClass, link, prior)</code></pre><p>It should be noted that not all model classes support every type of signature. The parameters passed above mean the following.</p><ol><li><p>The parameter <code>formula</code> must be a formula of type <a href="https://juliastats.org/StatsModels.jl/stable/api/#StatsModels.FormulaTerm"><code>StatsModels.FormulaTerm</code></a>. Any formula has an LHS and an RHS. The LHS represents the response variable, and the RHS represents the independent variables.</p></li><li><p>The parameter <code>data</code> must be a <a href="https://dataframes.juliadata.org/stable/lib/types/#DataFrames.DataFrame"><code>DataFrame</code></a>. This variable represents the dataset on which the model must be trained.</p></li><li><p><code>modelClass</code> represents the type of the statistical model to be used. Currently, CRRao supports four regression models, and the type of <code>modelClass</code> must be one of the following:</p><ul><li><a href="#CRRao.LinearRegression"><code>LinearRegression</code></a></li><li><a href="#CRRao.LogisticRegression"><code>LogisticRegression</code></a></li><li><a href="#CRRao.NegBinomRegression"><code>NegBinomRegression</code></a></li><li><a href="#CRRao.PoissonRegression"><code>PoissonRegression</code></a></li></ul></li><li><p>Certain model classes (like Logistic Regression) support link functions; this is represented by the <code>link</code> parameter. Currently four link functions are supported: Logit, Probit, Cloglog and Cauchit. So, the type of <code>link</code> must be one of the following:</p><ul><li><a href="#CRRao.Logit"><code>Logit</code></a></li><li><a href="#CRRao.Probit"><code>Probit</code></a></li><li><a href="#CRRao.Cloglog"><code>Cloglog</code></a></li><li><a href="#CRRao.Cauchit"><code>Cauchit</code></a></li></ul></li><li><p>CRRao also supports Bayesian models, and the priors to be can be specified while calling <code>fit</code>. Currently CRRao supports four different kinds of priors, and the type of the <code>prior</code> parameter must be one of the following.</p><ul><li><a href="api/@ref"><code>Prior_Ridge</code></a></li><li><a href="api/@ref"><code>Prior_Laplace</code></a></li><li><a href="api/@ref"><code>Prior_Cauchy</code></a></li><li><a href="api/@ref"><code>Prior_TDist</code></a></li></ul></li></ol><article class="docstring"><header><a class="docstring-binding" id="CRRao.fit" href="#CRRao.fit"><code>CRRao.fit</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression)</code></pre><p>Fit an OLS Linear Regression model on the input data. Uses the <code>lm</code> method from the <a href="https://github.com/JuliaStats/GLM.jl">GLM</a> package under the hood. Returns an object of type <code>FrequentistRegression{:LinearRegression}</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using CRRao, RDatasets, StatsPlots, StatsModels
julia&gt; df = dataset(&quot;datasets&quot;, &quot;mtcars&quot;)
32×12 DataFrame
 Row │ Model              MPG      Cyl    Disp     HP     DRat     WT       QSec     VS     AM     Gear   Carb  
     │ String31           Float64  Int64  Float64  Int64  Float64  Float64  Float64  Int64  Int64  Int64  Int64 
─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────
   1 │ Mazda RX4             21.0      6    160.0    110     3.9     2.62     16.46      0      1      4      4
   2 │ Mazda RX4 Wag         21.0      6    160.0    110     3.9     2.875    17.02      0      1      4      4
   3 │ Datsun 710            22.8      4    108.0     93     3.85    2.32     18.61      1      1      4      1
   4 │ Hornet 4 Drive        21.4      6    258.0    110     3.08    3.215    19.44      1      0      3      1
   5 │ Hornet Sportabout     18.7      8    360.0    175     3.15    3.44     17.02      0      0      3      2
   6 │ Valiant               18.1      6    225.0    105     2.76    3.46     20.22      1      0      3      1
  ⋮  │         ⋮             ⋮       ⋮       ⋮       ⋮       ⋮        ⋮        ⋮       ⋮      ⋮      ⋮      ⋮
  27 │ Porsche 914-2         26.0      4    120.3     91     4.43    2.14     16.7       0      1      5      2
  28 │ Lotus Europa          30.4      4     95.1    113     3.77    1.513    16.9       1      1      5      2
  29 │ Ford Pantera L        15.8      8    351.0    264     4.22    3.17     14.5       0      1      5      4
  30 │ Ferrari Dino          19.7      6    145.0    175     3.62    2.77     15.5       0      1      5      6
  31 │ Maserati Bora         15.0      8    301.0    335     3.54    3.57     14.6       0      1      5      8
  32 │ Volvo 142E            21.4      4    121.0    109     4.11    2.78     18.6       1      1      4      2
                                                                                                 20 rows omitted
julia&gt; container = fit(@formula(MPG ~ HP + WT + Gear), df, LinearRegression())
Model Class: Linear Regression
Likelihood Mode: Gauss
Link Function: Identity
Computing Method: Optimization
────────────────────────────────────────────────────────────────────────────
                  Coef.  Std. Error      t  Pr(&gt;|t|)   Lower 95%   Upper 95%
────────────────────────────────────────────────────────────────────────────
(Intercept)  32.0137     4.63226      6.91    &lt;1e-06  22.5249     41.5024
HP           -0.0367861  0.00989146  -3.72    0.0009  -0.0570478  -0.0165243
WT           -3.19781    0.846546    -3.78    0.0008  -4.93188    -1.46374
Gear          1.01998    0.851408     1.20    0.2410  -0.72405     2.76401
────────────────────────────────────────────────────────────────────────────
julia&gt; coeftable(container)
────────────────────────────────────────────────────────────────────────────
                  Coef.  Std. Error      t  Pr(&gt;|t|)   Lower 95%   Upper 95%
────────────────────────────────────────────────────────────────────────────
(Intercept)  32.0137     4.63226      6.91    &lt;1e-06  22.5249     41.5024
HP           -0.0367861  0.00989146  -3.72    0.0009  -0.0570478  -0.0165243
WT           -3.19781    0.846546    -3.78    0.0008  -4.93188    -1.46374
Gear          1.01998    0.851408     1.20    0.2410  -0.72405     2.76401
────────────────────────────────────────────────────────────────────────────
julia&gt; sigma(container)
2.5741691724978972
julia&gt; aic(container)
157.05277871921942
julia&gt; predict(container)
32-element Vector{Float64}:
 23.668849952338718
 22.85340824320634
 25.253556140740894
 20.746171762311384
 17.635570543830177
 20.14663845388644
 14.644831040166633
 23.61182872351372
  ⋮
 16.340457241090512
 27.47793682112109
 26.922715039574857
 28.11844900519874
 17.264981908248554
 21.818065399379595
 13.374047477198516
 23.193986311384343
julia&gt; residuals(container)
32-element Vector{Float64}:
 -2.668849952338718
 -1.8534082432063386
 -2.4535561407408935
  0.6538282376886144
  1.0644294561698224
 -2.0466384538864375
 -0.3448310401666319
  0.7881712764862776
  ⋮
  2.8595427589094875
 -0.1779368211210901
 -0.9227150395748573
  2.2815509948012576
 -1.4649819082485536
 -2.1180653993795957
  1.6259525228014837
 -1.7939863113843444
julia&gt; plot(cooksdistance(container))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/frequentist/linear_regression.jl#L1-L96">source</a></section><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::Logit)</code></pre><p>Fit a Logistic Regression model on the input data using the Logit link. Uses the <code>glm</code> method from the <a href="https://github.com/JuliaStats/GLM.jl">GLM</a> package under the hood. Returns an object of type <code>FrequentistRegression{:LogisticRegression}</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using CRRao, RDatasets, StatsModels
julia&gt; turnout = dataset(&quot;Zelig&quot;, &quot;turnout&quot;)
2000×5 DataFrame
  Row │ Race   Age    Educate  Income   Vote  
      │ Cat…   Int32  Float64  Float64  Int32 
──────┼───────────────────────────────────────
    1 │ white     60     14.0   3.3458      1
    2 │ white     51     10.0   1.8561      0
    3 │ white     24     12.0   0.6304      0
    4 │ white     38      8.0   3.4183      1
    5 │ white     25     12.0   2.7852      1
    6 │ white     67     12.0   2.3866      1
  ⋮   │   ⋮      ⋮       ⋮        ⋮       ⋮
 1995 │ white     22      7.0   0.2364      0
 1996 │ white     26     16.0   3.3834      0
 1997 │ white     34     12.0   2.917       1
 1998 │ white     51     16.0   7.8949      1
 1999 │ white     22     10.0   2.4811      0
 2000 │ white     59     10.0   0.5523      0
                             1988 rows omitted
julia&gt; container = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Logit())
Model Class: Logistic Regression
Likelihood Mode: Binomial
Link Function: Identity
Computing Method: Optimization
────────────────────────────────────────────────────────────────────────────
                  Coef.  Std. Error      z  Pr(&gt;|z|)   Lower 95%   Upper 95%
────────────────────────────────────────────────────────────────────────────
(Intercept)  -3.03426    0.325927    -9.31    &lt;1e-19  -3.67307    -2.39546
Age           0.0283543  0.00346034   8.19    &lt;1e-15   0.0215722   0.0351365
Race: white   0.250798   0.146457     1.71    0.0868  -0.0362521   0.537847
Income        0.177112   0.0271516    6.52    &lt;1e-10   0.123896    0.230328
Educate       0.175634   0.0203308    8.64    &lt;1e-17   0.135786    0.215481
────────────────────────────────────────────────────────────────────────────
julia&gt; coeftable(container)
────────────────────────────────────────────────────────────────────────────
                  Coef.  Std. Error      z  Pr(&gt;|z|)   Lower 95%   Upper 95%
────────────────────────────────────────────────────────────────────────────
(Intercept)  -3.03426    0.325927    -9.31    &lt;1e-19  -3.67307    -2.39546
Age           0.0283543  0.00346034   8.19    &lt;1e-15   0.0215722   0.0351365
Race: white   0.250798   0.146457     1.71    0.0868  -0.0362521   0.537847
Income        0.177112   0.0271516    6.52    &lt;1e-10   0.123896    0.230328
Educate       0.175634   0.0203308    8.64    &lt;1e-17   0.135786    0.215481
────────────────────────────────────────────────────────────────────────────
julia&gt; loglikelihood(container)
-1011.9906318515575
julia&gt; aic(container)
2033.981263703115
julia&gt; bic(container)
2061.9857760008254</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/frequentist/logistic_regression.jl#L7-L67">source</a></section><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::Probit)</code></pre><p>Fit a Logistic Regression model on the input data using the Probit link. Uses the <code>glm</code> method from the <a href="https://github.com/JuliaStats/GLM.jl">GLM</a> package under the hood. Returns an object of type <code>FrequentistRegression{:LogisticRegression}</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/frequentist/logistic_regression.jl#L77-L83">source</a></section><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::Cloglog)</code></pre><p>Fit a Logistic Regression model on the input data using the Cloglog link. Uses the <code>glm</code> method from the <a href="https://github.com/JuliaStats/GLM.jl">GLM</a> package under the hood. Returns an object of type <code>FrequentistRegression{:LogisticRegression}</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/frequentist/logistic_regression.jl#L93-L99">source</a></section><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::Cauchit)</code></pre><p>Fit a Logistic Regression model on the input data using the Cauchit link. Uses the <code>glm</code> method from the <a href="https://github.com/JuliaStats/GLM.jl">GLM</a> package under the hood. Returns an object of type <code>FrequentistRegression{:LogisticRegression}</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/frequentist/logistic_regression.jl#L109-L115">source</a></section><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::NegBinomRegression)</code></pre><p>Fit a Negative Binomial Regression model on the input data (with the default link function being the Log link). Uses the <code>glm</code> method from the <a href="https://github.com/JuliaStats/GLM.jl">GLM</a> package under the hood. Returns an object of type <code>FrequentistRegression{:NegativeBinomialRegression}</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using CRRao, RDatasets, StatsModels
julia&gt; sanction = dataset(&quot;Zelig&quot;, &quot;sanction&quot;)
78×8 DataFrame
 Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         
     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          
─────┼───────────────────────────────────────────────────────────────────
   1 │     1      4       3       1       1      4     15  major loss
   2 │     0      2       3       0       1      3      4  modest loss
   3 │     0      1       3       1       0      2      1  little effect
   4 │     1      1       3       1       1      2      1  little effect
   5 │     0      1       3       1       1      2      1  little effect
   6 │     0      1       3       0       1      2      1  little effect
  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮
  73 │     1      3       1       1       1      2     14  little effect
  74 │     0      2       1       0       0      1      2  net gain
  75 │     0      1       3       0       1      2      1  little effect
  76 │     0      4       3       1       0      2     13  little effect
  77 │     0      1       2       0       0      1      1  net gain
  78 │     1      3       1       1       1      2     10  little effect
                                                          66 rows omitted
julia&gt; container = fit(@formula(Num ~ Target + Coop + NCost), sanction, NegBinomRegression())
Model Class: Count Regression
Likelihood Mode: Negative Binomial
Link Function: Log
Computing Method: Optimization
─────────────────────────────────────────────────────────────────────────────────
                         Coef.  Std. Error      z  Pr(&gt;|z|)  Lower 95%  Upper 95%
─────────────────────────────────────────────────────────────────────────────────
(Intercept)         -1.10939      0.459677  -2.41    0.0158  -2.01034   -0.208444
Target               0.0117398    0.142779   0.08    0.9345  -0.268101   0.291581
Coop                 1.0506       0.111556   9.42    &lt;1e-20   0.831949   1.26924
NCost: major loss   -0.204244     0.508156  -0.40    0.6877  -1.20021    0.791723
NCost: modest loss   1.27142      0.290427   4.38    &lt;1e-04   0.702197   1.84065
NCost: net gain      0.176797     0.254291   0.70    0.4869  -0.321604   0.675197
─────────────────────────────────────────────────────────────────────────────────</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/frequentist/negativebinomial_regression.jl#L7-L52">source</a></section><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::PoissonRegression)</code></pre><p>Fit a Poisson Regression model on the input data (with the default link function being the Log link). Uses the <code>glm</code> method from the <a href="https://github.com/JuliaStats/GLM.jl">GLM</a> package under the hood. Returns an object of type <code>FrequentistRegression{:PoissonRegression}</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using CRRao, RDatasets, StatsModels
julia&gt; sanction = dataset(&quot;Zelig&quot;, &quot;sanction&quot;)
78×8 DataFrame
 Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         
     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          
─────┼───────────────────────────────────────────────────────────────────
   1 │     1      4       3       1       1      4     15  major loss
   2 │     0      2       3       0       1      3      4  modest loss
   3 │     0      1       3       1       0      2      1  little effect
   4 │     1      1       3       1       1      2      1  little effect
   5 │     0      1       3       1       1      2      1  little effect
   6 │     0      1       3       0       1      2      1  little effect
  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮
  73 │     1      3       1       1       1      2     14  little effect
  74 │     0      2       1       0       0      1      2  net gain
  75 │     0      1       3       0       1      2      1  little effect
  76 │     0      4       3       1       0      2     13  little effect
  77 │     0      1       2       0       0      1      1  net gain
  78 │     1      3       1       1       1      2     10  little effect
                                                          66 rows omitted
julia&gt; container = fit(@formula(Num ~ Target + Coop + NCost), sanction, PoissonRegression())
Model Class: Poisson Regression
Likelihood Mode: Poison
Link Function: Log
Computing Method: Optimization
─────────────────────────────────────────────────────────────────────────────────
                        Coef.  Std. Error      z  Pr(&gt;|z|)   Lower 95%  Upper 95%
─────────────────────────────────────────────────────────────────────────────────
(Intercept)         -1.91392    0.261667   -7.31    &lt;1e-12  -2.42678    -1.40106
Target               0.157769   0.0653822   2.41    0.0158   0.0296218   0.285915
Coop                 1.15127    0.0561861  20.49    &lt;1e-92   1.04114     1.26139
NCost: major loss   -0.324051   0.230055   -1.41    0.1590  -0.774951    0.126848
NCost: modest loss   1.71973    0.100518   17.11    &lt;1e-64   1.52272     1.91674
NCost: net gain      0.463907   0.16992     2.73    0.0063   0.13087     0.796944
─────────────────────────────────────────────────────────────────────────────────</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/frequentist/poisson_regression.jl#L7-L52">source</a></section><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression, prior::Prior_Ridge, h::Float64 = 0.01, sim_size::Int64 = 1000)</code></pre><p>Fit a Bayesian Linear Regression model on the input data with a Ridge prior.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using CRRao, RDatasets, StableRNGs, StatsModels
julia&gt; CRRao.set_rng(StableRNG(123))
StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)
julia&gt; df = dataset(&quot;datasets&quot;, &quot;mtcars&quot;)
32×12 DataFrame
 Row │ Model              MPG      Cyl    Disp     HP     DRat     WT       QSec     VS     AM     Gear   Carb  
     │ String31           Float64  Int64  Float64  Int64  Float64  Float64  Float64  Int64  Int64  Int64  Int64 
─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────
   1 │ Mazda RX4             21.0      6    160.0    110     3.9     2.62     16.46      0      1      4      4
   2 │ Mazda RX4 Wag         21.0      6    160.0    110     3.9     2.875    17.02      0      1      4      4
   3 │ Datsun 710            22.8      4    108.0     93     3.85    2.32     18.61      1      1      4      1
   4 │ Hornet 4 Drive        21.4      6    258.0    110     3.08    3.215    19.44      1      0      3      1
  ⋮  │         ⋮             ⋮       ⋮       ⋮       ⋮       ⋮        ⋮        ⋮       ⋮      ⋮      ⋮      ⋮
  30 │ Ferrari Dino          19.7      6    145.0    175     3.62    2.77     15.5       0      1      5      6
  31 │ Maserati Bora         15.0      8    301.0    335     3.54    3.57     14.6       0      1      5      8
  32 │ Volvo 142E            21.4      4    121.0    109     4.11    2.78     18.6       1      1      4      2
                                                                                                 25 rows omitted
julia&gt; container = fit(@formula(MPG ~ HP + WT + Gear), df, LinearRegression(), Prior_Ridge())
┌ Info: Found initial step size
└   ϵ = 0.00078125
Chains MCMC chain (10000×18×1 Array{Float64, 3}):

Iterations        = 1001:1:11000
Number of chains  = 1
Samples per chain = 10000
Wall duration     = 30.44 seconds
Compute duration  = 30.44 seconds
parameters        = v, σ, α, β[1], β[2], β[3]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec 
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 

           v    6.9097    3.7793     0.0378    0.0609   3848.1626    0.9999      126.4013
           σ    2.6726    0.3878     0.0039    0.0061   3787.1472    1.0000      124.3972
           α   28.6866    5.4205     0.0542    0.1106   2431.5304    1.0001       79.8690
        β[1]   -0.0395    0.0106     0.0001    0.0002   4057.7267    0.9999      133.2849
        β[2]   -2.7056    0.9635     0.0096    0.0176   2897.6230    1.0001       95.1788
        β[3]    1.5912    0.9825     0.0098    0.0198   2538.0548    1.0001       83.3680

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           v    2.5200    4.5014    6.0397    8.2382   16.6220
           σ    2.0519    2.4030    2.6297    2.8942    3.5482
           α   17.6034   25.2623   28.9229   32.3360   38.7343
        β[1]   -0.0612   -0.0464   -0.0393   -0.0325   -0.0191
        β[2]   -4.5163   -3.3443   -2.7385   -2.1041   -0.7211
        β[3]   -0.2205    0.9158    1.5520    2.2028    3.6202</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/bayesian/linear_regression.jl#L12-L73">source</a></section><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression, prior::Prior_Laplace, h::Float64 = 0.01, sim_size::Int64 = 1000)</code></pre><p>Fit a Bayesian Linear Regression model on the input data with a Laplace prior.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using CRRao, RDatasets, StableRNGs, StatsModels
julia&gt; CRRao.set_rng(StableRNG(123))
StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)
julia&gt; df = dataset(&quot;datasets&quot;, &quot;mtcars&quot;)
32×12 DataFrame
 Row │ Model              MPG      Cyl    Disp     HP     DRat     WT       QSec     VS     AM     Gear   Carb  
     │ String31           Float64  Int64  Float64  Int64  Float64  Float64  Float64  Int64  Int64  Int64  Int64 
─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────
   1 │ Mazda RX4             21.0      6    160.0    110     3.9     2.62     16.46      0      1      4      4
   2 │ Mazda RX4 Wag         21.0      6    160.0    110     3.9     2.875    17.02      0      1      4      4
   3 │ Datsun 710            22.8      4    108.0     93     3.85    2.32     18.61      1      1      4      1
   4 │ Hornet 4 Drive        21.4      6    258.0    110     3.08    3.215    19.44      1      0      3      1
  ⋮  │         ⋮             ⋮       ⋮       ⋮       ⋮       ⋮        ⋮        ⋮       ⋮      ⋮      ⋮      ⋮
  30 │ Ferrari Dino          19.7      6    145.0    175     3.62    2.77     15.5       0      1      5      6
  31 │ Maserati Bora         15.0      8    301.0    335     3.54    3.57     14.6       0      1      5      8
  32 │ Volvo 142E            21.4      4    121.0    109     4.11    2.78     18.6       1      1      4      2
                                                                                                 25 rows omitted
julia&gt; container = fit(@formula(MPG ~ HP + WT + Gear), df, LinearRegression(), Prior_Laplace())
┌ Info: Found initial step size
└   ϵ = 0.00078125
Chains MCMC chain (10000×18×1 Array{Float64, 3}):

Iterations        = 1001:1:11000
Number of chains  = 1
Samples per chain = 10000
Wall duration     = 28.55 seconds
Compute duration  = 28.55 seconds
parameters        = v, σ, α, β[1], β[2], β[3]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec 
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 

           v    4.2213    3.0653     0.0307    0.0506   3799.4211    0.9999      133.0609
           σ    2.6713    0.3829     0.0038    0.0068   3782.5307    1.0001      132.4694
           α   29.0523    5.2589     0.0526    0.1032   3144.5864    1.0004      110.1277
        β[1]   -0.0398    0.0106     0.0001    0.0002   4429.6471    1.0005      155.1323
        β[2]   -2.7161    0.9506     0.0095    0.0182   3299.1828    1.0009      115.5419
        β[3]    1.5129    0.9530     0.0095    0.0180   3383.7096    1.0002      118.5021

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           v    1.2438    2.3788    3.4110    5.1138   11.7423
           σ    2.0692    2.4016    2.6226    2.8897    3.5602
           α   17.8056   25.8001   29.2866   32.5385   38.8889
        β[1]   -0.0614   -0.0466   -0.0395   -0.0326   -0.0194
        β[2]   -4.5559   -3.3384   -2.7407   -2.1204   -0.7254
        β[3]   -0.2790    0.8794    1.4691    2.1092    3.5245</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/bayesian/linear_regression.jl#L101-L162">source</a></section><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression, prior::Prior_Cauchy, sim_size::Int64 = 1000)</code></pre><p>Fit a Bayesian Linear Regression model on the input data with a Cauchy prior.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using CRRao, RDatasets, StableRNGs, StatsModels
julia&gt; CRRao.set_rng(StableRNG(123))
StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)
julia&gt; df = dataset(&quot;datasets&quot;, &quot;mtcars&quot;)
32×12 DataFrame
 Row │ Model              MPG      Cyl    Disp     HP     DRat     WT       QSec     VS     AM     Gear   Carb  
     │ String31           Float64  Int64  Float64  Int64  Float64  Float64  Float64  Int64  Int64  Int64  Int64 
─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────
   1 │ Mazda RX4             21.0      6    160.0    110     3.9     2.62     16.46      0      1      4      4
   2 │ Mazda RX4 Wag         21.0      6    160.0    110     3.9     2.875    17.02      0      1      4      4
   3 │ Datsun 710            22.8      4    108.0     93     3.85    2.32     18.61      1      1      4      1
   4 │ Hornet 4 Drive        21.4      6    258.0    110     3.08    3.215    19.44      1      0      3      1
  ⋮  │         ⋮             ⋮       ⋮       ⋮       ⋮       ⋮        ⋮        ⋮       ⋮      ⋮      ⋮      ⋮
  30 │ Ferrari Dino          19.7      6    145.0    175     3.62    2.77     15.5       0      1      5      6
  31 │ Maserati Bora         15.0      8    301.0    335     3.54    3.57     14.6       0      1      5      8
  32 │ Volvo 142E            21.4      4    121.0    109     4.11    2.78     18.6       1      1      4      2
                                                                                                 25 rows omitted
julia&gt; container = fit(@formula(MPG ~ HP + WT + Gear), df, LinearRegression(), Prior_Cauchy(), 20000)
┌ Info: Found initial step size
└   ϵ = 0.000390625
Chains MCMC chain (20000×17×1 Array{Float64, 3}):

Iterations        = 1001:1:21000
Number of chains  = 1
Samples per chain = 20000
Wall duration     = 34.1 seconds
Compute duration  = 34.1 seconds
parameters        = σ, α, β[1], β[2], β[3]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec 
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 

           σ    2.5891    0.3413     0.0024    0.0036   8611.3030    1.0001      252.5087
           α   30.2926    4.6666     0.0330    0.0590   5600.5552    1.0013      164.2247
        β[1]   -0.0394    0.0100     0.0001    0.0001   7985.0944    1.0009      234.1464
        β[2]   -2.8393    0.8638     0.0061    0.0106   6031.2854    1.0012      176.8550
        β[3]    1.2738    0.8524     0.0060    0.0107   5814.5026    1.0014      170.4983

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           σ    2.0266    2.3485    2.5547    2.7908    3.3512
           α   20.8140   27.3265   30.3854   33.4168   39.1369
        β[1]   -0.0595   -0.0458   -0.0393   -0.0328   -0.0197
        β[2]   -4.5172   -3.4069   -2.8485   -2.2786   -1.1244
        β[3]   -0.3576    0.7039    1.2568    1.8199    3.0201</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/bayesian/linear_regression.jl#L189-L248">source</a></section><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression, prior::Prior_TDist, h::Float64 = 2.0, sim_size::Int64 = 1000)</code></pre><p>Fit a Bayesian Linear Regression model on the input data with a t(ν) distributed prior.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using CRRao, RDatasets, StableRNGs, StatsPlots, StatsModels
julia&gt; CRRao.set_rng(StableRNG(123))
StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)
julia&gt; df = dataset(&quot;datasets&quot;, &quot;mtcars&quot;)
32×12 DataFrame
 Row │ Model              MPG      Cyl    Disp     HP     DRat     WT       QSec     VS     AM     Gear   Carb  
     │ String31           Float64  Int64  Float64  Int64  Float64  Float64  Float64  Int64  Int64  Int64  Int64 
─────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────
   1 │ Mazda RX4             21.0      6    160.0    110     3.9     2.62     16.46      0      1      4      4
   2 │ Mazda RX4 Wag         21.0      6    160.0    110     3.9     2.875    17.02      0      1      4      4
   3 │ Datsun 710            22.8      4    108.0     93     3.85    2.32     18.61      1      1      4      1
   4 │ Hornet 4 Drive        21.4      6    258.0    110     3.08    3.215    19.44      1      0      3      1
  ⋮  │         ⋮             ⋮       ⋮       ⋮       ⋮       ⋮        ⋮        ⋮       ⋮      ⋮      ⋮      ⋮
  30 │ Ferrari Dino          19.7      6    145.0    175     3.62    2.77     15.5       0      1      5      6
  31 │ Maserati Bora         15.0      8    301.0    335     3.54    3.57     14.6       0      1      5      8
  32 │ Volvo 142E            21.4      4    121.0    109     4.11    2.78     18.6       1      1      4      2
                                                                                                 25 rows omitted
julia&gt; container = fit(@formula(MPG ~ HP + WT + Gear), df, LinearRegression(), Prior_TDist())
┌ Info: Found initial step size
└   ϵ = 1.1920928955078126e-8
Chains MCMC chain (10000×18×1 Array{Float64, 3}):

Iterations        = 1001:1:11000
Number of chains  = 1
Samples per chain = 10000
Wall duration     = 41.09 seconds
Compute duration  = 41.09 seconds
parameters        = ν, σ, α, β[1], β[2], β[3]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec 
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 

           ν    1.0538    0.5576     0.0056    0.0143   1340.7091    0.9999       32.6318
           σ    2.6251    0.3559     0.0036    0.0043   6374.0312    0.9999      155.1388
           α   30.1859    4.7935     0.0479    0.0605   5361.7257    1.0006      130.5001
        β[1]   -0.0396    0.0103     0.0001    0.0001   5835.9959    1.0003      142.0434
        β[2]   -2.8099    0.8772     0.0088    0.0114   5301.0033    1.0010      129.0221
        β[3]    1.2856    0.8699     0.0087    0.0106   5752.1640    1.0003      140.0030

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           ν    0.3670    0.6600    0.9301    1.2961    2.4821
           σ    2.0327    2.3758    2.5885    2.8442    3.4393
           α   20.4816   27.0685   30.2787   33.4481   39.3462
        β[1]   -0.0599   -0.0464   -0.0396   -0.0326   -0.0198
        β[2]   -4.4924   -3.3902   -2.8250   -2.2351   -1.0257
        β[3]   -0.3642    0.7021    1.2642    1.8397    3.0849
julia&gt; plot(container.chain)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/bayesian/linear_regression.jl#L271-L333">source</a></section><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression, prior::Prior_Gauss,alpha_prior_mean::Float64 = 0.0, beta_prior_mean::Float64, sim_size::Int64 = 1000, h::Float64 = 0.1)</code></pre><p>Fit a Bayesian Linear Regression model on the input data with a Gaussian prior with user specific prior mean for α and β. User doesnot have     idea about the prior sd of α and β. User ignore the specification for sd of α and β.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using CRRao, RDatasets, StableRNGs, StatsModels
julia&gt; CRRao.set_rng(StableRNG(123));
julia&gt; df = dataset(&quot;datasets&quot;, &quot;mtcars&quot;);                                                                                                
julia&gt; container = fit(@formula(MPG ~ HP + WT + Gear), df, LinearRegression(), Prior_Gauss(),0.0,[0.0,-3.0,1.0],1000)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/bayesian/linear_regression.jl#L418-L433">source</a></section><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression, prior::Prior_Gauss, alpha_prior_mean::Float64, alpha_prior_sd::Float64, beta_prior_mean::Vector{Float64}, beta_prior_sd::Vector{Float64}, sim_size::Int64 = 1000)</code></pre><p>Fit a Bayesian Linear Regression model on the input data with a Gaussian prior with user specific prior mean and sd for α and β. </p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using CRRao, RDatasets, StableRNGs, StatsModels
julia&gt; CRRao.set_rng(StableRNG(123));
julia&gt; df = dataset(&quot;datasets&quot;, &quot;mtcars&quot;);                                                                                                
julia&gt; container = fit(@formula(MPG ~ HP + WT + Gear), df, LinearRegression(), Prior_Gauss(),30.0,10.0,[0.0,-3.0,1.0],[0.1,1.0,1.0],1000)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/bayesian/linear_regression.jl#L470-L484">source</a></section><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::CRRaoLink, prior::Prior_Ridge, h::Float64 = 0.1, level::Float64 = 0.95, sim_size::Int64 = 1000)</code></pre><p>Fit a Bayesian Logistic Regression model on the input data with a Ridge prior with the provided <code>Link</code> function.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using CRRao, RDatasets, StableRNGs, StatsModels
julia&gt; CRRao.set_rng(StableRNG(123))
StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)
julia&gt; turnout = dataset(&quot;Zelig&quot;, &quot;turnout&quot;)
2000×5 DataFrame
  Row │ Race   Age    Educate  Income   Vote  
      │ Cat…   Int32  Float64  Float64  Int32 
──────┼───────────────────────────────────────
    1 │ white     60     14.0   3.3458      1
    2 │ white     51     10.0   1.8561      0
    3 │ white     24     12.0   0.6304      0
    4 │ white     38      8.0   3.4183      1
  ⋮   │   ⋮      ⋮       ⋮        ⋮       ⋮
 1998 │ white     51     16.0   7.8949      1
 1999 │ white     22     10.0   2.4811      0
 2000 │ white     59     10.0   0.5523      0
                             1993 rows omitted
julia&gt; container_logit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Logit(), Prior_Ridge())
┌ Warning: The current proposal will be rejected due to numerical error(s).
│   isfinite.((θ, r, ℓπ, ℓκ)) = (true, false, false, false)
└ @ AdvancedHMC ~/.julia/packages/AdvancedHMC/kB7Xa/src/hamiltonian.jl:47
Chains MCMC chain (1000×18×1 Array{Float64, 3}):

Iterations        = 501:1:1500
Number of chains  = 1
Samples per chain = 1000
Wall duration     = 6.98 seconds
Compute duration  = 6.98 seconds
parameters        = λ, α, β[1], β[2], β[3], β[4]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse        ess      rhat   ess_per_sec 
      Symbol   Float64   Float64    Float64   Float64    Float64   Float64       Float64 

           λ    2.3722    0.0007     0.0000    0.0001     5.1273    1.1072        0.7346
           α    0.7872    0.0003     0.0000    0.0001     2.9594    1.5660        0.4240
        β[1]    0.4843    0.0000     0.0000    0.0000   111.6465    1.0067       15.9952
        β[2]    0.6183    0.0004     0.0000    0.0001     2.3393    2.2610        0.3351
        β[3]   -1.4043    0.0003     0.0000    0.0001     3.1790    1.3848        0.4554
        β[4]   -3.0656    0.0007     0.0000    0.0001     2.4692    1.9479        0.3538

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           λ    2.3706    2.3721    2.3724    2.3726    2.3731
           α    0.7865    0.7872    0.7873    0.7874    0.7876
        β[1]    0.4842    0.4843    0.4843    0.4843    0.4843
        β[2]    0.6178    0.6180    0.6181    0.6186    0.6192
        β[3]   -1.4046   -1.4045   -1.4044   -1.4041   -1.4036
        β[4]   -3.0669   -3.0659   -3.0655   -3.0653   -3.0645

julia&gt; container_probit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Probit(), Prior_Ridge())
Chains MCMC chain (10000×17×1 Array{Float64, 3}):

Iterations        = 1001:1:11000
Number of chains  = 1
Samples per chain = 10000
Wall duration     = 177.96 seconds
Compute duration  = 177.96 seconds
parameters        = λ, β[1], β[2], β[3], β[4]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec 
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 

           λ    0.0867    0.0533     0.0005    0.0008   3682.1660    0.9999       20.6909
        β[1]    0.0033    0.0013     0.0000    0.0000   9024.2636    0.9999       50.7092
        β[2]   -0.0162    0.0577     0.0006    0.0007   5712.7441    1.0000       32.1011
        β[3]    0.0902    0.0137     0.0001    0.0002   6239.2706    1.0004       35.0598
        β[4]    0.0220    0.0068     0.0001    0.0001   6004.4582    0.9999       33.7403

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           λ    0.0354    0.0556    0.0733    0.1018    0.2139
        β[1]    0.0007    0.0024    0.0033    0.0041    0.0058
        β[2]   -0.1353   -0.0525   -0.0157    0.0218    0.0958
        β[3]    0.0634    0.0808    0.0901    0.0992    0.1179
        β[4]    0.0086    0.0174    0.0221    0.0265    0.0354
julia&gt; container_cloglog = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cloglog(), Prior_Ridge())
Chains MCMC chain (10000×17×1 Array{Float64, 3}):

Iterations        = 1001:1:11000
Number of chains  = 1
Samples per chain = 10000
Wall duration     = 94.56 seconds
Compute duration  = 94.56 seconds
parameters        = λ, β[1], β[2], β[3], β[4]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse       ess      rhat   ess_per_sec 
      Symbol   Float64   Float64    Float64   Float64   Float64   Float64       Float64 

           λ    0.4868    0.0003     0.0000    0.0000   20.6437    1.8077        0.2183
        β[1]   -0.1684    0.0026     0.0000    0.0003   20.2642    2.5746        0.2143
        β[2]    0.4824    0.0008     0.0000    0.0001   20.4744    2.2619        0.2165
        β[3]    0.9618    0.0058     0.0001    0.0006   20.2614    2.5797        0.2143
        β[4]   -0.3887    0.0004     0.0000    0.0000   21.1046    1.7123        0.2232

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           λ    0.4861    0.4865    0.4869    0.4870    0.4872
        β[1]   -0.1725   -0.1706   -0.1687   -0.1661   -0.1642
        β[2]    0.4813    0.4817    0.4823    0.4831    0.4840
        β[3]    0.9521    0.9566    0.9626    0.9664    0.9707
        β[4]   -0.3892   -0.3890   -0.3888   -0.3882   -0.3878
julia&gt; container_cauchit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cauchit(), Prior_Ridge())
┌ Info: Found initial step size
└   ϵ = 0.003125
Chains MCMC chain (10000×17×1 Array{Float64, 3}):

Iterations        = 1001:1:11000
Number of chains  = 1
Samples per chain = 10000
Wall duration     = 153.65 seconds
Compute duration  = 153.65 seconds
parameters        = λ, β[1], β[2], β[3], β[4]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec 
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 

           λ    0.1737    0.1033     0.0010    0.0015   3731.6701    0.9999       24.2865
        β[1]    0.0033    0.0024     0.0000    0.0000   8722.9987    1.0000       56.7711
        β[2]   -0.0598    0.1193     0.0012    0.0017   5364.6587    1.0008       34.9143
        β[3]    0.2191    0.0365     0.0004    0.0005   5826.8422    0.9999       37.9223
        β[4]    0.0204    0.0128     0.0001    0.0002   5304.7531    0.9999       34.5245

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           λ    0.0702    0.1130    0.1483    0.2052    0.4066
        β[1]   -0.0013    0.0016    0.0032    0.0049    0.0083
        β[2]   -0.3183   -0.1325   -0.0507    0.0204    0.1532
        β[3]    0.1488    0.1942    0.2187    0.2429    0.2919
        β[4]   -0.0046    0.0117    0.0203    0.0292    0.0459</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/bayesian/logistic_regression.jl#L12-L167">source</a></section><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::CRRaoLink, prior::Prior_Laplace, h::Float64 = 0.1, level::Float64 = 0.95, sim_size::Int64 = 1000)</code></pre><p>Fit a Bayesian Logistic Regression model on the input data with a Laplace prior with the provided <code>Link</code> function.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using CRRao, RDatasets, StableRNGs, StatsModels
julia&gt; CRRao.set_rng(StableRNG(123))
StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)
julia&gt; turnout = dataset(&quot;Zelig&quot;, &quot;turnout&quot;)
2000×5 DataFrame
  Row │ Race   Age    Educate  Income   Vote  
      │ Cat…   Int32  Float64  Float64  Int32 
──────┼───────────────────────────────────────
    1 │ white     60     14.0   3.3458      1
    2 │ white     51     10.0   1.8561      0
    3 │ white     24     12.0   0.6304      0
    4 │ white     38      8.0   3.4183      1
  ⋮   │   ⋮      ⋮       ⋮        ⋮       ⋮
 1998 │ white     51     16.0   7.8949      1
 1999 │ white     22     10.0   2.4811      0
 2000 │ white     59     10.0   0.5523      0
                             1993 rows omitted
julia&gt; container_logit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Logit(), Prior_Laplace())
┌ Info: Found initial step size
└   ϵ = 0.003125
Chains MCMC chain (10000×17×1 Array{Float64, 3}):

Iterations        = 1001:1:11000
Number of chains  = 1
Samples per chain = 10000
Wall duration     = 143.44 seconds
Compute duration  = 143.44 seconds
parameters        = λ, β[1], β[2], β[3], β[4]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec 
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 

           λ    0.1178    0.0826     0.0008    0.0012   4670.5185    0.9999       32.5619
        β[1]    0.0051    0.0022     0.0000    0.0000   9160.7544    1.0001       63.8669
        β[2]   -0.0228    0.0890     0.0009    0.0013   4963.2154    1.0002       34.6025
        β[3]    0.1628    0.0254     0.0003    0.0004   5795.2458    1.0000       40.4033
        β[4]    0.0321    0.0118     0.0001    0.0002   5366.5589    1.0006       37.4146

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           λ    0.0380    0.0677    0.0958    0.1410    0.3341
        β[1]    0.0007    0.0036    0.0051    0.0066    0.0095
        β[2]   -0.2299   -0.0690   -0.0133    0.0273    0.1522
        β[3]    0.1145    0.1454    0.1624    0.1796    0.2133
        β[4]    0.0090    0.0240    0.0323    0.0400    0.0549
julia&gt; container_probit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Probit(), Prior_Laplace())
┌ Info: Found initial step size
└   ϵ = 0.00078125
Chains MCMC chain (10000×17×1 Array{Float64, 3}):

Iterations        = 1001:1:11000
Number of chains  = 1
Samples per chain = 10000
Wall duration     = 171.43 seconds
Compute duration  = 171.43 seconds
parameters        = λ, β[1], β[2], β[3], β[4]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec 
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 

           λ    0.0821    0.0551     0.0006    0.0008   4512.1853    1.0003       26.3206
        β[1]    0.0033    0.0013     0.0000    0.0000   8915.4805    0.9999       52.0059
        β[2]   -0.0138    0.0553     0.0006    0.0008   5240.6484    1.0000       30.5698
        β[3]    0.0916    0.0141     0.0001    0.0002   6402.4324    1.0001       37.3468
        β[4]    0.0212    0.0070     0.0001    0.0001   5508.5643    1.0000       32.1326

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           λ    0.0275    0.0477    0.0667    0.0988    0.2270
        β[1]    0.0008    0.0024    0.0033    0.0042    0.0059
        β[2]   -0.1346   -0.0444   -0.0088    0.0192    0.0907
        β[3]    0.0641    0.0820    0.0913    0.1011    0.1195
        β[4]    0.0074    0.0165    0.0213    0.0260    0.0349
julia&gt; container_cloglog = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cloglog(), Prior_Laplace())
┌ Info: Found initial step size
└   ϵ = 0.0015625
Chains MCMC chain (10000×17×1 Array{Float64, 3}):

Iterations        = 1001:1:11000
Number of chains  = 1
Samples per chain = 10000
Wall duration     = 194.12 seconds
Compute duration  = 194.12 seconds
parameters        = λ, β[1], β[2], β[3], β[4]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec 
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 

           λ    0.0731    0.0509     0.0005    0.0007   4981.1484    1.0002       25.6608
        β[1]    0.0008    0.0012     0.0000    0.0000   9615.6483    1.0003       49.5358
        β[2]   -0.0266    0.0521     0.0005    0.0007   4812.7260    1.0001       24.7932
        β[3]    0.0759    0.0114     0.0001    0.0002   5448.6076    0.9999       28.0690
        β[4]    0.0069    0.0060     0.0001    0.0001   4591.7360    0.9999       23.6547

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           λ    0.0242    0.0421    0.0600    0.0873    0.2035
        β[1]   -0.0015    0.0000    0.0008    0.0016    0.0031
        β[2]   -0.1478   -0.0559   -0.0199    0.0063    0.0647
        β[3]    0.0538    0.0682    0.0760    0.0836    0.0983
        β[4]   -0.0045    0.0027    0.0068    0.0111    0.0188
julia&gt; container_cauchit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cauchit(), Prior_Laplace())
┌ Info: Found initial step size
└   ϵ = 0.0330078125
Chains MCMC chain (10000×17×1 Array{Float64, 3}):

Iterations        = 1001:1:11000
Number of chains  = 1
Samples per chain = 10000
Wall duration     = 151.32 seconds
Compute duration  = 151.32 seconds
parameters        = λ, β[1], β[2], β[3], β[4]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec 
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 

           λ    0.1426    0.1158     0.0012    0.0019   4085.6946    1.0002       27.0013
        β[1]    0.0032    0.0024     0.0000    0.0000   6944.6444    1.0001       45.8953
        β[2]   -0.0474    0.1145     0.0011    0.0016   4859.5428    1.0001       32.1154
        β[3]    0.2237    0.0363     0.0004    0.0005   4613.9690    0.9999       30.4925
        β[4]    0.0185    0.0126     0.0001    0.0002   4725.1966    0.9999       31.2275

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           λ    0.0446    0.0790    0.1127    0.1686    0.4193
        β[1]   -0.0013    0.0016    0.0032    0.0047    0.0081
        β[2]   -0.3124   -0.1050   -0.0278    0.0204    0.1493
        β[3]    0.1551    0.1986    0.2228    0.2472    0.2979
        β[4]   -0.0055    0.0097    0.0182    0.0269    0.0439</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/bayesian/logistic_regression.jl#L201-L356">source</a></section><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::CRRaoLink, prior::Prior_Cauchy, h::Float64 = 0.1, level::Float64 = 0.95, sim_size::Int64 = 1000)</code></pre><p>Fit a Bayesian Logistic Regression model on the input data with a Cauchy prior with the provided <code>Link</code> function.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using CRRao, RDatasets, StableRNGs, StatsModels
julia&gt; CRRao.set_rng(StableRNG(123))
StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)
julia&gt; turnout = dataset(&quot;Zelig&quot;, &quot;turnout&quot;)
2000×5 DataFrame
  Row │ Race   Age    Educate  Income   Vote  
      │ Cat…   Int32  Float64  Float64  Int32 
──────┼───────────────────────────────────────
    1 │ white     60     14.0   3.3458      1
    2 │ white     51     10.0   1.8561      0
    3 │ white     24     12.0   0.6304      0
    4 │ white     38      8.0   3.4183      1
  ⋮   │   ⋮      ⋮       ⋮        ⋮       ⋮
 1998 │ white     51     16.0   7.8949      1
 1999 │ white     22     10.0   2.4811      0
 2000 │ white     59     10.0   0.5523      0
                             1993 rows omitted
julia&gt; container_logit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Logit(), Prior_Cauchy())
┌ Info: Found initial step size
└   ϵ = 0.003125
Chains MCMC chain (10000×17×1 Array{Float64, 3}):

Iterations        = 1001:1:11000
Number of chains  = 1
Samples per chain = 10000
Wall duration     = 152.61 seconds
Compute duration  = 152.61 seconds
parameters        = λ, β[1], β[2], β[3], β[4]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec 
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 

           λ    0.0792    0.0857     0.0009    0.0012   4663.0871    0.9999       30.5560
        β[1]    0.0052    0.0022     0.0000    0.0000   7200.5641    0.9999       47.1834
        β[2]   -0.0205    0.0797     0.0008    0.0013   4355.2582    0.9999       28.5389
        β[3]    0.1653    0.0256     0.0003    0.0003   4895.3528    1.0004       32.0780
        β[4]    0.0306    0.0117     0.0001    0.0002   3982.8457    1.0001       26.0985

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           λ    0.0092    0.0297    0.0545    0.0981    0.2908
        β[1]    0.0010    0.0038    0.0052    0.0067    0.0094
        β[2]   -0.2124   -0.0533   -0.0088    0.0194    0.1293
        β[3]    0.1153    0.1481    0.1652    0.1822    0.2164
        β[4]    0.0080    0.0227    0.0304    0.0384    0.0537
julia&gt; container_probit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Probit(), Prior_Cauchy())
┌ Info: Found initial step size
└   ϵ = 0.00078125
Chains MCMC chain (10000×17×1 Array{Float64, 3}):

Iterations        = 1001:1:11000
Number of chains  = 1
Samples per chain = 10000
Wall duration     = 188.49 seconds
Compute duration  = 188.49 seconds
parameters        = λ, β[1], β[2], β[3], β[4]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec 
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 

           λ    0.0484    0.0512     0.0005    0.0007   5054.3994    1.0000       26.8155
        β[1]    0.0034    0.0013     0.0000    0.0000   8376.0026    0.9999       44.4379
        β[2]   -0.0101    0.0470     0.0005    0.0007   3497.1991    1.0000       18.5540
        β[3]    0.0927    0.0142     0.0001    0.0002   5007.2301    1.0000       26.5652
        β[4]    0.0202    0.0070     0.0001    0.0001   4277.4390    0.9999       22.6934

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           λ    0.0057    0.0185    0.0335    0.0599    0.1824
        β[1]    0.0009    0.0025    0.0034    0.0042    0.0059
        β[2]   -0.1236   -0.0297   -0.0045    0.0135    0.0783
        β[3]    0.0649    0.0830    0.0927    0.1021    0.1207
        β[4]    0.0068    0.0155    0.0202    0.0249    0.0343
julia&gt; container_cloglog = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cloglog(), Prior_Cauchy())
Chains MCMC chain (10000×17×1 Array{Float64, 3}):

Iterations        = 1001:1:11000
Number of chains  = 1
Samples per chain = 10000
Wall duration     = 121.33 seconds
Compute duration  = 121.33 seconds
parameters        = λ, β[1], β[2], β[3], β[4]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse       ess      rhat   ess_per_sec 
      Symbol   Float64   Float64    Float64   Float64   Float64   Float64       Float64 

           λ    0.2562    0.0001     0.0000    0.0000   24.5965    1.0444        0.2027
        β[1]    0.0457    0.0000     0.0000    0.0000   30.9386    1.3997        0.2550
        β[2]    0.3084    0.0008     0.0000    0.0001   20.7337    1.6095        0.1709
        β[3]    0.0931    0.0023     0.0000    0.0002   20.2943    2.5468        0.1673
        β[4]   -1.3797    0.0072     0.0001    0.0007   20.2381    2.7801        0.1668

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           λ    0.2561    0.2562    0.2562    0.2563    0.2564
        β[1]    0.0456    0.0457    0.0457    0.0457    0.0457
        β[2]    0.3069    0.3077    0.3083    0.3089    0.3100
        β[3]    0.0893    0.0912    0.0933    0.0948    0.0971
        β[4]   -1.3910   -1.3863   -1.3800   -1.3733   -1.3681
julia&gt; container_cauchit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cauchit(), Prior_Cauchy())
┌ Info: Found initial step size
└   ϵ = 0.05
Chains MCMC chain (10000×17×1 Array{Float64, 3}):

Iterations        = 1001:1:11000
Number of chains  = 1
Samples per chain = 10000
Wall duration     = 169.7 seconds
Compute duration  = 169.7 seconds
parameters        = λ, β[1], β[2], β[3], β[4]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec 
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 

           λ    0.0759    0.0909     0.0009    0.0014   3905.2459    0.9999       23.0129
        β[1]    0.0033    0.0023     0.0000    0.0000   5465.8674    1.0000       32.2094
        β[2]   -0.0318    0.0969     0.0010    0.0015   4113.9104    0.9999       24.2425
        β[3]    0.2285    0.0364     0.0004    0.0007   2716.3177    1.0006       16.0068
        β[4]    0.0158    0.0124     0.0001    0.0002   3167.8247    1.0000       18.6674

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           λ    0.0050    0.0211    0.0451    0.0970    0.3174
        β[1]   -0.0011    0.0018    0.0033    0.0048    0.0078
        β[2]   -0.2849   -0.0621   -0.0083    0.0151    0.1277
        β[3]    0.1586    0.2033    0.2276    0.2529    0.3004
        β[4]   -0.0067    0.0069    0.0153    0.0239    0.0415</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/bayesian/logistic_regression.jl#L390-L543">source</a></section><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::CRRaoLink, prior::Prior_TDist, h::Float64 = 1.0, level::Float64 = 0.95, sim_size::Int64 = 1000)</code></pre><p>Fit a Bayesian Logistic Regression model on the input data with a T-Dist prior with the provided <code>Link</code> function.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using CRRao, RDatasets, StableRNGs, StatsModels
julia&gt; CRRao.set_rng(StableRNG(123))
StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)
julia&gt; turnout = dataset(&quot;Zelig&quot;, &quot;turnout&quot;)
2000×5 DataFrame
  Row │ Race   Age    Educate  Income   Vote  
      │ Cat…   Int32  Float64  Float64  Int32 
──────┼───────────────────────────────────────
    1 │ white     60     14.0   3.3458      1
    2 │ white     51     10.0   1.8561      0
    3 │ white     24     12.0   0.6304      0
    4 │ white     38      8.0   3.4183      1
  ⋮   │   ⋮      ⋮       ⋮        ⋮       ⋮
 1998 │ white     51     16.0   7.8949      1
 1999 │ white     22     10.0   2.4811      0
 2000 │ white     59     10.0   0.5523      0
                             1993 rows omitted
julia&gt; container_logit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Logit(), Prior_TDist())
┌ Info: Found initial step size
└   ϵ = 0.003125
Chains MCMC chain (10000×18×1 Array{Float64, 3}):

Iterations        = 1001:1:11000
Number of chains  = 1
Samples per chain = 10000
Wall duration     = 191.64 seconds
Compute duration  = 191.64 seconds
parameters        = λ, ν, β[1], β[2], β[3], β[4]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean        std   naive_se      mcse          ess      rhat   ess_per_sec 
      Symbol   Float64    Float64    Float64   Float64      Float64   Float64       Float64 

           λ    0.3043     0.1678     0.0017    0.0022    5712.7538    1.0000       29.8104
           ν   27.2612   549.4979     5.4950    7.5617    5110.0078    1.0002       26.6652
        β[1]    0.0052     0.0023     0.0000    0.0000   11260.6563    1.0004       58.7607
        β[2]   -0.0589     0.1247     0.0012    0.0012    8832.9112    1.0000       46.0921
        β[3]    0.1667     0.0255     0.0003    0.0003    8308.4832    1.0000       43.3555
        β[4]    0.0333     0.0123     0.0001    0.0001    7630.2720    0.9999       39.8165

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           λ    0.1211    0.1958    0.2627    0.3635    0.7244
           ν    0.5077    1.4402    2.9883    7.2135   79.1950
        β[1]    0.0008    0.0037    0.0052    0.0067    0.0097
        β[2]   -0.3138   -0.1399   -0.0546    0.0244    0.1796
        β[3]    0.1177    0.1491    0.1665    0.1841    0.2177
        β[4]    0.0094    0.0251    0.0334    0.0414    0.0575
julia&gt; container_probit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Probit(), Prior_TDist())
┌ Info: Found initial step size
└   ϵ = 0.00078125
Chains MCMC chain (10000×18×1 Array{Float64, 3}):

Iterations        = 1001:1:11000
Number of chains  = 1
Samples per chain = 10000
Wall duration     = 262.37 seconds
Compute duration  = 262.37 seconds
parameters        = λ, ν, β[1], β[2], β[3], β[4]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean        std   naive_se      mcse          ess      rhat   ess_per_sec 
      Symbol   Float64    Float64    Float64   Float64      Float64   Float64       Float64 

           λ    0.2694     0.1551     0.0016    0.0020    5091.0519    1.0002       19.4040
           ν   21.1549   329.1679     3.2917    4.5051    5189.9802    1.0000       19.7811
        β[1]    0.0034     0.0013     0.0000    0.0000   12584.4962    0.9999       47.9645
        β[2]   -0.0331     0.0779     0.0008    0.0009    6959.6501    0.9999       26.5260
        β[3]    0.0936     0.0138     0.0001    0.0002    6043.0747    1.0012       23.0326
        β[4]    0.0218     0.0072     0.0001    0.0001    5938.2251    1.0010       22.6329

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           λ    0.1104    0.1761    0.2314    0.3177    0.6395
           ν    0.4766    1.3862    2.9031    6.9434   79.8040
        β[1]    0.0008    0.0025    0.0034    0.0043    0.0060
        β[2]   -0.1901   -0.0841   -0.0325    0.0184    0.1189
        β[3]    0.0671    0.0843    0.0935    0.1027    0.1210
        β[4]    0.0077    0.0169    0.0217    0.0267    0.0361
julia&gt; container_cloglog = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cloglog(), Prior_TDist())
┌ Info: Found initial step size
└   ϵ = 0.0015625
Chains MCMC chain (10000×18×1 Array{Float64, 3}):

Iterations        = 1001:1:11000
Number of chains  = 1
Samples per chain = 10000
Wall duration     = 241.88 seconds
Compute duration  = 241.88 seconds
parameters        = λ, ν, β[1], β[2], β[3], β[4]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean        std   naive_se      mcse          ess      rhat   ess_per_sec 
      Symbol   Float64    Float64    Float64   Float64      Float64   Float64       Float64 

           λ    0.2705     0.1490     0.0015    0.0018    6373.8367    1.0001       26.3518
           ν   25.1429   513.9686     5.1397    7.9414    4083.5546    1.0000       16.8829
        β[1]    0.0010     0.0012     0.0000    0.0000   11899.8637    0.9999       49.1984
        β[2]   -0.0562     0.0693     0.0007    0.0009    6611.6159    0.9999       27.3348
        β[3]    0.0774     0.0115     0.0001    0.0001    6350.5188    0.9999       26.2554
        β[4]    0.0081     0.0066     0.0001    0.0001    5974.2918    1.0000       24.6999

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           λ    0.1079    0.1732    0.2318    0.3236    0.6667
           ν    0.4629    1.3747    2.7811    6.9236   88.1070
        β[1]   -0.0014    0.0001    0.0010    0.0018    0.0034
        β[2]   -0.1960   -0.1021   -0.0563   -0.0089    0.0781
        β[3]    0.0549    0.0696    0.0775    0.0851    0.0996
        β[4]   -0.0050    0.0036    0.0081    0.0126    0.0209
julia&gt; container_cauchit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cauchit(), Prior_TDist())
┌ Info: Found initial step size
└   ϵ = 0.009375000000000001
Chains MCMC chain (10000×18×1 Array{Float64, 3}):

Iterations        = 1001:1:11000
Number of chains  = 1
Samples per chain = 10000
Wall duration     = 224.46 seconds
Compute duration  = 224.46 seconds
parameters        = λ, ν, β[1], β[2], β[3], β[4]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean        std   naive_se      mcse          ess      rhat   ess_per_sec 
      Symbol   Float64    Float64    Float64   Float64      Float64   Float64       Float64 

           λ    0.3293     0.1847     0.0018    0.0022    6475.4472    1.0000       28.8487
           ν   16.2524   148.5657     1.4857    2.0404    5495.7033    1.0003       24.4839
        β[1]    0.0036     0.0025     0.0000    0.0000   10948.8161    0.9999       48.7780
        β[2]   -0.1076     0.1519     0.0015    0.0016    7435.6058    1.0000       33.1263
        β[3]    0.2321     0.0361     0.0004    0.0004    7391.3297    0.9999       32.9291
        β[4]    0.0198     0.0133     0.0001    0.0002    6891.4114    1.0000       30.7019

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           λ    0.1305    0.2117    0.2828    0.3905    0.7982
           ν    0.5166    1.4512    2.9337    6.9187   79.9496
        β[1]   -0.0011    0.0019    0.0036    0.0052    0.0087
        β[2]   -0.4347   -0.2049   -0.0983   -0.0031    0.1670
        β[3]    0.1629    0.2075    0.2313    0.2560    0.3057
        β[4]   -0.0058    0.0106    0.0196    0.0287    0.0461</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/bayesian/logistic_regression.jl#L577-L740">source</a></section><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::LogisticRegression, Link::CRRaoLink, prior::Prior_HorseShoe, h::Float64 = 0.01, level::Float64 = 0.95, sim_size::Int64 = 1000)</code></pre><p>Fit a Bayesian Logistic Regression model on the input data with a HorseShoe prior with the provided <code>Link</code> function.</p><p><strong>Example</strong></p><p>```julia-repl julia&gt; using CRRao, RDatasets, StableRNGs, StatsModels julia&gt; CRRao.set<em>rng(StableRNG(123)) julia&gt; turnout = dataset(&quot;Zelig&quot;, &quot;turnout&quot;); julia&gt; container</em>logit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Logit(), Prior<em>HorseShoe()) julia&gt; container</em>probit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Probit(), Prior<em>HorseShoe()) julia&gt; container</em>cloglog = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cloglog(), Prior<em>HorseShoe()) julia&gt; container</em>cauchit = fit(@formula(Vote ~ Age + Race + Income + Educate), turnout, LogisticRegression(), Cauchit(), Prior_HorseShoe())</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/bayesian/logistic_regression.jl#L775-L791">source</a></section><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::NegBinomRegression, prior::Prior_Ridge, h::Float64 = 0.1, sim_size::Int64 = 1000)</code></pre><p>Fit a Bayesian Negative Binomial Regression model on the input data with a Ridge prior.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using CRRao, RDatasets, StableRNGs, StatsModels
julia&gt; CRRao.set_rng(StableRNG(123))
StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)
julia&gt; sanction = dataset(&quot;Zelig&quot;, &quot;sanction&quot;)
78×8 DataFrame
 Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         
     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          
─────┼───────────────────────────────────────────────────────────────────
   1 │     1      4       3       1       1      4     15  major loss
   2 │     0      2       3       0       1      3      4  modest loss
   3 │     0      1       3       1       0      2      1  little effect
   4 │     1      1       3       1       1      2      1  little effect
  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮
  76 │     0      4       3       1       0      2     13  little effect
  77 │     0      1       2       0       0      1      1  net gain
  78 │     1      3       1       1       1      2     10  little effect
                                                          71 rows omitted
julia&gt; container = fit(@formula(Num ~ Target + Coop + NCost), sanction, NegBinomRegression(), Prior_Ridge())
┌ Info: Found initial step size
└   ϵ = 0.025
Chains MCMC chain (10000×19×1 Array{Float64, 3}):

Iterations        = 1001:1:11000
Number of chains  = 1
Samples per chain = 10000
Wall duration     = 26.52 seconds
Compute duration  = 26.52 seconds
parameters        = λ, α, β[1], β[2], β[3], β[4], β[5]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec 
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 

           λ    2.0416    0.4460     0.0045    0.0045   8499.3498    0.9999      320.5246
           α   -1.0792    0.5148     0.0051    0.0089   3405.4069    1.0010      128.4235
        β[1]   -0.0049    0.1614     0.0016    0.0023   4627.1117    1.0009      174.4960
        β[2]    1.0615    0.1319     0.0013    0.0020   5046.9022    1.0001      190.3270
        β[3]   -0.1757    0.5563     0.0056    0.0063   8056.2338    1.0001      303.8139
        β[4]    1.2810    0.3214     0.0032    0.0035   6779.1552    0.9999      255.6532
        β[5]    0.1493    0.2799     0.0028    0.0036   6164.9114    1.0004      232.4890

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           λ    1.3159    1.7243    1.9928    2.3049    3.0445
           α   -2.0865   -1.4300   -1.0908   -0.7306   -0.0721
        β[1]   -0.3180   -0.1136   -0.0044    0.1053    0.3146
        β[2]    0.8046    0.9738    1.0594    1.1483    1.3262
        β[3]   -1.2332   -0.5561   -0.1992    0.2020    0.9502
        β[4]    0.6571    1.0654    1.2744    1.4900    1.9274
        β[5]   -0.4064   -0.0370    0.1501    0.3388    0.6903</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/bayesian/negativebinomial_regression.jl#L17-L80">source</a></section><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::NegBinomRegression, prior::Prior_Laplace, h::Float64 = 0.01, sim_size::Int64 = 1000)</code></pre><p>Fit a Bayesian Negative Binomial Regression model on the input data with a Laplace prior.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using CRRao, RDatasets, StableRNGs, StatsModels
julia&gt; CRRao.set_rng(StableRNG(123))
StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)
julia&gt; sanction = dataset(&quot;Zelig&quot;, &quot;sanction&quot;)
78×8 DataFrame
 Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         
     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          
─────┼───────────────────────────────────────────────────────────────────
   1 │     1      4       3       1       1      4     15  major loss
   2 │     0      2       3       0       1      3      4  modest loss
   3 │     0      1       3       1       0      2      1  little effect
   4 │     1      1       3       1       1      2      1  little effect
  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮
  76 │     0      4       3       1       0      2     13  little effect
  77 │     0      1       2       0       0      1      1  net gain
  78 │     1      3       1       1       1      2     10  little effect
                                                          71 rows omitted
julia&gt; container = fit(@formula(Num ~ Target + Coop + NCost), sanction, NegBinomRegression(), Prior_Laplace())
┌ Info: Found initial step size
└   ϵ = 0.05
Chains MCMC chain (10000×19×1 Array{Float64, 3}):

Iterations        = 1001:1:11000
Number of chains  = 1
Samples per chain = 10000
Wall duration     = 26.96 seconds
Compute duration  = 26.96 seconds
parameters        = λ, α, β[1], β[2], β[3], β[4], β[5]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec 
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 

           λ    2.1058    0.4611     0.0046    0.0052   8213.6672    0.9999      304.6048
           α   -1.0014    0.5020     0.0050    0.0084   3465.0499    1.0000      128.5018
        β[1]   -0.0207    0.1583     0.0016    0.0021   5223.4434    0.9999      193.7120
        β[2]    1.0465    0.1301     0.0013    0.0017   5029.9415    1.0000      186.5359
        β[3]   -0.1426    0.4996     0.0050    0.0057   7487.9201    0.9999      277.6903
        β[4]    1.2832    0.3245     0.0032    0.0035   6912.6238    0.9999      256.3554
        β[5]    0.1198    0.2656     0.0027    0.0039   5505.7699    1.0000      204.1821

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           λ    1.3431    1.7782    2.0523    2.3788    3.1662
           α   -2.0082   -1.3266   -1.0000   -0.6730   -0.0202
        β[1]   -0.3373   -0.1240   -0.0190    0.0823    0.2921
        β[2]    0.7927    0.9595    1.0454    1.1337    1.3056
        β[3]   -1.1412   -0.4702   -0.1379    0.1801    0.8557
        β[4]    0.6480    1.0707    1.2824    1.4966    1.9203
        β[5]   -0.4026   -0.0558    0.1158    0.2980    0.6499</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/bayesian/negativebinomial_regression.jl#L111-L174">source</a></section><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::NegBinomRegression, prior::Prior_Cauchy, h::Float64 = 1.0, sim_size::Int64 = 1000)</code></pre><p>Fit a Bayesian Negative Binomial Regression model on the input data with a Cauchy prior.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using CRRao, RDatasets, StableRNGs, StatsModels
julia&gt; CRRao.set_rng(StableRNG(123))
StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)
julia&gt; sanction = dataset(&quot;Zelig&quot;, &quot;sanction&quot;)
78×8 DataFrame
 Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         
     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          
─────┼───────────────────────────────────────────────────────────────────
   1 │     1      4       3       1       1      4     15  major loss
   2 │     0      2       3       0       1      3      4  modest loss
   3 │     0      1       3       1       0      2      1  little effect
   4 │     1      1       3       1       1      2      1  little effect
  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮
  76 │     0      4       3       1       0      2     13  little effect
  77 │     0      1       2       0       0      1      1  net gain
  78 │     1      3       1       1       1      2     10  little effect
                                                          71 rows omitted
julia&gt; container = fit(@formula(Num ~ Target + Coop + NCost), sanction, NegBinomRegression(), Prior_Cauchy())
┌ Info: Found initial step size
└   ϵ = 0.2
Chains MCMC chain (10000×19×1 Array{Float64, 3}):

Iterations        = 1001:1:11000
Number of chains  = 1
Samples per chain = 10000
Wall duration     = 27.58 seconds
Compute duration  = 27.58 seconds
parameters        = λ, α, β[1], β[2], β[3], β[4], β[5]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec 
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 

           λ    2.0219    0.4304     0.0043    0.0047   7839.1614    1.0001      284.1923
           α   -1.0233    0.5192     0.0052    0.0091   3193.5541    1.0010      115.7756
        β[1]   -0.0192    0.1632     0.0016    0.0025   4320.9927    1.0006      156.6485
        β[2]    1.0535    0.1327     0.0013    0.0021   4739.9448    1.0008      171.8367
        β[3]   -0.1552    0.5453     0.0055    0.0069   7763.7273    1.0002      281.4576
        β[4]    1.2743    0.3250     0.0032    0.0041   6655.6093    1.0008      241.2851
        β[5]    0.1298    0.2822     0.0028    0.0036   5253.2578    1.0000      190.4458

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           λ    1.3226    1.7126    1.9731    2.2757    2.9804
           α   -2.0538   -1.3647   -1.0180   -0.6733   -0.0207
        β[1]   -0.3375   -0.1285   -0.0189    0.0881    0.3042
        β[2]    0.8001    0.9647    1.0516    1.1418    1.3138
        β[3]   -1.1825   -0.5301   -0.1676    0.2010    0.9589
        β[4]    0.6478    1.0553    1.2704    1.4870    1.9319
        β[5]   -0.4131   -0.0613    0.1305    0.3166    0.6901</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/bayesian/negativebinomial_regression.jl#L205-L268">source</a></section><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::NegBinomRegression, prior::Prior_TDist, h::Float64 = 1.0, sim_size::Int64 = 1000)</code></pre><p>Fit a Bayesian Negative Binomial Regression model on the input data with a t(ν) distributed prior.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using CRRao, RDatasets, StableRNGs, StatsModels
julia&gt; CRRao.set_rng(StableRNG(123))
StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)
julia&gt; sanction = dataset(&quot;Zelig&quot;, &quot;sanction&quot;)
78×8 DataFrame
 Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         
     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          
─────┼───────────────────────────────────────────────────────────────────
   1 │     1      4       3       1       1      4     15  major loss
   2 │     0      2       3       0       1      3      4  modest loss
   3 │     0      1       3       1       0      2      1  little effect
   4 │     1      1       3       1       1      2      1  little effect
  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮
  76 │     0      4       3       1       0      2     13  little effect
  77 │     0      1       2       0       0      1      1  net gain
  78 │     1      3       1       1       1      2     10  little effect
                                                          71 rows omitted
julia&gt; container = fit(@formula(Num ~ Target + Coop + NCost), sanction, NegBinomRegression(), Prior_TDist())
┌ Info: Found initial step size
└   ϵ = 0.05
Chains MCMC chain (10000×20×1 Array{Float64, 3}):

Iterations        = 1001:1:11000
Number of chains  = 1
Samples per chain = 10000
Wall duration     = 32.4 seconds
Compute duration  = 32.4 seconds
parameters        = λ, ν, α, β[1], β[2], β[3], β[4], β[5]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean        std   naive_se      mcse         ess      rhat   ess_per_sec 
      Symbol   Float64    Float64    Float64   Float64     Float64   Float64       Float64 

           λ    2.0021     0.4262     0.0043    0.0048   7946.6182    0.9999      245.2887
           ν   20.4978   213.5274     2.1353    2.7473   6455.1193    0.9999      199.2505
           α   -1.0562     0.5154     0.0052    0.0076   4162.0565    1.0010      128.4704
        β[1]   -0.0096     0.1617     0.0016    0.0022   5232.3275    1.0005      161.5065
        β[2]    1.0581     0.1308     0.0013    0.0016   5850.3314    1.0004      180.5825
        β[3]   -0.1725     0.5396     0.0054    0.0056   7961.1718    0.9999      245.7379
        β[4]    1.2762     0.3222     0.0032    0.0036   7541.0855    0.9999      232.7711
        β[5]    0.1400     0.2822     0.0028    0.0037   6538.0847    1.0009      201.8114

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%      97.5% 
      Symbol   Float64   Float64   Float64   Float64    Float64 

           λ    1.2972    1.6983    1.9536    2.2528     2.9704
           ν    0.6529    1.9144    3.8831    9.7303   108.9970
           α   -2.0694   -1.4014   -1.0565   -0.7076    -0.0696
        β[1]   -0.3338   -0.1176   -0.0090    0.0979     0.3056
        β[2]    0.8046    0.9695    1.0576    1.1482     1.3157
        β[3]   -1.2170   -0.5366   -0.1879    0.1806     0.9140
        β[4]    0.6519    1.0551    1.2762    1.4910     1.9133
        β[5]   -0.4045   -0.0511    0.1377    0.3314     0.6957</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/bayesian/negativebinomial_regression.jl#L298-L363">source</a></section><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::NegBinomRegression, prior::Prior_HorseShoe, sim_size::Int64 = 1000)</code></pre><p>Fit a Bayesian Negative Binomial Regression model on the input data with a HorseShoe prior. </p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using CRRao, RDatasets, StableRNGs, StatsPlots, StatsModels
julia&gt; CRRao.set_rng(StableRNG(123))
julia&gt; sanction = dataset(&quot;Zelig&quot;, &quot;sanction&quot;);
julia&gt; container = fit(@formula(Num ~ Target + Coop + NCost), sanction, NegBinomRegression(), Prior_HorseShoe())</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/bayesian/negativebinomial_regression.jl#L394-L408">source</a></section><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::PoissonRegression, prior::Prior_Ridge, h::Float64 = 0.1, sim_size::Int64 = 1000)</code></pre><p>Fit a Bayesian Poisson Regression model on the input data with a Ridge prior.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using CRRao, RDatasets, StableRNGs, StatsModels
julia&gt; CRRao.set_rng(StableRNG(123))
StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)
julia&gt; sanction = dataset(&quot;Zelig&quot;, &quot;sanction&quot;)
78×8 DataFrame
 Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         
     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          
─────┼───────────────────────────────────────────────────────────────────
   1 │     1      4       3       1       1      4     15  major loss
   2 │     0      2       3       0       1      3      4  modest loss
   3 │     0      1       3       1       0      2      1  little effect
   4 │     1      1       3       1       1      2      1  little effect
  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮
  76 │     0      4       3       1       0      2     13  little effect
  77 │     0      1       2       0       0      1      1  net gain
  78 │     1      3       1       1       1      2     10  little effect
                                                          71 rows omitted
julia&gt; container = fit(@formula(Num ~ Target + Coop + NCost), sanction, PoissonRegression(), Prior_Ridge())
┌ Info: Found initial step size
└   ϵ = 0.025
Chains MCMC chain (10000×19×1 Array{Float64, 3}):

Iterations        = 1001:1:11000
Number of chains  = 1
Samples per chain = 10000
Wall duration     = 28.3 seconds
Compute duration  = 28.3 seconds
parameters        = λ, α, β[1], β[2], β[3], β[4], β[5]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec 
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 

           λ    1.3118    0.4894     0.0049    0.0066   5733.9828    1.0000      202.6214
           α   -1.8003    0.2607     0.0026    0.0038   4247.2367    1.0000      150.0843
        β[1]    0.1392    0.0656     0.0007    0.0008   5949.9827    1.0000      210.2542
        β[2]    1.1334    0.0563     0.0006    0.0007   5344.6101    1.0003      188.8622
        β[3]   -0.3259    0.2281     0.0023    0.0026   7065.4440    0.9999      249.6712
        β[4]    1.6983    0.0988     0.0010    0.0012   6534.2641    1.0001      230.9009
        β[5]    0.4053    0.1688     0.0017    0.0023   5330.2762    1.0006      188.3556

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           λ    0.7113    0.9826    1.2040    1.5098    2.5620
           α   -2.3202   -1.9764   -1.7971   -1.6229   -1.3003
        β[1]    0.0115    0.0950    0.1399    0.1825    0.2690
        β[2]    1.0246    1.0950    1.1331    1.1712    1.2451
        β[3]   -0.7923   -0.4776   -0.3205   -0.1703    0.1022
        β[4]    1.5095    1.6308    1.6977    1.7645    1.8936
        β[5]    0.0755    0.2930    0.4068    0.5190    0.7331</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/bayesian/poisson_regression.jl#L12-L75">source</a></section><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::PoissonRegression, prior::Prior_Laplace, h::Float64 = 0.1, sim_size::Int64 = 1000)</code></pre><p>Fit a Bayesian Poisson Regression model on the input data with a Laplace prior.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using CRRao, RDatasets, StableRNGs, StatsModels
julia&gt; CRRao.set_rng(StableRNG(123))
StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)
julia&gt; sanction = dataset(&quot;Zelig&quot;, &quot;sanction&quot;)
78×8 DataFrame
 Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         
     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          
─────┼───────────────────────────────────────────────────────────────────
   1 │     1      4       3       1       1      4     15  major loss
   2 │     0      2       3       0       1      3      4  modest loss
   3 │     0      1       3       1       0      2      1  little effect
   4 │     1      1       3       1       1      2      1  little effect
  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮
  76 │     0      4       3       1       0      2     13  little effect
  77 │     0      1       2       0       0      1      1  net gain
  78 │     1      3       1       1       1      2     10  little effect
                                                          71 rows omitted
julia&gt; container = fit(@formula(Num ~ Target + Coop + NCost), sanction, PoissonRegression(), Prior_Laplace())
┌ Info: Found initial step size
└   ϵ = 0.025
Chains MCMC chain (10000×19×1 Array{Float64, 3}):

Iterations        = 1001:1:11000
Number of chains  = 1
Samples per chain = 10000
Wall duration     = 26.38 seconds
Compute duration  = 26.38 seconds
parameters        = λ, α, β[1], β[2], β[3], β[4], β[5]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec 
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 

           λ    1.1036    0.5676     0.0057    0.0086   5101.9856    1.0003      193.4109
           α   -1.7912    0.2625     0.0026    0.0041   4611.2398    1.0002      174.8072
        β[1]    0.1360    0.0649     0.0006    0.0008   6345.1220    0.9999      240.5369
        β[2]    1.1324    0.0561     0.0006    0.0008   6267.6347    1.0006      237.5994
        β[3]   -0.2965    0.2234     0.0022    0.0027   7304.0984    1.0001      276.8906
        β[4]    1.7010    0.1012     0.0010    0.0013   7420.3061    0.9999      281.2960
        β[5]    0.3928    0.1730     0.0017    0.0021   6264.6983    0.9999      237.4881

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           λ    0.4544    0.7383    0.9620    1.3049    2.5869
           α   -2.3130   -1.9684   -1.7862   -1.6133   -1.2838
        β[1]    0.0093    0.0924    0.1354    0.1801    0.2627
        β[2]    1.0241    1.0943    1.1313    1.1698    1.2448
        β[3]   -0.7542   -0.4437   -0.2889   -0.1370    0.1132
        β[4]    1.5029    1.6331    1.6994    1.7690    1.9002
        β[5]    0.0581    0.2740    0.3946    0.5113    0.7309</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/bayesian/poisson_regression.jl#L105-L168">source</a></section><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::LinearRegression, prior::Prior_Cauchy, h::Float64 = 1.0, sim_size::Int64 = 1000)</code></pre><p>Fit a Bayesian Poisson Regression model on the input data with a Cauchy prior.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using CRRao, RDatasets, StableRNGs, StatsModels
julia&gt; CRRao.set_rng(StableRNG(123))
StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)
julia&gt; sanction = dataset(&quot;Zelig&quot;, &quot;sanction&quot;)
78×8 DataFrame
 Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         
     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          
─────┼───────────────────────────────────────────────────────────────────
   1 │     1      4       3       1       1      4     15  major loss
   2 │     0      2       3       0       1      3      4  modest loss
   3 │     0      1       3       1       0      2      1  little effect
   4 │     1      1       3       1       1      2      1  little effect
  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮
  76 │     0      4       3       1       0      2     13  little effect
  77 │     0      1       2       0       0      1      1  net gain
  78 │     1      3       1       1       1      2     10  little effect
                                                          71 rows omitted
julia&gt; container = fit(@formula(Num ~ Target + Coop + NCost), sanction, PoissonRegression(), Prior_Cauchy())
┌ Info: Found initial step size
└   ϵ = 0.025
Chains MCMC chain (10000×19×1 Array{Float64, 3}):

Iterations        = 1001:1:11000
Number of chains  = 1
Samples per chain = 10000
Wall duration     = 27.23 seconds
Compute duration  = 27.23 seconds
parameters        = λ, α, β[1], β[2], β[3], β[4], β[5]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec 
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 

           λ    0.8558    0.4620     0.0046    0.0050   7120.2358    0.9999      261.5138
           α   -1.7984    0.2622     0.0026    0.0038   4736.5277    0.9999      173.9644
        β[1]    0.1383    0.0649     0.0006    0.0008   6989.3372    1.0001      256.7061
        β[2]    1.1322    0.0573     0.0006    0.0008   5442.3181    0.9999      199.8868
        β[3]   -0.2928    0.2169     0.0022    0.0025   6830.7146    1.0000      250.8802
        β[4]    1.7040    0.0974     0.0010    0.0011   6738.4680    0.9999      247.4921
        β[5]    0.3945    0.1673     0.0017    0.0023   5730.9957    0.9999      210.4894

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           λ    0.2927    0.5424    0.7551    1.0504    1.9964
           α   -2.3125   -1.9749   -1.7957   -1.6220   -1.2893
        β[1]    0.0112    0.0950    0.1366    0.1813    0.2677
        β[2]    1.0198    1.0937    1.1315    1.1709    1.2457
        β[3]   -0.7403   -0.4351   -0.2887   -0.1398    0.1058
        β[4]    1.5135    1.6384    1.7053    1.7704    1.8926
        β[5]    0.0677    0.2823    0.3952    0.5066    0.7253</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/bayesian/poisson_regression.jl#L198-L261">source</a></section><section><div><pre><code class="language-julia hljs">fit(formula::FormulaTerm, data::DataFrame, modelClass::PoissonRegression, prior::Prior_TDist, h::Float64 = 2.0, sim_size::Int64 = 1000)</code></pre><p>Fit a Bayesian Poisson Regression model on the input data with a t(ν) distributed prior.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using CRRao, RDatasets, StableRNGs, StatsModels
julia&gt; CRRao.set_rng(StableRNG(123))
StableRNGs.LehmerRNG(state=0x000000000000000000000000000000f7)
julia&gt; sanction = dataset(&quot;Zelig&quot;, &quot;sanction&quot;)
78×8 DataFrame
 Row │ Mil    Coop   Target  Import  Export  Cost   Num    NCost         
     │ Int32  Int32  Int32   Int32   Int32   Int32  Int32  Cat…          
─────┼───────────────────────────────────────────────────────────────────
   1 │     1      4       3       1       1      4     15  major loss
   2 │     0      2       3       0       1      3      4  modest loss
   3 │     0      1       3       1       0      2      1  little effect
   4 │     1      1       3       1       1      2      1  little effect
  ⋮  │   ⋮      ⋮      ⋮       ⋮       ⋮       ⋮      ⋮          ⋮
  76 │     0      4       3       1       0      2     13  little effect
  77 │     0      1       2       0       0      1      1  net gain
  78 │     1      3       1       1       1      2     10  little effect
                                                          71 rows omitted
julia&gt; container = fit(@formula(Num ~ Target + Coop + NCost), sanction, PoissonRegression(), Prior_TDist())
┌ Info: Found initial step size
└   ϵ = 0.0125
Chains MCMC chain (10000×20×1 Array{Float64, 3}):

Iterations        = 1001:1:11000
Number of chains  = 1
Samples per chain = 10000
Wall duration     = 28.44 seconds
Compute duration  = 28.44 seconds
parameters        = λ, ν, α, β[1], β[2], β[3], β[4], β[5]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec 
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64 

           λ    0.9887    0.4187     0.0042    0.0046   6943.4330    1.0001      244.1861
           ν    3.0837    7.9963     0.0800    0.1140   4422.8043    1.0000      155.5409
           α   -1.8065    0.2648     0.0026    0.0042   3384.5428    0.9999      119.0274
        β[1]    0.1399    0.0656     0.0007    0.0009   5242.0449    1.0001      184.3518
        β[2]    1.1339    0.0565     0.0006    0.0009   4397.9611    1.0004      154.6672
        β[3]   -0.3097    0.2208     0.0022    0.0029   5930.8888    1.0000      208.5771
        β[4]    1.7026    0.1000     0.0010    0.0012   5706.3129    0.9999      200.6792
        β[5]    0.4025    0.1701     0.0017    0.0024   4239.8288    0.9999      149.1060

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5% 
      Symbol   Float64   Float64   Float64   Float64   Float64 

           λ    0.4024    0.7020    0.9159    1.1889    2.0151
           ν    0.5755    1.1693    1.8160    3.0737   12.7800
           α   -2.3383   -1.9843   -1.8024   -1.6222   -1.3155
        β[1]    0.0116    0.0952    0.1392    0.1836    0.2704
        β[2]    1.0255    1.0953    1.1331    1.1717    1.2464
        β[3]   -0.7635   -0.4518   -0.3017   -0.1559    0.1005
        β[4]    1.5112    1.6334    1.7023    1.7700    1.9025
        β[5]    0.0680    0.2864    0.4016    0.5174    0.7395</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/bayesian/poisson_regression.jl#L291-L356">source</a></section></article><h2 id="Model-Classes"><a class="docs-heading-anchor" href="#Model-Classes">Model Classes</a><a id="Model-Classes-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Classes" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CRRao.LinearRegression" href="#CRRao.LinearRegression"><code>CRRao.LinearRegression</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LinearRegression</code></pre><p>Type representing the Linear Regression model class.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/CRRao.jl#L7-L13">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CRRao.LogisticRegression" href="#CRRao.LogisticRegression"><code>CRRao.LogisticRegression</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">LogisticRegression</code></pre><p>Type representing the Logistic Regression model class.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/CRRao.jl#L16-L22">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CRRao.NegBinomRegression" href="#CRRao.NegBinomRegression"><code>CRRao.NegBinomRegression</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">NegBinomRegression</code></pre><p>Type representing the Negative Binomial Regression model class.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/CRRao.jl#L25-L31">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CRRao.PoissonRegression" href="#CRRao.PoissonRegression"><code>CRRao.PoissonRegression</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PoissonRegression</code></pre><p>Type representing the Poisson Regression model class.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/CRRao.jl#L34-L40">source</a></section></article><h2 id="Link-functions."><a class="docs-heading-anchor" href="#Link-functions.">Link functions.</a><a id="Link-functions.-1"></a><a class="docs-heading-anchor-permalink" href="#Link-functions." title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CRRao.CRRaoLink" href="#CRRao.CRRaoLink"><code>CRRao.CRRaoLink</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CRRaoLink</code></pre><p>Abstract type representing link functions which are used to dispatch to appropriate calls.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/CRRao.jl#L51-L57">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CRRao.Logit" href="#CRRao.Logit"><code>CRRao.Logit</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Logit &lt;: CRRaoLink</code></pre><p>A type representing the Logit link function, which is defined by the formula</p><p class="math-container">\[z\mapsto \dfrac{1}{1 + \exp(-z)}\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/CRRao.jl#L66-L76">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CRRao.Probit" href="#CRRao.Probit"><code>CRRao.Probit</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Probit &lt;: CRRaoLink</code></pre><p>A type representing the Probit link function, which is defined by the formula</p><p class="math-container">\[z\mapsto \mathbb{P}[Z\le z]\]</p><p>where <span>$Z\sim \text{Normal}(0, 1)$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/CRRao.jl#L83-L95">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CRRao.Cloglog" href="#CRRao.Cloglog"><code>CRRao.Cloglog</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Cloglog &lt;: CRRaoLink</code></pre><p>A type representing the Cloglog link function, which is defined by the formula </p><p class="math-container">\[z\mapsto 1 - \exp(-\exp(z))\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/CRRao.jl#L102-L112">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CRRao.Cauchit" href="#CRRao.Cauchit"><code>CRRao.Cauchit</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Cauchit &lt;: CRRaoLink</code></pre><p>A type representing the Cauchit link function, which is defined by the formula</p><p class="math-container">\[z\mapsto \dfrac{1}{2} + \dfrac{\text{atan}(z)}{\pi}\]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/CRRao.jl#L119-L129">source</a></section></article><h2 id="Setting-Random-Number-Generators"><a class="docs-heading-anchor" href="#Setting-Random-Number-Generators">Setting Random Number Generators</a><a id="Setting-Random-Number-Generators-1"></a><a class="docs-heading-anchor-permalink" href="#Setting-Random-Number-Generators" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CRRao.set_rng" href="#CRRao.set_rng"><code>CRRao.set_rng</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">set_rng(rng)</code></pre><p>Set the random number generator. This is useful if you want to work with reproducible results. <code>rng</code> must be a random number generator.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">using StableRNGs
CRRao.set_rng(StableRNG(1234))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/xKDR/CRRao.jl/blob/651fa5d9e36d193f5c24675dfe3ab6cd37312f87/src/random_number_generator.jl#L3-L16">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../man/guide/">« Guide</a><a class="docs-footer-nextpage" href="../frequentist_regression/">Frequentist Regression Models »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Monday 17 October 2022 09:01">Monday 17 October 2022</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
